<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"superzlw.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="IntroductionThe concept of the diffusion probability model was initially proposed by Jascha Sohl-Dickstein et al. in 2015. However, due to limitations in hardware devices such as memory at that time,">
<meta property="og:type" content="article">
<meta property="og:title" content="Diffusion Probabilistic Models (DPMs)">
<meta property="og:url" content="https://superzlw.github.io/posts/42.html">
<meta property="og:site_name" content="SuperZLW&#39;s Blog">
<meta property="og:description" content="IntroductionThe concept of the diffusion probability model was initially proposed by Jascha Sohl-Dickstein et al. in 2015. However, due to limitations in hardware devices such as memory at that time,">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://superzlw.github.io/images/379.png">
<meta property="og:image" content="https://superzlw.github.io/images/380.png">
<meta property="og:image" content="https://superzlw.github.io/images/381.png">
<meta property="og:image" content="https://superzlw.github.io/images/382.png">
<meta property="og:image" content="https://superzlw.github.io/images/383.png">
<meta property="og:image" content="https://superzlw.github.io/images/384.png">
<meta property="og:image" content="d:/blog/source/images/385.png">
<meta property="og:image" content="https://superzlw.github.io/images/difussion.gif">
<meta property="article:published_time" content="2023-06-11T11:52:49.470Z">
<meta property="article:modified_time" content="2023-07-16T21:58:55.828Z">
<meta property="article:author" content="super wzl">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://superzlw.github.io/images/379.png">

<link rel="canonical" href="https://superzlw.github.io/posts/42.html">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Diffusion Probabilistic Models (DPMs) | SuperZLW's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">SuperZLW's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">我很笨，但是我不懒</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://superzlw.github.io/posts/42.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="super wzl">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SuperZLW's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Diffusion Probabilistic Models (DPMs)
        </h1>

        <div class="post-meta">
          <span class="post-time">

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-06-11 13:52:49" itemprop="dateCreated datePublished" datetime="2023-06-11T13:52:49+02:00">2023-06-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-07-16 23:58:55" itemprop="dateModified" datetime="2023-07-16T23:58:55+02:00">2023-07-16</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/" itemprop="url" rel="index"><span itemprop="name">学习总结</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/posts/42.html#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/posts/42.html" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>The concept of the diffusion probability model was initially proposed by Jascha Sohl-Dickstein et al. in 2015. However, due to limitations in hardware devices such as memory at that time, this model did not receive much attention. Thanks to the development of technology, especially the advancements in GPUs and memory devices in recent years, the diffusion model has started to gain recognition. </p>
<span id="more"></span>
<p>In the past two years, the diffusion model has emerged as a generative model, and its performance has amazed people, leading to a significant increase in research efforts and the production of high-quality papers.</p>
<p>The graph below illustrates the increasing level of attention the diffusion model has received in 2022, showcasing its importance:</p>
<p><img src="../images/379.png" alt=""></p>
<p>Understanding the classic paper on diffusion probability models is essential for anyone interested in working with such models. Unlike other neural networks, diffusion probability models involve a significant amount of mathematical background knowledge. This blog post aims to explain the most crucial parts of the model using simple language.</p>
<p>Before diving in, I would like to express my gratitude for the development of the Internet, which has allowed me to easily access high-quality blogs and videos. These resources have played a vital role in helping me understand diffusion probability models. The references to these resources are provided at the end of this post. If you find any inaccuracies or areas that need improvement in my content, please feel free to contact me via email. High-quality communication is the foundation of progress.</p>
<h1 id="Diffusion-process-and-Reverse-process"><a href="#Diffusion-process-and-Reverse-process" class="headerlink" title="Diffusion process and Reverse process"></a>Diffusion process and Reverse process</h1><p>The diffusion probability model consists of two main components: the diffusion process and the reverse diffusion process. To simplify the explanation (although not entirely accurate), we can illustrate it using an image. In the diffusion process, noise is continually added to the image until it becomes a completely noisy image, as shown in the figure below:</p>
<p><img src="../images/380.png" alt=""></p>
<p><em>The data distribution (left) undergoes Gaussian diffusion, which gradually transforms it into an identity-covariance Gaussian (right)</em></p>
<p>On the other hand, the reverse diffusion process involves obtaining the target image from a noisy image, as depicted in the figure below:</p>
<p><img src="../images/381.png" alt=""></p>
<p><em>An identity-covariance Gaussian (right) undergoes a Gaussian diffusion process with learned mean and covariance functions, and is gradually transformed back into the data distribution (left).</em></p>
<h2 id="Diffusion-process"><a href="#Diffusion-process" class="headerlink" title="Diffusion process"></a>Diffusion process</h2><p><em>“In probability theory and statistics, diffusion processes are a class of continuous-time Markov process with almost surely continuous sample paths.”</em></p>
<p align="right">---- Wikipedia</p>

<p>This definition is like a master of ambiguity, saying everything and yet saying nothing at all, let’s attempt to dissect this definition and break it down into simpler terms.</p>
<p><strong>First is the <em>Markov process</em>.</strong> A Markov process is characterized by its memorylessness property, which means that the conditional probability of the process depends only on its current state and is independent of its past history or future states:</p>
<script type="math/tex; mode=display">
p(X_{t_n}|X_{t_{n-1}}, ... , X_{t_0})=p(X_{t_n}|X_{t_{n-1}})\qquad \forall t_0 <t_1<...<t_{n-1}<t_n</script><p><strong>Then is <em>continuous sample paths</em>.</strong> If all possible observed trajectories of a process can be observed as continuous, we say that it has continuous sample paths. This means that the process does not exhibit abrupt jumps or discontinuities in its observed behavior over time.</p>
<p>Any diffusion process can be described using a stochastic differential equation (SDE) in the following form:</p>
<script type="math/tex; mode=display">
dX_t = a(X_t, t)dt+\sigma(X_t, t)dWt</script><p>where $a$ is the drift coefficient, $\sigma$ is the diffusion coefficient and W is the Wiener process. And the Wiener process introduces (continuous) randomness with independent Gaussian increments, that is:</p>
<script type="math/tex; mode=display">
dW_t\approx W_{t+dt}-W_t \sim \mathcal{N}(0, dt)</script><p>In an alternative representation, difussion function can be written as:</p>
<script type="math/tex; mode=display">
X_{t+dt}-X_t \approx a(X_t,t)dt+\sigma(X_t,t)U \qquad where\quad U\sim \mathcal{N}(0,dt)</script><p>that is:</p>
<script type="math/tex; mode=display">
X_{t+dt}\approx X_t+a(X_t, t)dt+U' \qquad where \quad U'\sim \mathcal{N}(0,\sigma^2(X_t,t)dt)</script><p>As we can observe, the state at the next time step is equal to the state at the current time step plus a deterministic drift term and a stochastic diffusion term. The diffusion term is defined by a normal random variable, with the variance proportional to the square of the diffusion coefficient. Therefore, the diffusion coefficient represents the intensity of the randomness to be applied.</p>
<p>Here, let’s borrow an illustration from Joseph’s blog to provide a more visual explanation of the effects of the drift term and the diffusion term:</p>
<p><img src="../images/382.png" title="" alt="" width="632"></p>
<p>From this, we can observe that the drift term provides a trend or direction, while the diffusion term introduces randomness. A higher diffusion coefficient implies greater randomness or variability in the process. In other words, a larger diffusion coefficient leads to increased levels of random fluctuations, making the process more unpredictable and volatile.</p>
<p><strong>Additionally, when the absolute value of the drift coefficient is less than 1 ($|a|&lt;1$), it is referred to as the contracting drift coefficient.</strong></p>
<p>When a diffusion process has a “contracting” drift coefficient and a non-zero diffusion coefficient, it can gradually transform data from a complex distribution to a set of independent Gaussian noises. </p>
<p>Here, let’s take the example of one-dimensional data, where the drift coefficient and diffusion coefficient are represented by $\sqrt{1-p}$ and $\sqrt{p}$  respectively ($p &lt;1$). Although this is not a rigorous proof, it provides an intuitive understanding of the concept:</p>
<script type="math/tex; mode=display">
\begin{aligned}
X_T&=\sqrt{1-p}X_{T-1}+\sqrt{p}u_T \\
   &= (\sqrt{1-p})^2X_{T-2}+\sqrt{1-p}\sqrt{p}u_{T-1}+\sqrt{p}u_T \\
&= ... \\
&= (\sqrt{1-p})^TX_0+\sum^{T-1}_{i=0}\sqrt{p}(\sqrt{1-p})^iu_{T-i}    \\
&= (\sqrt{1-p})^TX_0+\sqrt{p}\sum^{T-1}_{i=0}(\sqrt{1-p})^iu_{T-i}\\
&= (\sqrt{1-p})^TX_0+\sqrt{p}\sum^{T-1}_{i=0}\mathcal{N}(0,(1-p)^i\sigma)\\
&= (\sqrt{1-p})^TX_0+\sqrt{p}\mathcal{N}(0,\frac{1-(1-p)^T}{1-(1-p)}\sigma)\\
&= (\sqrt{1-p})^TX_0+\sqrt{1-(1-p)^T}\mathcal{z}_T\\
\end{aligned}</script><p>where $\mathcal{z}$ is Gussian Distribution $\mathcal{N}(0, \sigma)$.</p>
<p>Let’s further set $\sigma$ as $I$, and $(1-p)^T$ as $\overline{\alpha}_T$. that is:</p>
<script type="math/tex; mode=display">
X_T = \mathcal{N}(X_T;\sqrt{\overline{\alpha}_T}X_0,(1-\overline{\alpha}_T)I)</script><p>Let’s write it in a more general form (just for the sake of elegance):</p>
<script type="math/tex; mode=display">
q(x_t|x_0)=\mathcal{N}(x_t;\sqrt{\overline{\alpha}_t}x_0,(1-\overline{\alpha}_t)I)</script><p>Now we have a relationship between $x_0$ and $x_t$.</p>
<p>According to the properties of the Wiener process, all the Gaussians in the diffusion term are independent. Therefore, as $T$ approaches infinity, we can obtain the following result:</p>
<script type="math/tex; mode=display">
(\sqrt{1-p})^T \stackrel{T\rightarrow \infty}{\rightarrow}  0</script><script type="math/tex; mode=display">
\sqrt{1-(1-p)^T} \stackrel{T\rightarrow\infty}{\rightarrow}1</script><p>(<strong>Note:</strong> Always remember, this is a Wiener process, and the Gaussians are independent.)</p>
<p>That is:</p>
<blockquote>
<p>Given a sufficiently long period of time or a sufficiently large number of iterations, we can transform any initial input into a standard normal distribution.</p>
</blockquote>
<p>Although this is 1-D situation, but the same reasoning can be extended to multidimensional data, such as images. In the context of diffusion processes in higher dimensions, such as image data, the concept of independent Gaussians still holds. Each pixel or voxel in the image can be considered as a random variable, and the diffusion process introduces independent Gaussian fluctuations to each of these variables. As time progresses, the diffusion and drift terms act collectively to transform the initial image distribution into a set of independent Gaussian distributions at each pixel or voxel. This property is crucial in various applications, including image denoising, segmentation, and other image processing tasks based on diffusion probability models.</p>
<p>We performed a series of calculations based on 1-D above to illustrate that the following mathematical form</p>
<script type="math/tex; mode=display">
X_T=\sqrt{1-p}X_{T-1}+\sqrt{p}u_T</script><p>is reasonable, that is, the input signal can be transformed into an independent Gaussian distribution through a series of operations. The same form is also applicable in high dimensions, but Another question, is p always the same? Obviously not, p is always changing (the reason will be mentioned later), so we can write a more general form:</p>
<script type="math/tex; mode=display">
q(x_t|x_{t-1})=\mathcal{N}(x_t;\sqrt{1-p_t}x_{t-1},p_tI)</script><p>$p_t$ indicates at each step the trade-off between information to be kept from the previous step and noise to be added.</p>
<p>And in other form (The following formula is easy to prove, which is omitted here):</p>
<script type="math/tex; mode=display">
q(x_t|x_0)=\mathcal{N}(x_t;\sqrt{\overline{\alpha}_t}x_0,(1-\overline{\alpha}_t)I)</script><script type="math/tex; mode=display">
q(x_t|x_{t-1})=\mathcal{N}(x_t;\sqrt{\alpha_t}x_{t-1},(1-\alpha_t)I)</script><p>where:</p>
<script type="math/tex; mode=display">
\alpha_t=(1-p_t) \qquad and \qquad \overline{\alpha}_t=\prod_{t=1}^{t}\alpha
_t=\prod_{t=1}^t(1-p_t) \qquad and\qquad 
\{\alpha_t \in (0,1)\}_{t=1}^T</script><p>This is the essence of the diffusion process: using it, we can gradually transform data from a complex distribution into isotropic Gaussian noise. This process is relatively straightforward. However, if we want to reverse this process and transform simple distributions into complex ones, it becomes very challenging. Starting from a noisy image, we are unable to recover the underlying structure. </p>
<p><img src="../images/383.png" alt=""></p>
<p>Diffusion provides us with a progressive and structured approach to transform data from complex distributions to isotropic Gaussian noise.</p>
<p><img src="../images/384.png" alt=""></p>
<p>(<strong>Note:</strong> <em>Generally, in setting the diffusion coefficient, it is common to gradually decrease the coefficient over time. This approach is often referred to as annealing or annealed diffusion.</em></p>
<p><em>The purpose of reducing the diffusion coefficient over time is to control the rate of diffusion and achieve a more controlled and structured restoration process. By starting with a higher diffusion coefficient, the restoration process initially allows for more randomness and exploration in the diffusion, which helps to break down complex distributions. As the diffusion progresses, the coefficient is gradually reduced, leading to a decrease in the amount of randomness and emphasizing the preservation of important structures and details</em>.)</p>
<h2 id="Reverse-process"><a href="#Reverse-process" class="headerlink" title="Reverse process"></a>Reverse process</h2><p>There is an interesting question: While theoretically, it is possible to directly achieve the final image restoration from a noisy image by combining multiple recovery steps into one large step, there are several reasons why it is not practical or necessary to do so. Some of the main reasons are:</p>
<ol>
<li><p>Computational Complexity: Performing a single large step that encompasses all the recovery operations can be computationally intensive and time-consuming. It may require significantly more resources and processing power, making it impractical for real-time or resource-constrained applications.</p>
</li>
<li><p>Information Loss: Each intermediate step in the progressive recovery process provides valuable information and constraints that guide the restoration process. By merging all the steps into one, we might lose this valuable information, resulting in a suboptimal or less accurate final result.</p>
</li>
<li><p>Convergence and Stability: The progressive nature of the step-by-step recovery allows for better convergence and stability. It enables the algorithm to refine the restoration iteratively, gradually improving the quality of the image. By attempting to accomplish the entire process in one large step, we may encounter convergence issues or unstable behavior, leading to inferior results.</p>
</li>
<li><p>Interpretability and Control: Breaking down the recovery process into smaller steps allows for better interpretability and control. Each step can be individually analyzed, adjusted, or optimized based on specific requirements or prior knowledge. It provides a more fine-grained approach to understand and address the challenges in the restoration process.</p>
</li>
<li><p>Each recovery step in the progressive restoration process may not be entirely different from the others. It is possible to use a single model with different parameters to represent each recovery step. This approach allows us to leverage the shared knowledge and capabilities of the model while adapting its behavior to different stages of the restoration process.</p>
</li>
</ol>
<p>Overall, while it may seem feasible to combine all the recovery steps into a single large step, the step-by-step approach offers practical advantages in terms of computational efficiency, information preservation, convergence, stability, interpretability, and control.</p>
<p>And now back to our main part: reverse process. In the process of recovering the original data from Gaussian noise, we can make the assumption that it follows a Gaussian distribution as well. However, we can’t simply fit the distribution step by step (well, we could try using a Gaussian Mixture Model, but let’s not get into that complexity). Instead, we need to construct a parameterized distribution to make estimations. Reverse process is also a process of Markov chain.</p>
<p>That is:</p>
<script type="math/tex; mode=display">
p_{\theta}(X_{T-1}|X_T)=\mathcal{N}(X_{T-1}; \mu_{\theta}(X_T, T),\Sigma_{\theta}(X_T,T))</script><p>$\mu_{\theta}$ and $\Sigma_{\theta}$ represent two networks respectively. </p>
<p>Now let’s review the diffusion process again, but this time we focus on the posterior probability. </p>
<p>Diffusion Conditional Probability of the Posterior $q(X_{T-1}|X_T, X_0)$ can be expressed by formul, that means, we can cauculate $X_{T-1}$ given $X_T$ and $X_0$.</p>
<p>(<strong>Note</strong>: $X_0$ is required here)</p>
<p>Assume:</p>
<script type="math/tex; mode=display">
q(X_{T-1}|X_T,X_0)=\mathcal{N}(X_{T-1};\widetilde{\mu}(X_T,X_0),\widetilde{\beta}_TI)</script><p>Using Bayes’ rule, we have:</p>
<script type="math/tex; mode=display">
\begin{aligned}
q(X_{T-1}|X_T,X_0)&=q(X_T|X_{T-1},X_0)\frac{q(X_{T-1}|X_0)}{q(X_T|X_0)}\\
                  &\propto \exp(-\frac{1}{2}(\frac{(X_T-\sqrt{\alpha_T}X_{T-1})^2}{1-\alpha_T}+\frac{(X_{T-1}-\sqrt{\overline{\alpha}_{T-1}}X_0)^2}{1-\overline{\alpha}_{T-1}}-
\frac{(X_{T}-\sqrt{\overline{\alpha}_T}X_0)^2}{1-\overline{\alpha}_T}))\\
&= \exp(-\frac{1}{2}((\frac{\alpha_T}{1-\alpha_T}+\frac{1}{1-\overline{\alpha}_{T-1}})X_{T-1}^2
-(\frac{2\sqrt{\alpha_T}}{1-\alpha_T}X_T+\frac{2\sqrt{\overline{\alpha}_{T-1}}}{1-
\overline{\alpha}_{T-1}}X_0)X_{T-1}+C(X_T,X_0)))
\end{aligned}</script><p>In the midst of this journey, we encounter a series of steps involving the diffusion equation, the Markov chain, and the following relationship:</p>
<script type="math/tex; mode=display">
q(X_{1:T}|X_0)=\prod_{t=1}^Tq(X_t|X_{t-1})</script><script type="math/tex; mode=display">
ax^2+bx=a(x+\frac{b}{2a})^2+C</script><p>Probability Density Function of Gaussian Distribution:</p>
<script type="math/tex; mode=display">
f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}</script><p>Following the standard Gaussian density function, the variance $\widetilde{\beta}_t$ and the mean $\widetilde{\mu}_t(x_t,x_0)$ can be parameterized as follows ($t$ here is same as $T$ above):</p>
<script type="math/tex; mode=display">
\widetilde{\beta}_t=\frac{1}{\frac{\alpha_t}{1-\alpha_t}+\frac{1}{1
-\overline{\alpha}_{t-1}}}=\frac{1-\overline{\alpha}
_{t-1}}{1-\overline{\alpha}_t}(1-\alpha_t)</script><script type="math/tex; mode=display">
\widetilde{\mu}_t(x_t,x_0)=(\frac{\sqrt{\alpha_t}}{\beta_t}X_t+
\frac{\sqrt{\overline{\alpha}_t}}{1-\overline{\alpha}_t}X_0)/
(\frac{\alpha_t}{\beta_t}+\frac{1}{1-\overline{\alpha}_{t-1}})=
\frac{\sqrt{\alpha_t}(1-\overline{\alpha}_{t-1})}{1-\overline{\alpha}_t}X_t
+\frac{\sqrt{\overline{\alpha}_{t-1}}(1-\alpha_t)}{1-\overline{\alpha}_t}X_0</script><p>Based on the relationship between $x_0$ and $x$ mentioned earlier, we have:</p>
<script type="math/tex; mode=display">
X_0=\frac{1}{\sqrt{\overline{\alpha}_T}}(X_T-\sqrt{1-\overline{\alpha}_T}\mathcal{z}_T)</script><p>We insert the $X_0$ here into $q(X_{T-1}|X_T, X_0)$ , then we could have a new mean-value-expression(the process is very complex and typing the formula here is also too troublesome…). But we could know that, there is no more $X_0$ here but occurs a noise-term, and base on it, we could design a Neural Network. </p>
<p>the result is(<em>Intermediate steps are omitted</em>.):</p>
<script type="math/tex; mode=display">
\widetilde{\mu}_t = \frac{1}{\sqrt{\alpha_t}}(X_t-\frac{1-\alpha_t}{\sqrt{
 1-\overline{\alpha}_t
}}z_t)</script><p>Now let’s put them aside, and back to our flow. If we want to optimize the network, one of the most important thing is: Loss Function. And since we focus on the reverse process, that is, we want to optimize the reverse process, so the function must be related to $\mu_{\theta}$ and $\Sigma_{\theta}$.  But here we use another form: $-\log p_{\theta}(x_0)$, and we plus a KL divergence to build the upper bound(since KL divergence is always great than 0):</p>
<script type="math/tex; mode=display">
\begin{aligned}

-\log p_{\theta}(x_0) &\leqslant -\log p_{\theta}(x_0)+D_{KL}(q(x_{1:T}|x_0)
||p_{\theta}(x_{1:T}|x_0)) \\
&= -\log p_{\theta}(x_0) + \mathbb{E}_{x_{1:T}\sim q(x_{1:T}|x_0)}[\log \frac
{q(x_{1:T}|x_0)}{p_{\theta}(x_{0:T})/p_{\theta}(x_0)}]\\
&= -\log p_{\theta}(x_0) + \mathbb{E}_q[\log \frac{q(x_{1:T}|x_0)}{p_{\theta}(x_{0:T})}
+\log p_{\theta}(x_0)]\\
&= \mathbb{E}_q[\log \frac{q(x_{1:T}|x_0)}{p_{\theta}(x_{0:T})}
]

\end{aligned}</script><p>Let:</p>
<script type="math/tex; mode=display">
L_{VLB}=\mathbb{E}_{q(x_{0:T})}[\log \frac{q(x_{1:T}|x_0)}{p_{\theta}(x_{0:T})}]
\geqslant -\mathbb{E}_{q(x_0)}\log p_{\theta}(x_0)</script><p>Further:</p>
<p><img src="D:\blog\source\images\385.png" alt=""></p>
<p>The first term here doesn’t depend on $\theta$, so we don’t need to consider about it, the last term is easy to optimise and can be combined with the second term. </p>
<p>Now what we need to do is to train the reverse process and find $\mu_{\theta}$ , $\Sigma_{\theta}$ to minimise the upper bound:</p>
<script type="math/tex; mode=display">
\mathbb{E}_q(\sum_{t=2}^TD_{KL}(q(x_{t-1}|x_t, x_0)||p_{\theta}(x_{t-1}
|x_t))-\log p_{\theta}(x_0|x_1))</script><p>In the paper, $\Sigma_{\theta}$ is setted as a contant, which is related to $(1-\alpha_t)$, so we don’t have to train it. What we need to do for now is to train a mean value.</p>
<p>So here, we let:</p>
<script type="math/tex; mode=display">
\Sigma_{\theta}(x_t, t)=\sigma_t^2I \qquad where \qquad \sigma_t^2=
\frac{1-\overline{\alpha}_{t-1}}{1-\overline{\alpha}_t}(1-\alpha_t)</script><p>The variance of  $p_{\theta}(x_{t-1}|x_t)$  is related to the variance of $q(x_{t-1}|x_t,x_0)$. </p>
<p>Let’s keep going, now the function is:</p>
<script type="math/tex; mode=display">
\mathbb{E}_q[\frac{1}{2\sigma_t^2}|| \widetilde{\mu}_t(x_t, x_0)-\mu_{\theta}(x_t, t) ||^2]+C</script><p>Throught the calculation above, we got the expression for $\widetilde{\mu}_t(x_t, x_0)$ :</p>
<script type="math/tex; mode=display">
\widetilde{\mu}_t = \frac{1}{\sqrt{\alpha_t}}(x_t-\frac{1-\alpha_t}{\sqrt{
 1-\overline{\alpha}_t
}}z_t)</script><p>By using Renormalization, we have:</p>
<script type="math/tex; mode=display">
\mu_{\theta}(x_t, t)=\frac{1}{\sqrt{\alpha_t}}(x_t-\frac{(1-\alpha_t)}{\sqrt{
    1-\overline{\alpha}_t
}}z_{\theta}(x_t, t)</script><p>and now we have:</p>
<script type="math/tex; mode=display">
\mathbb{E}[\frac{(1-\alpha_t)^2}{2\sigma_t^2\alpha_t(1-\overline{\alpha}_t)}
||z_t-z_{\theta}(x_t, t)||^2
]+C</script><p><em>(This is the formula that surprised me the most. Every step along the way makes sense, but here, is it actually minimizing the distance between the noise and the model? ? ? ? I still need some time to understand this step, I always feel weird)</em></p>
<p>Then the author found that if the coefficients are dropped, the training will be more stable and better, so we have:</p>
<script type="math/tex; mode=display">
L_{simple}(\theta)=\mathbb{E}_{t, x_0, z}[||z-z_{\theta}(\sqrt{\overline{
    \alpha
}_t}x_0+\sqrt{1-\overline{
    \alpha
}_t}z,t)||^2]</script><p>This is what we need to optimize!</p>
<h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><p>The code part can refer to this: <a target="_blank" rel="noopener" href="https://colab.research.google.com/drive/1XmvnGgWg1GY8HSVscAA_LKJfXWXtWR1y?usp=sharing">Google Colab</a></p>
<p><em>(Note: I didn’t add any comments, because the parameter names are basically the same as the names of the formulas in the blog. Comments may be added later)</em></p>
<p>The code effect is shown in the figure below:</p>
<p><img src="../images/difussion.gif" alt=""></p>
<h3 id="Additions-wiener-process"><a href="#Additions-wiener-process" class="headerlink" title="Additions: wiener process"></a>Additions: wiener process</h3><p><em>(If you’re not interested in delving into the details, you can skip this part. For understanding the diffusion probability model, it is sufficient to grasp the concept that “the Wiener process introduces (continuous) randomness with independent Gaussian increments.”)</em></p>
<p>The Wiener process is often associated with Brownian motion, and these two concepts can be considered equivalent. However, some sources argue that the Wiener process is a standard form of Brownian motion. Here, we won’t delve into the precise definitions of these terms but focus more on the properties of the Wiener process.</p>
<p>The Wiener process is a <strong>stochastic process, and its basic probability distribution at any given time $t$ follows a normal distribution with a mean of 0 and a variance of</strong> $t$. That is:</p>
<script type="math/tex; mode=display">
W(t)\sim \mathcal{N}(0, t)</script><p>Mean is an extremely important property of distribution because it tells you where the center of the distribution is.</p>
<p>Another important property of the Wiener process is that <strong>each increment of the process is also normally distributed</strong>, that is:</p>
<script type="math/tex; mode=display">
W(t)-W(s)\sim \mathcal{N}(0, t-s)</script><p>Where $t$ and $s$ are two distinct time points, with $t&gt;s$.</p>
<p>In addition to the mentioned properties, the Wiener process has several other important characteristics:</p>
<ul>
<li><p>Variance $(W(t)-W(s))=t-s$</p>
</li>
<li><p>Covariance $(W(t), W(s))=Min(t,s)$</p>
</li>
<li><p>Wiener process is a Markov Process</p>
</li>
<li><p>Wiener process is continuous, not differentiable</p>
</li>
</ul>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>[1] <a target="_blank" rel="noopener" href="https://medium.com/fintechexplained/what-is-brownian-motion-36de732b1645">What Is Brownian Motion?. Explaining Wiener Process — A Must-Know… | by Farhad Malik | FinTechExplained | Medium</a></p>
<p>[2] <a target="_blank" rel="noopener" href="https://towardsdatascience.com/understanding-diffusion-probabilistic-models-dpms-1940329d6048">Understanding Diffusion Probabilistic Models (DPMs) | by Joseph Rocca | Towards Data Science</a></p>
<p>[3] <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1503.03585.pdf">Deep Unsupervised Learning using Nonequilibrium Thermodynamics (arxiv.org)</a></p>
<p>[4] <a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1b541197HX/?spm_id_from=333.880.my_history.page.click&amp;vd_source=60a30fcab490c05a3a49048140bf284a">54、Probabilistic Diffusion Model概率扩散模型理论与完整PyTorch代码详细解读_哔哩哔哩_bilibili</a></p>

    </div>

    <div>
      
        <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">------ 本文结束------</div>
    
</div>
      
    </div>


    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/posts/39.html" rel="prev" title="Summary — “Continual learning strategies for cancer-independent detection of lymphnode metastases”">
      <i class="fa fa-chevron-left"></i> Summary — “Continual learning strategies for cancer-independent detection of lymphnode metastases”
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Diffusion-process-and-Reverse-process"><span class="nav-number">2.</span> <span class="nav-text">Diffusion process and Reverse process</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Diffusion-process"><span class="nav-number">2.1.</span> <span class="nav-text">Diffusion process</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reverse-process"><span class="nav-number">2.2.</span> <span class="nav-text">Reverse process</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Code"><span class="nav-number">3.</span> <span class="nav-text">Code</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Additions-wiener-process"><span class="nav-number">3.0.1.</span> <span class="nav-text">Additions: wiener process</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Reference"><span class="nav-number">4.</span> <span class="nav-text">Reference</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="super wzl"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">super wzl</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">36</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        


<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">super wzl</span>
</div>
<!--
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>
  <div class="addthis_inline_share_toolbox">
    <script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-611ffbcfffca5ad1" async="async"></script>
  </div>
-->

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="/lib/three/three.min.js"></script>
    <script defer src="/lib/three/canvas_lines.min.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'JbTCzSGo7EeV9tTlVHeWHuWx-MdYXbMMI',
      appKey     : '3yO7D6U6pVQy4RVNcsKIxy4m',
      placeholder: "来唠两句吗？",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
