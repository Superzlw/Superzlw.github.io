<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Diffusion Probabilistic Models (DPMs)</title>
    <url>/posts/42.html</url>
    <content><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>The concept of the diffusion probability model was initially proposed by Jascha Sohl-Dickstein et al. in 2015. However, due to limitations in hardware devices such as memory at that time, this model did not receive much attention. Thanks to the development of technology, especially the advancements in GPUs and memory devices in recent years, the diffusion model has started to gain recognition. </p>
<span id="more"></span>
<p>In the past two years, the diffusion model has emerged as a generative model, and its performance has amazed people, leading to a significant increase in research efforts and the production of high-quality papers.</p>
<p>The graph below illustrates the increasing level of attention the diffusion model has received in 2022, showcasing its importance:</p>
<p><img src="../images/379.png" alt=""></p>
<p>Understanding the classic paper on diffusion probability models is essential for anyone interested in working with such models. Unlike other neural networks, diffusion probability models involve a significant amount of mathematical background knowledge. This blog post aims to explain the most crucial parts of the model using simple language.</p>
<p>Before diving in, I would like to express my gratitude for the development of the Internet, which has allowed me to easily access high-quality blogs and videos. These resources have played a vital role in helping me understand diffusion probability models. The references to these resources are provided at the end of this post. If you find any inaccuracies or areas that need improvement in my content, please feel free to contact me via email. High-quality communication is the foundation of progress.</p>
<h1 id="Diffusion-process-and-Reverse-process"><a href="#Diffusion-process-and-Reverse-process" class="headerlink" title="Diffusion process and Reverse process"></a>Diffusion process and Reverse process</h1><p>The diffusion probability model consists of two main components: the diffusion process and the reverse diffusion process. To simplify the explanation (although not entirely accurate), we can illustrate it using an image. In the diffusion process, noise is continually added to the image until it becomes a completely noisy image, as shown in the figure below:</p>
<p><img src="../images/380.png" alt=""></p>
<p><em>The data distribution (left) undergoes Gaussian diffusion, which gradually transforms it into an identity-covariance Gaussian (right)</em></p>
<p>On the other hand, the reverse diffusion process involves obtaining the target image from a noisy image, as depicted in the figure below:</p>
<p><img src="../images/381.png" alt=""></p>
<p><em>An identity-covariance Gaussian (right) undergoes a Gaussian diffusion process with learned mean and covariance functions, and is gradually transformed back into the data distribution (left).</em></p>
<h2 id="Diffusion-process"><a href="#Diffusion-process" class="headerlink" title="Diffusion process"></a>Diffusion process</h2><p><em>“In probability theory and statistics, diffusion processes are a class of continuous-time Markov process with almost surely continuous sample paths.”</em></p>
<p align="right">---- Wikipedia</p>

<p>This definition is like a master of ambiguity, saying everything and yet saying nothing at all, let’s attempt to dissect this definition and break it down into simpler terms.</p>
<p><strong>First is the <em>Markov process</em>.</strong> A Markov process is characterized by its memorylessness property, which means that the conditional probability of the process depends only on its current state and is independent of its past history or future states:</p>
<script type="math/tex; mode=display">
p(X_{t_n}|X_{t_{n-1}}, ... , X_{t_0})=p(X_{t_n}|X_{t_{n-1}})\qquad \forall t_0 <t_1<...<t_{n-1}<t_n</script><p><strong>Then is <em>continuous sample paths</em>.</strong> If all possible observed trajectories of a process can be observed as continuous, we say that it has continuous sample paths. This means that the process does not exhibit abrupt jumps or discontinuities in its observed behavior over time.</p>
<p>Any diffusion process can be described using a stochastic differential equation (SDE) in the following form:</p>
<script type="math/tex; mode=display">
dX_t = a(X_t, t)dt+\sigma(X_t, t)dWt</script><p>where $a$ is the drift coefficient, $\sigma$ is the diffusion coefficient and W is the Wiener process. And the Wiener process introduces (continuous) randomness with independent Gaussian increments, that is:</p>
<script type="math/tex; mode=display">
dW_t\approx W_{t+dt}-W_t \sim \mathcal{N}(0, dt)</script><p>In an alternative representation, difussion function can be written as:</p>
<script type="math/tex; mode=display">
X_{t+dt}-X_t \approx a(X_t,t)dt+\sigma(X_t,t)U \qquad where\quad U\sim \mathcal{N}(0,dt)</script><p>that is:</p>
<script type="math/tex; mode=display">
X_{t+dt}\approx X_t+a(X_t, t)dt+U' \qquad where \quad U'\sim \mathcal{N}(0,\sigma^2(X_t,t)dt)</script><p>As we can observe, the state at the next time step is equal to the state at the current time step plus a deterministic drift term and a stochastic diffusion term. The diffusion term is defined by a normal random variable, with the variance proportional to the square of the diffusion coefficient. Therefore, the diffusion coefficient represents the intensity of the randomness to be applied.</p>
<p>Here, let’s borrow an illustration from Joseph’s blog to provide a more visual explanation of the effects of the drift term and the diffusion term:</p>
<p><img src="../images/382.png" title="" alt="" width="632"></p>
<p>From this, we can observe that the drift term provides a trend or direction, while the diffusion term introduces randomness. A higher diffusion coefficient implies greater randomness or variability in the process. In other words, a larger diffusion coefficient leads to increased levels of random fluctuations, making the process more unpredictable and volatile.</p>
<p><strong>Additionally, when the absolute value of the drift coefficient is less than 1 ($|a|&lt;1$), it is referred to as the contracting drift coefficient.</strong></p>
<p>When a diffusion process has a “contracting” drift coefficient and a non-zero diffusion coefficient, it can gradually transform data from a complex distribution to a set of independent Gaussian noises. </p>
<p>Here, let’s take the example of one-dimensional data, where the drift coefficient and diffusion coefficient are represented by $\sqrt{1-p}$ and $\sqrt{p}$  respectively ($p &lt;1$). Although this is not a rigorous proof, it provides an intuitive understanding of the concept:</p>
<script type="math/tex; mode=display">
\begin{aligned}
X_T&=\sqrt{1-p}X_{T-1}+\sqrt{p}u_T \\
   &= (\sqrt{1-p})^2X_{T-2}+\sqrt{1-p}\sqrt{p}u_{T-1}+\sqrt{p}u_T \\
&= ... \\
&= (\sqrt{1-p})^TX_0+\sum^{T-1}_{i=0}\sqrt{p}(\sqrt{1-p})^iu_{T-i}    \\
&= (\sqrt{1-p})^TX_0+\sqrt{p}\sum^{T-1}_{i=0}(\sqrt{1-p})^iu_{T-i}\\
&= (\sqrt{1-p})^TX_0+\sqrt{p}\sum^{T-1}_{i=0}\mathcal{N}(0,(1-p)^i\sigma)\\
&= (\sqrt{1-p})^TX_0+\sqrt{p}\mathcal{N}(0,\frac{1-(1-p)^T}{1-(1-p)}\sigma)\\
&= (\sqrt{1-p})^TX_0+\sqrt{1-(1-p)^T}\mathcal{z}_T\\
\end{aligned}</script><p>where $\mathcal{z}$ is Gussian Distribution $\mathcal{N}(0, \sigma)$.</p>
<p>Let’s further set $\sigma$ as $I$, and $(1-p)^T$ as $\overline{\alpha}_T$. that is:</p>
<script type="math/tex; mode=display">
X_T = \mathcal{N}(X_T;\sqrt{\overline{\alpha}_T}X_0,(1-\overline{\alpha}_T)I)</script><p>Let’s write it in a more general form (just for the sake of elegance):</p>
<script type="math/tex; mode=display">
q(x_t|x_0)=\mathcal{N}(x_t;\sqrt{\overline{\alpha}_t}x_0,(1-\overline{\alpha}_t)I)</script><p>Now we have a relationship between $x_0$ and $x_t$.</p>
<p>According to the properties of the Wiener process, all the Gaussians in the diffusion term are independent. Therefore, as $T$ approaches infinity, we can obtain the following result:</p>
<script type="math/tex; mode=display">
(\sqrt{1-p})^T \stackrel{T\rightarrow \infty}{\rightarrow}  0</script><script type="math/tex; mode=display">
\sqrt{1-(1-p)^T} \stackrel{T\rightarrow\infty}{\rightarrow}1</script><p>(<strong>Note:</strong> Always remember, this is a Wiener process, and the Gaussians are independent.)</p>
<p>That is:</p>
<blockquote>
<p>Given a sufficiently long period of time or a sufficiently large number of iterations, we can transform any initial input into a standard normal distribution.</p>
</blockquote>
<p>Although this is 1-D situation, but the same reasoning can be extended to multidimensional data, such as images. In the context of diffusion processes in higher dimensions, such as image data, the concept of independent Gaussians still holds. Each pixel or voxel in the image can be considered as a random variable, and the diffusion process introduces independent Gaussian fluctuations to each of these variables. As time progresses, the diffusion and drift terms act collectively to transform the initial image distribution into a set of independent Gaussian distributions at each pixel or voxel. This property is crucial in various applications, including image denoising, segmentation, and other image processing tasks based on diffusion probability models.</p>
<p>We performed a series of calculations based on 1-D above to illustrate that the following mathematical form</p>
<script type="math/tex; mode=display">
X_T=\sqrt{1-p}X_{T-1}+\sqrt{p}u_T</script><p>is reasonable, that is, the input signal can be transformed into an independent Gaussian distribution through a series of operations. The same form is also applicable in high dimensions, but Another question, is p always the same? Obviously not, p is always changing (the reason will be mentioned later), so we can write a more general form:</p>
<script type="math/tex; mode=display">
q(x_t|x_{t-1})=\mathcal{N}(x_t;\sqrt{1-p_t}x_{t-1},p_tI)</script><p>$p_t$ indicates at each step the trade-off between information to be kept from the previous step and noise to be added.</p>
<p>And in other form (The following formula is easy to prove, which is omitted here):</p>
<script type="math/tex; mode=display">
q(x_t|x_0)=\mathcal{N}(x_t;\sqrt{\overline{\alpha}_t}x_0,(1-\overline{\alpha}_t)I)</script><script type="math/tex; mode=display">
q(x_t|x_{t-1})=\mathcal{N}(x_t;\sqrt{\alpha_t}x_{t-1},(1-\alpha_t)I)</script><p>where:</p>
<script type="math/tex; mode=display">
\alpha_t=(1-p_t) \qquad and \qquad \overline{\alpha}_t=\prod_{t=1}^{t}\alpha
_t=\prod_{t=1}^t(1-p_t) \qquad and\qquad 
\{\alpha_t \in (0,1)\}_{t=1}^T</script><p>This is the essence of the diffusion process: using it, we can gradually transform data from a complex distribution into isotropic Gaussian noise. This process is relatively straightforward. However, if we want to reverse this process and transform simple distributions into complex ones, it becomes very challenging. Starting from a noisy image, we are unable to recover the underlying structure. </p>
<p><img src="../images/383.png" alt=""></p>
<p>Diffusion provides us with a progressive and structured approach to transform data from complex distributions to isotropic Gaussian noise.</p>
<p><img src="../images/384.png" alt=""></p>
<p>(<strong>Note:</strong> <em>Generally, in setting the diffusion coefficient, it is common to gradually decrease the coefficient over time. This approach is often referred to as annealing or annealed diffusion.</em></p>
<p><em>The purpose of reducing the diffusion coefficient over time is to control the rate of diffusion and achieve a more controlled and structured restoration process. By starting with a higher diffusion coefficient, the restoration process initially allows for more randomness and exploration in the diffusion, which helps to break down complex distributions. As the diffusion progresses, the coefficient is gradually reduced, leading to a decrease in the amount of randomness and emphasizing the preservation of important structures and details</em>.)</p>
<h2 id="Reverse-process"><a href="#Reverse-process" class="headerlink" title="Reverse process"></a>Reverse process</h2><p>There is an interesting question: While theoretically, it is possible to directly achieve the final image restoration from a noisy image by combining multiple recovery steps into one large step, there are several reasons why it is not practical or necessary to do so. Some of the main reasons are:</p>
<ol>
<li><p>Computational Complexity: Performing a single large step that encompasses all the recovery operations can be computationally intensive and time-consuming. It may require significantly more resources and processing power, making it impractical for real-time or resource-constrained applications.</p>
</li>
<li><p>Information Loss: Each intermediate step in the progressive recovery process provides valuable information and constraints that guide the restoration process. By merging all the steps into one, we might lose this valuable information, resulting in a suboptimal or less accurate final result.</p>
</li>
<li><p>Convergence and Stability: The progressive nature of the step-by-step recovery allows for better convergence and stability. It enables the algorithm to refine the restoration iteratively, gradually improving the quality of the image. By attempting to accomplish the entire process in one large step, we may encounter convergence issues or unstable behavior, leading to inferior results.</p>
</li>
<li><p>Interpretability and Control: Breaking down the recovery process into smaller steps allows for better interpretability and control. Each step can be individually analyzed, adjusted, or optimized based on specific requirements or prior knowledge. It provides a more fine-grained approach to understand and address the challenges in the restoration process.</p>
</li>
<li><p>Each recovery step in the progressive restoration process may not be entirely different from the others. It is possible to use a single model with different parameters to represent each recovery step. This approach allows us to leverage the shared knowledge and capabilities of the model while adapting its behavior to different stages of the restoration process.</p>
</li>
</ol>
<p>Overall, while it may seem feasible to combine all the recovery steps into a single large step, the step-by-step approach offers practical advantages in terms of computational efficiency, information preservation, convergence, stability, interpretability, and control.</p>
<p>And now back to our main part: reverse process. In the process of recovering the original data from Gaussian noise, we can make the assumption that it follows a Gaussian distribution as well. However, we can’t simply fit the distribution step by step (well, we could try using a Gaussian Mixture Model, but let’s not get into that complexity). Instead, we need to construct a parameterized distribution to make estimations. Reverse process is also a process of Markov chain.</p>
<p>That is:</p>
<script type="math/tex; mode=display">
p_{\theta}(X_{T-1}|X_T)=\mathcal{N}(X_{T-1}; \mu_{\theta}(X_T, T),\Sigma_{\theta}(X_T,T))</script><p>$\mu_{\theta}$ and $\Sigma_{\theta}$ represent two networks respectively. </p>
<p>Now let’s review the diffusion process again, but this time we focus on the posterior probability. </p>
<p>Diffusion Conditional Probability of the Posterior $q(X_{T-1}|X_T, X_0)$ can be expressed by formul, that means, we can cauculate $X_{T-1}$ given $X_T$ and $X_0$.</p>
<p>(<strong>Note</strong>: $X_0$ is required here)</p>
<p>Assume:</p>
<script type="math/tex; mode=display">
q(X_{T-1}|X_T,X_0)=\mathcal{N}(X_{T-1};\widetilde{\mu}(X_T,X_0),\widetilde{\beta}_TI)</script><p>Using Bayes’ rule, we have:</p>
<script type="math/tex; mode=display">
\begin{aligned}
q(X_{T-1}|X_T,X_0)&=q(X_T|X_{T-1},X_0)\frac{q(X_{T-1}|X_0)}{q(X_T|X_0)}\\
                  &\propto \exp(-\frac{1}{2}(\frac{(X_T-\sqrt{\alpha_T}X_{T-1})^2}{1-\alpha_T}+\frac{(X_{T-1}-\sqrt{\overline{\alpha}_{T-1}}X_0)^2}{1-\overline{\alpha}_{T-1}}-
\frac{(X_{T}-\sqrt{\overline{\alpha}_T}X_0)^2}{1-\overline{\alpha}_T}))\\
&= \exp(-\frac{1}{2}((\frac{\alpha_T}{1-\alpha_T}+\frac{1}{1-\overline{\alpha}_{T-1}})X_{T-1}^2
-(\frac{2\sqrt{\alpha_T}}{1-\alpha_T}X_T+\frac{2\sqrt{\overline{\alpha}_{T-1}}}{1-
\overline{\alpha}_{T-1}}X_0)X_{T-1}+C(X_T,X_0)))
\end{aligned}</script><p>In the midst of this journey, we encounter a series of steps involving the diffusion equation, the Markov chain, and the following relationship:</p>
<script type="math/tex; mode=display">
q(X_{1:T}|X_0)=\prod_{t=1}^Tq(X_t|X_{t-1})</script><script type="math/tex; mode=display">
ax^2+bx=a(x+\frac{b}{2a})^2+C</script><p>Probability Density Function of Gaussian Distribution:</p>
<script type="math/tex; mode=display">
f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}</script><p>Following the standard Gaussian density function, the variance $\widetilde{\beta}_t$ and the mean $\widetilde{\mu}_t(x_t,x_0)$ can be parameterized as follows ($t$ here is same as $T$ above):</p>
<script type="math/tex; mode=display">
\widetilde{\beta}_t=\frac{1}{\frac{\alpha_t}{1-\alpha_t}+\frac{1}{1
-\overline{\alpha}_{t-1}}}=\frac{1-\overline{\alpha}
_{t-1}}{1-\overline{\alpha}_t}(1-\alpha_t)</script><script type="math/tex; mode=display">
\widetilde{\mu}_t(x_t,x_0)=(\frac{\sqrt{\alpha_t}}{\beta_t}X_t+
\frac{\sqrt{\overline{\alpha}_t}}{1-\overline{\alpha}_t}X_0)/
(\frac{\alpha_t}{\beta_t}+\frac{1}{1-\overline{\alpha}_{t-1}})=
\frac{\sqrt{\alpha_t}(1-\overline{\alpha}_{t-1})}{1-\overline{\alpha}_t}X_t
+\frac{\sqrt{\overline{\alpha}_{t-1}}(1-\alpha_t)}{1-\overline{\alpha}_t}X_0</script><p>Based on the relationship between $x_0$ and $x$ mentioned earlier, we have:</p>
<script type="math/tex; mode=display">
X_0=\frac{1}{\sqrt{\overline{\alpha}_T}}(X_T-\sqrt{1-\overline{\alpha}_T}\mathcal{z}_T)</script><p>We insert the $X_0$ here into $q(X_{T-1}|X_T, X_0)$ , then we could have a new mean-value-expression(the process is very complex and typing the formula here is also too troublesome…). But we could know that, there is no more $X_0$ here but occurs a noise-term, and base on it, we could design a Neural Network. </p>
<p>the result is(<em>Intermediate steps are omitted</em>.):</p>
<script type="math/tex; mode=display">
\widetilde{\mu}_t = \frac{1}{\sqrt{\alpha_t}}(X_t-\frac{1-\alpha_t}{\sqrt{
 1-\overline{\alpha}_t
}}z_t)</script><p>Now let’s put them aside, and back to our flow. If we want to optimize the network, one of the most important thing is: Loss Function. And since we focus on the reverse process, that is, we want to optimize the reverse process, so the function must be related to $\mu_{\theta}$ and $\Sigma_{\theta}$.  But here we use another form: $-\log p_{\theta}(x_0)$, and we plus a KL divergence to build the upper bound(since KL divergence is always great than 0):</p>
<script type="math/tex; mode=display">
\begin{aligned}

-\log p_{\theta}(x_0) &\leqslant -\log p_{\theta}(x_0)+D_{KL}(q(x_{1:T}|x_0)
||p_{\theta}(x_{1:T}|x_0)) \\
&= -\log p_{\theta}(x_0) + \mathbb{E}_{x_{1:T}\sim q(x_{1:T}|x_0)}[\log \frac
{q(x_{1:T}|x_0)}{p_{\theta}(x_{0:T})/p_{\theta}(x_0)}]\\
&= -\log p_{\theta}(x_0) + \mathbb{E}_q[\log \frac{q(x_{1:T}|x_0)}{p_{\theta}(x_{0:T})}
+\log p_{\theta}(x_0)]\\
&= \mathbb{E}_q[\log \frac{q(x_{1:T}|x_0)}{p_{\theta}(x_{0:T})}
]

\end{aligned}</script><p>Let:</p>
<script type="math/tex; mode=display">
L_{VLB}=\mathbb{E}_{q(x_{0:T})}[\log \frac{q(x_{1:T}|x_0)}{p_{\theta}(x_{0:T})}]
\geqslant -\mathbb{E}_{q(x_0)}\log p_{\theta}(x_0)</script><p>Further:</p>
<p><img src="D:\blog\source\images\385.png" alt=""></p>
<p>The first term here doesn’t depend on $\theta$, so we don’t need to consider about it, the last term is easy to optimise and can be combined with the second term. </p>
<p>Now what we need to do is to train the reverse process and find $\mu_{\theta}$ , $\Sigma_{\theta}$ to minimise the upper bound:</p>
<script type="math/tex; mode=display">
\mathbb{E}_q(\sum_{t=2}^TD_{KL}(q(x_{t-1}|x_t, x_0)||p_{\theta}(x_{t-1}
|x_t))-\log p_{\theta}(x_0|x_1))</script><p>In the paper, $\Sigma_{\theta}$ is setted as a contant, which is related to $(1-\alpha_t)$, so we don’t have to train it. What we need to do for now is to train a mean value.</p>
<p>So here, we let:</p>
<script type="math/tex; mode=display">
\Sigma_{\theta}(x_t, t)=\sigma_t^2I \qquad where \qquad \sigma_t^2=
\frac{1-\overline{\alpha}_{t-1}}{1-\overline{\alpha}_t}(1-\alpha_t)</script><p>The variance of  $p_{\theta}(x_{t-1}|x_t)$  is related to the variance of $q(x_{t-1}|x_t,x_0)$. </p>
<p>Let’s keep going, now the function is:</p>
<script type="math/tex; mode=display">
\mathbb{E}_q[\frac{1}{2\sigma_t^2}|| \widetilde{\mu}_t(x_t, x_0)-\mu_{\theta}(x_t, t) ||^2]+C</script><p>Throught the calculation above, we got the expression for $\widetilde{\mu}_t(x_t, x_0)$ :</p>
<script type="math/tex; mode=display">
\widetilde{\mu}_t = \frac{1}{\sqrt{\alpha_t}}(x_t-\frac{1-\alpha_t}{\sqrt{
 1-\overline{\alpha}_t
}}z_t)</script><p>By using Renormalization, we have:</p>
<script type="math/tex; mode=display">
\mu_{\theta}(x_t, t)=\frac{1}{\sqrt{\alpha_t}}(x_t-\frac{(1-\alpha_t)}{\sqrt{
    1-\overline{\alpha}_t
}}z_{\theta}(x_t, t)</script><p>and now we have:</p>
<script type="math/tex; mode=display">
\mathbb{E}[\frac{(1-\alpha_t)^2}{2\sigma_t^2\alpha_t(1-\overline{\alpha}_t)}
||z_t-z_{\theta}(x_t, t)||^2
]+C</script><p><em>(This is the formula that surprised me the most. Every step along the way makes sense, but here, is it actually minimizing the distance between the noise and the model? ? ? ? I still need some time to understand this step, I always feel weird)</em></p>
<p>Then the author found that if the coefficients are dropped, the training will be more stable and better, so we have:</p>
<script type="math/tex; mode=display">
L_{simple}(\theta)=\mathbb{E}_{t, x_0, z}[||z-z_{\theta}(\sqrt{\overline{
    \alpha
}_t}x_0+\sqrt{1-\overline{
    \alpha
}_t}z,t)||^2]</script><p>This is what we need to optimize!</p>
<h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><p>The code part can refer to this: <a href="https://colab.research.google.com/drive/1XmvnGgWg1GY8HSVscAA_LKJfXWXtWR1y?usp=sharing">Google Colab</a></p>
<p><em>(Note: I didn’t add any comments, because the parameter names are basically the same as the names of the formulas in the blog. Comments may be added later)</em></p>
<p>The code effect is shown in the figure below:</p>
<p><img src="../images/difussion.gif" alt=""></p>
<h1 id="Additions"><a href="#Additions" class="headerlink" title="Additions"></a>Additions</h1><p>The content of this part can be ignored, but it is helpful to understand the derivation process of DPM.</p>
<h2 id="wiener-process"><a href="#wiener-process" class="headerlink" title="wiener process"></a>wiener process</h2><p><em>(If you’re not interested in delving into the details, you can skip this part. For understanding the diffusion probability model, it is sufficient to grasp the concept that “the Wiener process introduces (continuous) randomness with independent Gaussian increments.”)</em></p>
<p>The Wiener process is often associated with Brownian motion, and these two concepts can be considered equivalent. However, some sources argue that the Wiener process is a standard form of Brownian motion. Here, we won’t delve into the precise definitions of these terms but focus more on the properties of the Wiener process.</p>
<p>The Wiener process is a <strong>stochastic process, and its basic probability distribution at any given time $t$ follows a normal distribution with a mean of 0 and a variance of</strong> $t$. That is:</p>
<script type="math/tex; mode=display">
W(t)\sim \mathcal{N}(0, t)</script><p>Mean is an extremely important property of distribution because it tells you where the center of the distribution is.</p>
<p>Another important property of the Wiener process is that <strong>each increment of the process is also normally distributed</strong>, that is:</p>
<script type="math/tex; mode=display">
W(t)-W(s)\sim \mathcal{N}(0, t-s)</script><p>Where $t$ and $s$ are two distinct time points, with $t&gt;s$.</p>
<p>In addition to the mentioned properties, the Wiener process has several other important characteristics:</p>
<ul>
<li><p>Variance $(W(t)-W(s))=t-s$</p>
</li>
<li><p>Covariance $(W(t), W(s))=Min(t,s)$</p>
</li>
<li><p>Wiener process is a Markov Process</p>
</li>
<li><p>Wiener process is continuous, not differentiable</p>
</li>
</ul>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>[1] <a href="https://medium.com/fintechexplained/what-is-brownian-motion-36de732b1645">What Is Brownian Motion?. Explaining Wiener Process — A Must-Know… | by Farhad Malik | FinTechExplained | Medium</a></p>
<p>[2] <a href="https://towardsdatascience.com/understanding-diffusion-probabilistic-models-dpms-1940329d6048">Understanding Diffusion Probabilistic Models (DPMs) | by Joseph Rocca | Towards Data Science</a></p>
<p>[3] <a href="https://arxiv.org/pdf/1503.03585.pdf">Deep Unsupervised Learning using Nonequilibrium Thermodynamics (arxiv.org)</a></p>
<p>[4] <a href="https://www.bilibili.com/video/BV1b541197HX/?spm_id_from=333.880.my_history.page.click&amp;vd_source=60a30fcab490c05a3a49048140bf284a">54、Probabilistic Diffusion Model概率扩散模型理论与完整PyTorch代码详细解读_哔哩哔哩_bilibili</a></p>
]]></content>
      <categories>
        <category>学习总结</category>
      </categories>
  </entry>
  <entry>
    <title>Summary — “Continual learning strategies for cancer-independent detection of lymphnode metastases”</title>
    <url>/posts/39.html</url>
    <content><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>update later……</p>
]]></content>
      <categories>
        <category>学习总结</category>
      </categories>
  </entry>
  <entry>
    <title>Summary — “CaDIS - Cataract Dataset for Image Segmentation”</title>
    <url>/posts/37.html</url>
    <content><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>This blog post is a summary of a dataset with information that may be useful in current projects. And the paper is:</p>
<p><a href="https://arxiv.org/pdf/1906.11586.pdf">CaDIS: Cataract Dataset for Image Segmentation</a></p>
<span id="more"></span>
<p>This blog contains the following contents:</p>
<ul>
<li><p>Dataset</p>
</li>
<li><p>Experiments</p>
</li>
<li><p>Problems of this dataset</p>
</li>
</ul>
<p><strong>Note: This blog represents only the <code>Information extraction</code>.</strong></p>
<h1 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h1><h2 id="Annotation-process"><a href="#Annotation-process" class="headerlink" title="Annotation process"></a>Annotation process</h2><p>Some points to note in the annotation:</p>
<ul>
<li><p>The number of pixels for each class is greater than 50;</p>
</li>
<li><p>Annotation errors may originate from: blurred boundaries caused by motion. In addition, specular reflections may also lead to inaccurate boundary delineation, especially for the instrument tips when they are inside the anatomy.</p>
</li>
</ul>
<h2 id="Dataset-statistics"><a href="#Dataset-statistics" class="headerlink" title="Dataset statistics"></a>Dataset statistics</h2><p>The dataset includes 36 classes: 29 surgical instrument classes, 4 anatomy classes and 3 miscellaneous classes, namely:<br><img src="/images/365.png" alt=""></p>
<p>Classes are imbalanced: </p>
<blockquote>
<p>The anatomy classes appear more frequently than the surgical instruments. The anatomy also covers the largest part of the scene.</p>
<p>The classes about “Instrument” in dataset is highly imbalanced, for example, there are 17 instrument classes appear in less than half of the videos. So, accurate instrument classification is more challenging.</p>
<p>Some Instruments have high similarity but completely different functions, such as “cannulas”, which is also a difficulty of this dataset.</p>
</blockquote>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Three-different-tasks"><a href="#Three-different-tasks" class="headerlink" title="Three different tasks"></a>Three different tasks</h2><p>Three different tasks are proposed in this dataset, that is, classes are merged in three different ways.</p>
<h3 id="Task-1-8-classes"><a href="#Task-1-8-classes" class="headerlink" title="Task 1 (8 classes):"></a>Task 1 (8 classes):</h3><p>The first task is focused on differentiating between anatomy and instruments within every frame. That is, here we only have 8 classes: 4 anatomy classes, 3 miscellaneous classes and 1 classes with all Instruments:<br><img src="/images/366.png" alt=""></p>
<h3 id="Task-2-17-classes"><a href="#Task-2-17-classes" class="headerlink" title="Task 2 (17 classes):"></a>Task 2 (17 classes):</h3><p>The second task is not only focuses on anatomy, but also classifies Instruments based on appearance similarity and instrument type. That is, this task is to identify anatomical structures and also the main types of instruments that appear in the scene. </p>
<p>The classes are as follow:<br><img src="/images/367.png" alt=""></p>
<p>The specific merged classes are as follows:</p>
<ul>
<li><p>Cannula</p>
<ul>
<li>Hydrosdissection Cannula; Viscoelastic Cannula; Rycroft Cannula; Rycroft Cannula Handle; Charleux Cannula; Hydrosdissection Cannula Handle</li>
</ul>
</li>
<li><p>Cap. Cystotome</p>
<ul>
<li>Capsulorhexis Cystotome; Capsulorhexis Cystotome Handle</li>
</ul>
</li>
<li><p>Tissue Forceps</p>
<ul>
<li>Bonn Forceps; Troutman Forceps</li>
</ul>
</li>
<li><p>Primary Knife</p>
<ul>
<li>Primary Knife; Primary Knife Handle</li>
</ul>
</li>
<li><p>Ph. Handpiece</p>
<ul>
<li>Phacoemulsifier Handpiece; Phacoemulsifier Handpiece Handle</li>
</ul>
</li>
<li><p>Lens Injector</p>
<ul>
<li>Lens Injector; Lens Injector Handle</li>
</ul>
</li>
<li><p>I/A Handpiece</p>
<ul>
<li>Irrigation/Aspiration (I/A) Handpiece; Irrigation/Aspiration Handpiece Handle</li>
</ul>
</li>
<li><p>Secondary Knife</p>
<ul>
<li>Secondary Knife; Secondary Knife Handle</li>
</ul>
</li>
<li><p>Micromanipulator</p>
<ul>
<li>Micromanipulator</li>
</ul>
</li>
<li><p>Cap. Forceps</p>
<ul>
<li>Capsulorhexis Forceps</li>
</ul>
</li>
<li><p>Ignore</p>
<ul>
<li>Suture Needle; Needle Holder; Vitrectomy Handpiece; Mendez Ring; Marker; Cotton; Iris Hooks</li>
</ul>
</li>
</ul>
<p><strong>Note</strong>: the instruments that only appear in the training set, cover relatively few pixels in the frame and cannot be merged with another instrument class were ignored during training. </p>
<h3 id="Task-3-25-classes"><a href="#Task-3-25-classes" class="headerlink" title="Task 3 (25 classes)"></a>Task 3 (25 classes)</h3><p>This task builds on Task 2 for a finer classification. In task 2, we can find that each Instrument and its corresponding Handle are merged together, and this task is to separate them.</p>
<p>The classes are as follow:<br><img src="/images/368.png" alt=""></p>
<ul>
<li>Ignore<ul>
<li>Suture Needle; Needle Holder; Charleux Cannula; Primary Knife Handle; Vitrectomy Handpiece; Mendez Ring; Marker; Hydrosdissection Cannula Handle; Troutman Forceps; Cotton; Iris Hooks</li>
</ul>
</li>
</ul>
<p><strong>Note</strong>: The classes that do not appear in all splits and are present in less than 5 videos were ignored during training.</p>
<h2 id="Models"><a href="#Models" class="headerlink" title="Models"></a>Models</h2><p>Three models are used in the paper, namely, <code>UNet</code>, <code>DeepLabV3+</code>, <code>UPerNet</code> and <code>HRNetV2</code>. (all the models are open-source)</p>
<h3 id="For-training"><a href="#For-training" class="headerlink" title="For training"></a>For training</h3><h4 id="Training-dataset"><a href="#Training-dataset" class="headerlink" title="Training dataset"></a>Training dataset</h4><p>The same data augmentation was applied for all model before training, they used: Each training image was <code>normalized</code>, <code>flipped</code>, <code>randomly rotated</code> and <code>hue</code> and <code>saturation</code> was also adjusted. The input images were <code>downsized</code> to 270 x 480.</p>
<p>No post-processing was performed.</p>
<h4 id="Training-configuration"><a href="#Training-configuration" class="headerlink" title="Training configuration"></a>Training configuration</h4><ul>
<li><p>pre-trainged weight</p>
<ul>
<li>the weights for <code>UPerNet</code> and <code>HRNetV2</code> were initialized using pre-trained weights on ImageNet;</li>
<li>the weights for <code>DeepLabV3+</code> was based on Pascal VOC;</li>
</ul>
</li>
<li><p>epochs, GPU</p>
<ul>
<li>100 epochs</li>
<li>two NVIDIA GTX 1080 Ti GPUs</li>
</ul>
</li>
<li><p>learning rate, optimizer</p>
<ul>
<li>the Cross Entropy loss function was used with learning rate equal to $10^{-4}$ using the Adam Optimizer.</li>
<li>The $\beta_1$, $\beta_2$ and $\epsilon$ values for the Adam Optimizer<br>were set to 0.9, 0.999 and $10^{-8}$, which are proposed as good</li>
</ul>
</li>
</ul>
<h4 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics"></a>Metrics</h4><p>The paper used 3 metrics to evaluate the models, namely, <code>PA</code>, <code>PAC</code>, <code>mIOU</code>, but because of the imbalanced of the dataset and classes (that is, some classes only appear in some videos’ frame, and the number of pixel of some classes are larger than others). So, we mainly focus on <code>mIOU</code>. </p>
<h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><h4 id="For-task-1"><a href="#For-task-1" class="headerlink" title="For task 1"></a>For task 1</h4><p>By using different models, the results based on task 1 are as follow:<br><img src="/images/369.png" alt=""></p>
<p>and <code>mIOU</code> per class is:<br><img src="/images/372.png" alt=""></p>
<p>From this chart we could find that:</p>
<ul>
<li><code>HRNetV2</code> presenting the highest <code>mIOU</code>;</li>
<li><code>DeepLabV3+</code> presenting the lowest for both validation and test sets;</li>
<li>There is small differences in the mIOU between the models because the imbalance among the classes is reduced by representing all instruments with one class.</li>
</ul>
<h4 id="For-task-2"><a href="#For-task-2" class="headerlink" title="For task 2"></a>For task 2</h4><p>By using different models, the results based on task 2 are as follow:<br><img src="/images/370.png" alt=""></p>
<p>and <code>mIOU</code> per class is:<br><img src="/images/371.png" alt=""></p>
<p>From this chart we could find that:</p>
<ul>
<li>The mIOU between the different models starts to differ significantly as the class Instruments starts to get imbalanced;</li>
<li>For large classes, such as the anatomical classes and instrument classes that are represented by large number of pixels, have high mIOU;</li>
<li>Cannula group of classes and the capsulorhexis cystotome have a relatively low mIoU, beccause these classes are represented by a small number of pixels and a large percentage of them is classified as anatomy around the boundary.</li>
<li>The capsulorhexis forceps present a low mIoU for all models and this is because they are frequently misclassified as tissue forceps, These two classes could have been merged into one group, however as the capsulorhexis forceps appear in all splits with sufficient number of training images, they are represented by a separate class.</li>
</ul>
<h4 id="For-task-3"><a href="#For-task-3" class="headerlink" title="For task 3"></a>For task 3</h4><p>By using different models, the results based on task 3 are as follow:<br><img src="/images/373.png" alt=""></p>
<p>and <code>mIOU</code> per class is:<br><img src="/images/374.png" alt=""></p>
<p>From this chart we could find that:</p>
<ul>
<li>As the number of classes increases, class imbalance is more evident;</li>
</ul>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><ul>
<li><code>DeepLabV3+</code>, <code>UPerNet</code> and <code>HRNet</code> achieve higher mIOU for instrument segmentation and classification than <code>UNet</code>;</li>
<li><code>UNet</code> gives a lower mIoU at instrument segmentation and classification;</li>
<li><code>UPerNet</code> and <code>HRNet</code> have the higher mIoU at simultaneous anatomy segmentation and instrument classification.</li>
</ul>
<h1 id="Additional"><a href="#Additional" class="headerlink" title="Additional"></a>Additional</h1><p>According to the finding of “RVIM Lab”, some annotations in the dataset are wrong, and at least 179 images were labeled wrong, and they(“RVIM Lab”) relabeled some of them. Such as the following examples:<br><img src="/images/375.jpg" alt=""></p>
<p>we could find that, the annotations of <code>Hand</code>  and <code>Micromanipulator</code> are wrong, the correct mask should be:<br><img src="/images/376.jpg" alt=""></p>
<p>Another example:<br><img src="/images/377.jpg" alt=""></p>
<p>the correct one should be:<br><img src="/images/378.jpg" alt=""></p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference:"></a>Reference:</h1><p>[1] <a href="https://arxiv.org/pdf/1906.11586.pdf">Paper: CaDIS: Cataract Dataset for Image Segmentation</a></p>
<p>[2] <a href="https://link.springer.com/chapter/10.1007/978-3-030-87202-1_49">Paper: Effective semantic segmentation in Cataract Surgery: What matters most?</a></p>
]]></content>
      <categories>
        <category>学习总结</category>
      </categories>
  </entry>
  <entry>
    <title>Summary — “Swin Transformer”</title>
    <url>/posts/36.html</url>
    <content><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>The swin transformer is proposed because the previous transformer has the following two shortcomings in the field of visual processing:</p>
<span id="more"></span>
<ul>
<li>Unlike NLP, the pixel resolution of an image is much larger than that of text information. If pixels are used as tokens, the computational complexity is unacceptable.</li>
<li>The objects in the image often have different scales, which is also not effectively handled by the previous viT, such as: instance segmentation in densely predicted scenes.</li>
</ul>
<p>In order to solve these problems, swin transformer has proposed some new solutions.<br>This blog mainly focuses on the following aspects:</p>
<ul>
<li>Patch Merging</li>
<li>Window Self-Attention</li>
<li>Shift Window Self-Attention</li>
<li>Relative Position Index</li>
</ul>
<p>And as repeatedly emphasized in the paper, the swin transformer not only solves the above two problems, but more importantly, the author proposes a general-purpose backbone network, although the performance of the unified model in the NLP and CV fields is not as good as viT, but Its future development is worth looking forward to.</p>
<p><strong>NOTICE: Reading this blog requires a basic understanding of viT.</strong></p>
<h1 id="Patch-Merging"><a href="#Patch-Merging" class="headerlink" title="Patch Merging"></a>Patch Merging</h1><p>As mentioned above, viT does not handle the problem of image multi-scale very well. Looking back at CNN, when dealing with multi-scale problems, we have a good method: FPN.<br>So how do we apply this idea to transformers? The author proposes Patch Merging. Simply explained, this is to achieve the CNN effect with a non-CNN method.Continuing to read the paper, we will find that the author built the network with the effect achieved by CNN as a template.</p>
<p>First, in terms of scale, the comparison between swin transformer and viT can be seen in the following figure:<br><img src="/images/349.png" alt=""></p>
<p>From the above figure we can see that the scale of the swin transformer increases step by step compared to the single scale of viT ($ 16 \times 16 $). With each increment, the feature map size is halved and the number of channels is doubled, which is very similar to CNN.</p>
<p>That is to say, the patch size at the beginning is $4\times 4$, followed by $8 \times 8$,<br>$16 \times 16$, and the receptive field is getting bigger and bigger, forming multiple scale.</p>
<p>Then we look at the structure of the entire network:<br><img src="/images/351.png" alt=""></p>
<p>If the input image is a 3-channel image, the size is $H \times W$, after the first layer, the<br>scale is reduced to 1/4, and the number of channels becomes $4\times 4 \times 3 = 96$, after the first layer stage, the scale remains unchanged, and the number of channels becomes C, where C depends on the size of the model, as shown in the following figure. After each stage, the scale is reduced by half and the number of channels is doubled.<br><img src="/images/352.png" alt=""></p>
<blockquote>
<p>Notice: Patch Partition + Linear Embedding in the figure has a similar effect as Patch Merging.</p>
</blockquote>
<h2 id="Detail-of-Patch-Merging"><a href="#Detail-of-Patch-Merging" class="headerlink" title="Detail of Patch Merging"></a>Detail of Patch Merging</h2><p>Now we know what Patch Merging does, but how does it work? Next I will use a simple example to illustrate.<br><img src="/images/353.png" alt=""><br>Here we have a feature map of $4 \times 4$, take a single channel as an example, divide it into 4 windows, take the patches at the same position in each window (marked with the same color here), and merge them to increase the number of channels, Then LayerNorm is performed, and finally a linear operation is performed to halve the number of channels. This is the whole process of patch-merging, which realizes the operation of halving the scale and doubling the number of channels.</p>
<h1 id="Window-Self-Attention-and-Shift-Window-Self-Attention"><a href="#Window-Self-Attention-and-Shift-Window-Self-Attention" class="headerlink" title="Window Self-Attention and Shift Window Self-Attention"></a>Window Self-Attention and Shift Window Self-Attention</h1><p>For the computational complexity mentioned above, the paper uses a window self-attention model. In order to connect different windows and achieve a global approach, a shift window self-attention model is proposed.</p>
<p>Take this picture in the paper as an example:<br><img src="/images/350.png" alt=""></p>
<h2 id="Window-Self-Attention-W-MSA"><a href="#Window-Self-Attention-W-MSA" class="headerlink" title="Window Self-Attention(W-MSA)"></a>Window Self-Attention(W-MSA)</h2><p>First look at the following figures. The entire image has $8\times 8$ patches, divided into 4 windows, each window has $4\times 4$ patches.<br><img src="/images/354.png" alt=""><br>If we use MSA, such as the left picture, we have to calculate the self-attention of one patch with all other patch, and if we use W-MSA, we only need to calculate the self-attention of each window. As the scale of the feature map increases, MSA is a squared increase, while W-MSA increases linearly.</p>
<p>The paper gave the calculation-functions for these two methods:<br><img src="/images/355.png" alt=""><br>Parameter Description:</p>
<ul>
<li>h: height of feature map</li>
<li>w: the width of the feature map</li>
<li>C: the number of channels of the feature map</li>
<li>M: size of a single window</li>
</ul>
<h3 id="Additional-Part-Computational-complexity"><a href="#Additional-Part-Computational-complexity" class="headerlink" title="Additional Part(Computational complexity)"></a>Additional Part(Computational complexity)</h3><p>This part is an additional part to explain how the above complexity calculation formula comes from.</p>
<p>Here we use <code>FOLPs</code> as the metric, and first we need to know the FOLPs of the matrix calculation. Such as $A^{m \times n}\times B^{n \times v}$, the FOLPs is: $m \times n \times v$.</p>
<p>Review the calculation of <code>Attention</code>:</p>
<script type="math/tex; mode=display">
Attention(Q,K,V) = SoftMax(\frac{QK^T}{\sqrt{d}})V</script><p>For each patch in the feature map, the corresponding q, k, v are generated through $W_q, W_k, W_v$,<br>It is assumed here that the vector lengths of q, k, v are consistent with the depth C of the feature map. Then the process of generating Q for all patches is as follows:</p>
<script type="math/tex; mode=display">
A^{hw\times C}\cdot W_q^{C\times C} = Q^{hw\times C}</script><p>The FOLPs is: $hw \times C \times C$, the same as K and V, with total of $3hwC^2$.</p>
<p>And the is $Q K^T$, the FOLPs is $(hw)^2C$, then ignore division by $sqrt{d}$ and $softmax$,<br>and finally, multiply by V, so we can obtain the following FOLPs:</p>
<script type="math/tex; mode=display">
3hwC^2+(hw)^2C+(hw)^2C = 3hwC^2+2(hw)^2C</script><p>Finally is the matrix about mutil-head $W_o$, the FOLPs is $hwC^2$.</p>
<p>So we get: $4hwC^2+2(hw)^2C$</p>
<p>For W-MSA, we can also get the function with a similar calculation process.</p>
<h2 id="Shift-Window-Self-Attention"><a href="#Shift-Window-Self-Attention" class="headerlink" title="Shift Window Self-Attention"></a>Shift Window Self-Attention</h2><p>Now comes the part that I think is the most interesting.</p>
<p>Although we significantly reduce the computational complexity through windowed self-attention, it also creates a problem: global information is lost. We only focus on attention within a window, and there is no connection between windows.</p>
<p>To solve this problem, the author proposes a new method: <strong>Shift Window Self-Attention</strong>. As shown below:<br><img src="/images/350.png" alt=""></p>
<p>After finishing the W-MSA on the left, we shift the image a few patches to the right and down, this amount of “Shift” depends on $\frac{|M|}{2}$, and then divide the image again.</p>
<p>In this way we can connect patches that were not in the same window before. But this creates a new problem: as shown on the right, the new feature map has 9 windows. If they are calculated separately, their sizes are different and cannot be calculated uniformly. If padding is used here to expand it to the same size, the self-attention of 9 windows needs to be calculated, which increases the computational complexity.</p>
<p>In order to unify the self-attention of the calculation window without increasing the computational complexity, the author proposes an interesting method: cyclically shift the window to achieve unified calculation and reduce the computational complexity.</p>
<p>As shown below:<br><img src="/images/356.png" alt=""><br>We spliced parts A, B, and C separately, taking the middle window as an example, after shifting, it can contact the original 4 windows.</p>
<p>But now a new problem has arisen. Although this method can keep the computational complexity from increasing, the window formed by shifting and splicing will cause errors if we directly calculate self-attention. Since they have moved from far away, the connection between the two should be minimal.</p>
<p>For example, if “C” stands for “sky”, we concatenate it with “ground” below, and the calculated self-attention is obviously inappropriate.</p>
<p>To avoid this error, the authors propose a new masking method.</p>
<p>Regarding this method, the author gave a detailed explanation in <code>issues #38</code> in his GitHub, as shown below:<br><img src="/images/357.png" alt=""><br>That is to say, in the required position, the mask value is 0, and in the unwanted position, the value is -100. Since the value after self-attention calculation is very small, after subtracting 100, through softmax, in the unwanted position, the value will approach 0 indefinitely.</p>
<p>Finally, after the calculation, it is necessary to restore the shifted part back to its original position.</p>
<h1 id="Relative-Position-Index"><a href="#Relative-Position-Index" class="headerlink" title="Relative Position Index"></a>Relative Position Index</h1><p>This part belongs to the optimization part of the project and is not required, but it can improve the performance of the model(as shown below).<br><img src="/images/358.png" alt=""></p>
<p>The process of building a relative position index is as follows:</p>
<p><strong>Step 1:</strong> Build a relative position matrix based on each patch, flatten, merge.<br><img src="/images/359.png" alt=""></p>
<p><strong>Step 2:</strong> The simple relative position index has actually been completed, but in the source code, in order to express more concisely, each index is represented by one-dimensional rather than two-dimensional coordinates. But how? If we directly add the two-dimension coordination together, then (0, -1) and (-1, 0) will have the same value.<br>In order to avoid this problem, author use the following method:</p>
<p>Add M-1 to the original relative position index (M is the size of the window, in this example M=2), after adding, there will be no negative numbers in the index.<br><img src="/images/360.png" alt=""></p>
<p><strong>Step 3:</strong> Then multiply all row values by 2M-1:<br><img src="/images/361.png" alt=""></p>
<p><strong>Step 4:</strong> Add row and column values:<br><img src="/images/362.png" alt=""></p>
<p><strong>Step 5:</strong> Finally get the value according to the index:<br><img src="/images/363.png" alt=""></p>
<p>Now the process of relative position index and value is completed.</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>[1] <a href="https://arxiv.org/abs/2103.14030">Paper: Swin Transformer</a></p>
<p>[2] <a href="https://www.bilibili.com/video/BV13L4y1475U?spm_id_from=333.999.header_right.history_list.click&amp;vd_source=60a30fcab490c05a3a49048140bf284a">Video-1: From Bilibili</a></p>
<p>[3] <a href="https://www.bilibili.com/video/BV1pL4y1v7jC?spm_id_from=333.999.0.0&amp;vd_source=60a30fcab490c05a3a49048140bf284a">Video-2: From Bilibili</a></p>
<p>[4] <a href="https://blog.csdn.net/qq_37541097/article/details/121119988?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522165568155116782395358199%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fblog.%2522%257D&amp;request_id=165568155116782395358199&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_ecpm_v1~rank_v31_ecpm-1-121119988-null-null.nonecase&amp;utm_term=swin&amp;spm=1018.2226.3001.4450">Blog-1: From CSDN</a></p>
]]></content>
      <categories>
        <category>学习总结</category>
      </categories>
  </entry>
  <entry>
    <title>Summary - “Attention is all you need”</title>
    <url>/posts/34.html</url>
    <content><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>This blog is a summary of the papers I read recently, and it is also convenient for viewing at any time during the current project.<br><span id="more"></span></p>
<p>The project is based on viT semantic segmentation, so the papers here are related to it, including the following:</p>
<ul>
<li><p><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> </p>
</li>
<li><p><a href="https://arxiv.org/abs/2011.14503">End-to-End Video Instance Segmentation with Transformers</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2105.05633">Segmenter: Transformer for Semantic Segmentation</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2010.11929">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a></p>
</li>
</ul>
<p>(<em>Due to the limitations of English, the expression may be a bit strange, but as long as the meaning is correct, who cares?</em>)</p>
<h1 id="Attention-is-all-you-need"><a href="#Attention-is-all-you-need" class="headerlink" title="Attention is all you need"></a>Attention is all you need</h1><p>(The Chinese version can refer to my previous blog: <a href="https://superzlw.github.io/posts/16.html">here</a>,<br>the content is different from this one, and relatively brief, but also could be very helpful to understand it.)</p>
<p>(Here I recommend a awesome <a href="http://jalammar.github.io/illustrated-transformer/">blog</a>.<br>Most of the pictures here are from this blog as well as my previous blog.)</p>
<h2 id="WHAT-and-WHY"><a href="#WHAT-and-WHY" class="headerlink" title="WHAT? and WHY?"></a>WHAT? and WHY?</h2><p>First, I will give a brief introduction to this paper from these two parts, and then elaborate on its principles later.</p>
<h3 id="What-is-it"><a href="#What-is-it" class="headerlink" title="What is it?"></a>What is it?</h3><p>“Attention” is a method proposed by Bengio’s team in 2014 and widely used in various deep learning fields. </p>
<p>On this basis, this paper further proposes a new concept: Transformer. As the title of the paper says,<br>the traditional CNN and RNN are abandoned in the Transformer,<br>and the entire network structure is completely composed of the Attention mechanism.<br>More precisely, the Transformer consists of and only consists of self-Attenion<br>and Feed Forward Neural Network.</p>
<p>A trainable neural network based on Transformer can be built by stacking Transformers.<br>The author’s experiment is to build an encoder-decoder with 6 layers<br>of encoder and decoder, and a total of 12 layers of Encoder-Decoder, which performed best<br>in machine translate.</p>
<h3 id="Why-use-it"><a href="#Why-use-it" class="headerlink" title="Why use it?"></a>Why use it?</h3><p>The reason why the author uses “Attention” is to consider that the calculation<br>of RNN (or LSTM, GRU, etc.) is limited to sequential, that is to say,<br>RNN-related algorithms can only be calculated from left to right or from<br>right to left. The mechanism brings up two problems:</p>
<ul>
<li><p>The calculation at time t depends on the calculation result at time t-1,<br>which limits the parallelism of the model;</p>
</li>
<li><p>Information will be lost in the process of sequential calculation. Although<br>LSTM and other structures alleviate the problem of long-term dependence to a<br>certain extent, it is still powerless for particularly long-term dependence.</p>
</li>
</ul>
<p>The proposal of Transformer solves the above two problems. First, it uses “Attention”<br>to reduce the distance between any two positions in the sequence to a constant; second,<br>it is not a sequential structure similar to RNN, so it has better Parallelism, in line<br>with existing GPU frameworks.</p>
<p>Said in the paper:</p>
<blockquote>
<p>Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence aligned RNNs or convolution。</p>
</blockquote>
<h2 id="Principle"><a href="#Principle" class="headerlink" title="Principle"></a>Principle</h2><h3 id="Structure-of-the-Transformer"><a href="#Structure-of-the-Transformer" class="headerlink" title="Structure of the Transformer"></a>Structure of the Transformer</h3><p>Here take the machine translation as example, if we want to translate a sentence as below:<br><img src="/images/327.png" alt=""></p>
<p>Expand the transformer module, we can see:<br><img src="/images/328.png" alt=""></p>
<p>The Encoder-Decoder are what we mention above.<br>Further expand the module, we can get that:<br><img src="/images/329.png" alt=""></p>
<p>And the Encoder-Decoder looks like:<br><img src="/images/330.png" alt=""></p>
<h3 id="How-it-works"><a href="#How-it-works" class="headerlink" title="How it works"></a>How it works</h3><p>First of all, we need to do word-embedding as the input of<br>the first Encoder, all the encoder receive a list of vectors each<br>of the size 512, it means that, both word-embedding and<br>the output of a Encoder should be in the same size, that is,<br>list of vectors each of the size 512. This size(512) is<br>depended on the longest word of our training data, it should be<br>a hyperparameter.</p>
<p>We could see this figure, the FC layer is also named feed<br>forward layer.<br><img src="/images/331.png" alt=""></p>
<p>It shows that, after self-attention, there are dependencies<br>between these paths, but no dependencies in feed forward layer,<br>thus the various paths can be executed in parallel while<br>flowing through the feed-forward layer.</p>
<h3 id="Detail-of-the-“Self-Attention”"><a href="#Detail-of-the-“Self-Attention”" class="headerlink" title="Detail of the “Self-Attention”"></a>Detail of the “Self-Attention”</h3><p><strong>Step 1: create 3 vectors</strong></p>
<p>After Embedding of one word, we get an embedding vector, with 512<br>size. Next, we need to create 3 new vector, named: Queries<br>Vector, Keys Vector and Values Vector. The size of these 3 vectors<br>should be smaller than Embedding Vector, usually, they could be 64.</p>
<p>But how can we obtain these 3 vectors?</p>
<p>Here we need 3 matrix, that is $W^q$, $W^k$ and $W^v$. We multiply embedding<br>vector by the $W^q$ to obtain vector q, that is, Queries Vector, and the same to get<br>other two vectors.</p>
<p><strong>Step 2: compute scores</strong><br>These scores are used to measure how much focus to place on other parts of the<br>input sentence as we encode a word at a certain position. For example, we have the<br>following sentence:</p>
<blockquote>
<p>The animal didn’t cross the street because it was too tired</p>
</blockquote>
<p>What does <code>it</code> mean here? Fur us, it is a quite easy question, but<br>computer does not think so. So it is necessary to compute this score.<br>Just like the figure below:<br><img src="/images/332.png" alt=""></p>
<p>To calculate these scores, there are 2 common methods:<br><img src="/images/333.png" alt=""></p>
<p>Here we only consider the left one. It means that, we use dot product<br>of the query vector with the key vector of the respective word we’re scoring.</p>
<p><strong>Step 3, 4</strong><br>Divide the score by 8 (the square root of the dimension of the key vectors<br>used in the paper), and then use SOftmax to normalizes the score.<br>Such as the figure below:<br><img src="/images/334.png" alt=""></p>
<blockquote>
<p><strong>Note</strong>: the score of itself is also required and the<br>score above should first divide by 8, and then softmax. The activate<br>function could also be replaced by other function likes ReLU.</p>
</blockquote>
<p><strong>Step 5, 6</strong><br>Multiply each value vector by the softmax score, this step<br>can be used to ignore the unrelated word.</p>
<p>And then sum up the weighted value vectors, to produces the<br>output of the self-attention layer at this position.</p>
<p>Such as the figure below:<br><img src="/images/335.png" alt=""></p>
<h4 id="In-Matrix-Form"><a href="#In-Matrix-Form" class="headerlink" title="In Matrix Form"></a>In Matrix Form</h4><p>This part shows how to compute with matrix, several picture are<br>enough:<br><img src="/images/336.png" alt=""><br><img src="/images/337.png" alt=""><br><img src="/images/338.png" alt=""></p>
<blockquote>
<p><strong>Note</strong>: Only $W^q$, $W^k$ and $W^v$ should be learned.</p>
</blockquote>
<h3 id="Multi-head-Self-attention"><a href="#Multi-head-Self-attention" class="headerlink" title="Multi-head-Self-attention"></a>Multi-head-Self-attention</h3><p>Use multi-head can improve the ability of the model to focus<br>on different positions. Take 2 heads as example:<br><img src="/images/339.png" alt=""><br>There are 2 sets of Query/Key/Value weight matrices, that means that,<br>we will get 2 $b$ by using one embedding vector, but we only need<br>one vector as input for next layer.</p>
<p>So here we introduce a new weight matrix $W^0$:<br><img src="/images/340.png" alt=""><br>It means that, We concat the matrices then multiply them by an additional<br>weights matrix $W^0$<br><img src="/images/341.png" alt=""><br>(<strong>this image takes 8 heads as example</strong>)</p>
<h3 id="Position-Encoding"><a href="#Position-Encoding" class="headerlink" title="Position Encoding"></a>Position Encoding</h3><p>The last problem is, although we consider the whole sentence, we do not consider<br>the relative position of the individual words.</p>
<p>To solve this problem, we need to use an additional vector, which<br>can tell the model the exactly position of each word, or the distance<br>between different word. And this vector could be considered in<br>embedding vector.</p>
<p>For example, if the dimension of the embedding vector is 4, it could<br>be:<br><img src="/images/342.png" alt=""></p>
<p>This position vector in paper looks like this:<br><img src="/images/343.png" alt=""><br>Each column represents a position vector. The first column will be<br>the vector we want to add to the first word embedding in the<br>input sequence. Each column contains 512 values, each with a value<br>between 1 and -1</p>
<h3 id="A-small-detail"><a href="#A-small-detail" class="headerlink" title="A small detail"></a>A small detail</h3><p>One detail in the encoder architecture is that each sub-layer<br>(self-attention, ffnn) in each encoder has a residual connection around it,<br>followed by a layer normalization step. Such like this:<br><img src="/images/344.png" alt=""><br>More specifically:<br><img src="/images/345.png" alt=""></p>
<h2 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h2><p>After learning the principle of the Transformer as well as<br>the Encoder, now we could start our Decoder.</p>
<p>After processing the input sequence by encoder, the output of<br>the top encoder is then transformed into a set of attention vectors<br>K and V. These are to be used by each decoder in its “encoder-decoder<br>attention” layer which helps the decoder focus on appropriate places in<br>the input sequence:<br><img src="/images/346.png" alt=""></p>
<p>Repeat this process, until a special signal is outputted, to tell<br>the decoder that the process should be finished.</p>
<p>What’s more, the self-attention in decoder is a bit different from the<br>self-attention in encoder. In the decoder, the self-attention layer is only<br>allowed to attend to earlier positions in the output sequence. </p>
<p>This process can be showed as below:<br><img src="/images/347.png" alt=""><br><img src="/images/348.png" alt=""><br>we could see that, the self-attention of decoder only care about<br>the earlier positions in the output sequence, for example, we use<br>“I”, “am”, “a” that we obtained before to get the new word “student”.</p>
<p>It should be noted that, the K and V vectors from encoder should be used<br>in every decoder part.</p>
<p>Finally, we get a float vector, we should turn it into a word, this is what<br>the last layer, softmax layer, do. It is a simple full connected layer,<br>that projects the vector produced by the stack of decoders, into a much,<br>much larger vector called a logits vector.</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>[1] Paper: Attention is all you need</p>
<p>[2] <a href="http://jalammar.github.io/illustrated-transformer/">http://jalammar.github.io/illustrated-transformer/</a></p>
<p>[3] <a href="https://zhuanlan.zhihu.com/p/48508221">https://zhuanlan.zhihu.com/p/48508221</a></p>
]]></content>
      <categories>
        <category>学习总结</category>
      </categories>
  </entry>
  <entry>
    <title>一次项目的反思和总结</title>
    <url>/posts/33.html</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在这次Data Science2 的课程项目中，选了Kaggle上一个正在进行的比赛作为课题，这应该是我第一次跟人合作做这么大的项目，也是我第一次正经参加Kaggle的比赛，整个比赛过程走了很多弯路，踩了很多坑，觉得非常有必要做个记录，作为督促自己的成长吧。第一部分主要是一些反思和总结，另外在这两个月里看了不少论文，准备每天写一点，放在第二部分了。<br><span id="more"></span></p>
<h1 id="反思1：整体规划"><a href="#反思1：整体规划" class="headerlink" title="反思1：整体规划"></a>反思1：整体规划</h1><h2 id="问题："><a href="#问题：" class="headerlink" title="问题："></a>问题：</h2><p>在这个项目中，我一开始并没有一个清晰的规划，认为可以像做其它课程小项目一样，先做着，有问题查资料，反正就一直做下去，属于船到桥头自然直的那种，这就是“走弯路”的开始。大项目不比小项目，大项目一开始就会有很多个方向需要选择，并且每个方向会遇到的事都不明朗，如果一开始不做充足的调查准备工作，随意凭直觉或者一些不太靠谱的依据选一个方向，如果错了，就得重新来过，非常浪费时间。\</p>
<h2 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h2><p>根据队友的经验，这种程度的项目理应有两个规划，动态和静态的，或者也可以说是短期和长期。静态规划是一个比较笼统的东西，但花的时间应该是非常多的，首先，我们得知道，一开始我们有多少个方向可以选择，每个方向大致需要做什么工作，可能会出现的问题，以及需要经历的阶段，这个静态过程非常耗时，但这是为了节省后面的时间，提高后面的效率，规划到什么程度这是经验性的东西，不好说，现在我也没什么概念，但这时间一定得花。第二是动态规划，也就是短期的，因为静态规划只是一个大概的东西，具体在进行的过程中会出现很多不确定的因素，遇到这些问题短期内我们要怎么做，这就是动态的，也应该是具体的。\</p>
<h2 id="实例："><a href="#实例：" class="headerlink" title="实例："></a>实例：</h2><p>以这次项目为例，做细胞实例分割，一开始的时候以Anchor-free 和 Anchor-based 将四人小组分为两个，我和另一个人做Anchor-based ，其实这个时候我们应该花时间做静态规划的，但我们没有，而是直接来了个短期的目标，做pipeline，包括基于Pytorch的Dataset和Dataloader，这个对我们的项目很重要，但根据后面的发展，这个对比赛不重要。还有数据集的处理，Kaggle讨论区其实有非常多的资源，甚至现成的代码或者别人已经处理好的数据集，但我们没有看，这部分花了非常多的时间。所以，如果我们一开始就花一定的时间充分了解比赛，了解Kaggle平台能提供的资源，后面的进度会快很多。\</p>
<h1 id="反思2：模型的选择"><a href="#反思2：模型的选择" class="headerlink" title="反思2：模型的选择"></a>反思2：模型的选择</h1><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>这个比赛中我们尝试了非常多的模型，而且犯了一个大忌：从复杂到简单。回头想想这真的很要命，一开始我们的思路非常简单，因为这是实例分割，就从COCO数据集里找模型，在排名靠前的模型里找，确定了个：CBNetV2 + Swin Transfomer + HTC, 参数量一两个亿，丝毫没有考虑到我们现有数据集的量能不能训出这模型，也基本没有考虑训这模型我们所需要的时间已经金钱成本。</p>
<h2 id="总结：-1"><a href="#总结：-1" class="headerlink" title="总结："></a>总结：</h2><p>其实Olivier说的非常有道理，我们应该先定下一个baseline model，比如我们这里定的是Mask RCNN，我们就应该先努力去调它，调出一个好的结果，然后再根据需要慢慢改进或者复杂化，而不是一上来就搞一个超级复杂的模型，我们压根没那些时间，硬件，金钱去调这种模型，数据量也不够，最后训出来的结果非常差，而且对于这么大的模型，要调参我们无从下手，一个模型跑两天我们真是疯了。</p>
<h1 id="反思3：认清自己以及比赛"><a href="#反思3：认清自己以及比赛" class="headerlink" title="反思3：认清自己以及比赛"></a>反思3：认清自己以及比赛</h1><h2 id="问题：-1"><a href="#问题：-1" class="headerlink" title="问题："></a>问题：</h2><p>就是我们要量力而为，一开始我居然想在半个月里手写HTC，真是疯了，这也花了不少时间。也就是在这个时候我们基本确定了用MMDetection 搭建模型，之前走了这么多弯路，直到这时候我们才有半只脚踏上了正规，但我们依旧在错误的路上走着，想用MMDetection个非常复杂的模型。</p>
<h2 id="总结：-2"><a href="#总结：-2" class="headerlink" title="总结："></a>总结：</h2><p>正确了解自己能力和所要处理的问题之间的差距。另外也要时刻思考，现阶段我们需要实现什么，同时也要思考下一阶段的部署，不要说多么长远，至少，三天以内的安排得清楚。队友进度一直走在我前面，应该就是他不停在思考短期的计划，并且行动力也跟得上。\<br>到后面用MMDetection的时候，其实我们已经是在单纯地调config的参数了，而且调的时候既没有经验或实验数据支持，也没有理论支持，只能调一些最简单直观的，说难听一点，这个工作给任何一个毫无相关专业知识的中学生可能都能做。Kaggle不应该是用这种方法就能拿到好分数的比赛。\</p>
<h1 id="回顾"><a href="#回顾" class="headerlink" title="回顾"></a>回顾</h1><p>站在现在回顾这个比赛，我认为合理的流程或许应该是这样的：\</p>
<blockquote>
<ol>
<li>了解比赛规则，包括提交的东西，能不能联网，训练时间的限制，能否使用外部数据集等，能有多具体就做多具体。\</li>
<li>了解数据集的构成，哪些有用哪些没用。\</li>
<li>前两步一定要去讨论区看看，看看别人对这比赛的想法，对这数据集的看法以及处理方法。\</li>
<li>确定baseline model，这步很重要，如果像这个比赛一样很直观的话，可以直接定下 Mask RCNN，如果不了解，找论文，论文的来源，讨论区或许有，如果不想看论文，翻翻讨论区，看出现频率最高的模型是哪个，再去深入了解。\</li>
<li>搭好模型，弄好数据集，尽早提交第一次成果。根据分数调整baseline model，让分数尽量高，调整灵感可来自讨论区，和实验的结果分析。\</li>
<li>当baseline model 已经达到自己所能调整的极限时，开始往外拓展，在baseline model的基础上进行模型复杂化，以Mask RCNN为例，可以改backbone，可以加上cascade，然后把之前Mask RCNN调好的参数用上去，或许不一定很契合，需要微调，但我认为这种做法并不坏。但要注意的是，每次更改模型就选一个方向，比如先改backbone，调到最优，再改其它，有队友的话可以各自负责一个方向，但基底应该是一样的。\</li>
</ol>
</blockquote>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>总而言之，通过这个比赛真真切切认识到规划的重要性，接下来想多找几个项目积累经验，另外，MMDetection的源码得看，下次再用的时候不能只局限于调config。\<br>暂时只想到这些，之后再仔仔细细回顾整个过程，或许还有补充。</p>
]]></content>
      <categories>
        <category>未分类</category>
      </categories>
  </entry>
  <entry>
    <title>高斯过程</title>
    <url>/posts/30.html</url>
    <content><![CDATA[<h1 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h1><p>这个写起来有点烦，先占位提醒自己，之后再写。</p>
]]></content>
      <categories>
        <category>学习总结</category>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>图像去噪 (Graphcuts + α-expansion)</title>
    <url>/posts/31.html</url>
    <content><![CDATA[<h1 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h1><p>记录一种基于α拓展和图像切割技术的图像去噪的方法，作业的其中一题，貌似也没在网上看到相关的东西，觉得挺有意思就记一下。顺便也总结一下其中用到的方法。<br><span id="more"></span></p>
<h1 id="Graph-Cuts"><a href="#Graph-Cuts" class="headerlink" title="Graph Cuts"></a>Graph Cuts</h1><p>这里以二元分割为例，即是说 disparity 只取[0, 1]，如下图所示：<img src="/images/316.png" alt=""><br>然后现在就要对每个像素进行分类，或者说切割，但我们要怎么判断一个像素属于哪个 disparity 呢，这里就要引入Cost，有两种，第一种如下图：<img src="/images/317.png" alt=""><br>也就是说这里的 5 表示该像素<strong>不</strong>分配到 $d=1$的Cost，同理，对于下面$d=0$也会有对应的Cost；\<br>第二种是：<img src="/images/318.png" alt=""><br>也即是说这里的2表示把相邻像素分配给不同disparity的Cost。\<br>现在我们要考虑的就是如何把不同的像素归到不同的disparity，这里介绍一种最常用的方法：最大流/最小割法 (max-flow/min-cut)。</p>
<h2 id="最大流-最小割法-max-flow-min-cut"><a href="#最大流-最小割法-max-flow-min-cut" class="headerlink" title="最大流/最小割法 (max-flow/min-cut)"></a>最大流/最小割法 (max-flow/min-cut)</h2><h3 id="最大流"><a href="#最大流" class="headerlink" title="最大流"></a>最大流</h3><p><strong>这部分直接去看【参考1】比较好</strong></p>
<p>乍看之下这好像是两种方法，但在这里这两个东西其实本质上是一样的。\<br>【参考1】其实已经讲得很好了，我这里结合【参考2】再理一遍，至少让我印象深刻一点。\<br>以水管输送水为例，要从起始点送水到目标点，某一路径最大允许流量应取决于该路径最脆弱那段水管所能承受的最大流量，不然会爆，以下图为例: <img src="/images/319.png" alt="" title="图源于参考1"><br>这里只有一条路径，且最小值为2，即是说该路径的最大流为2。\<br>如果路径多一点的话那就不太好算了，以下图为例：<img src="/images/320.png" alt="" title="图源于参考1"><br>对于路径“s-&gt;u-&gt;v-&gt;t”，最小值为2，即该路径允许的最大流为2，用了流量2后当前状态就变成下图的状态了：<img src="/images/321.png" alt="" title="图源于参考1"><br>(这里的x/y 表示容量最大为y，现已经用了x。)\<br>这时候就会发现没路径可走了，并作出判断：该图最大流为2。但显然这种判断是错的，下面这种情况才是对的：<img src="/images/322.png" alt="" title="图源于参考1"><br>为了得到正确的流量，上面那种简单路径叠加的方法明显是行不通的，这时候就要用residual graph了。</p>
<h4 id="residual-graph"><a href="#residual-graph" class="headerlink" title="residual graph"></a>residual graph</h4><p>这个方法的核心思想就是可以撤销之前的操作，这样就可以考虑得更全面(学过“图”算法的应该很容易理解，但我没学过，只在leetcode上做过广度/深度优先搜索算法题，意思差不多，所以对这部分我只是基本理解，并不全面)。\<br>当我们选择了路径“s-&gt;u-&gt;v-&gt;t”时，其允许的最大流为<strong>2</strong>，此时按照这个流量从终点往起点回溯，图如下：<img src="/images/323.png" alt="" title="图源于参考1">抵消部分如果是0就省略了。\<br>然后现在在根据这个新图找一条新的路径，比如说“s-&gt;v-&gt;u-&gt;t”，其允许的最大流为<strong>1</strong>，回溯后又得到一个新图如下：<img src="/images/324.png" alt="" title="图源于参考1"><br>根据这个图我们没办法找到新路径了，所以最大流为2+1=3，这就是最终的结果了。\</p>
<h3 id="最小割"><a href="#最小割" class="headerlink" title="最小割"></a>最小割</h3><p>最小割的话就简单说了，即是把不同像素归到不同的disparity，使这种归类所需的能量最小，比如说下面这幅图：<img src="/images/325.png" alt=""></p>
<h1 id="𝝰-expansion"><a href="#𝝰-expansion" class="headerlink" title="𝝰-expansion"></a>𝝰-expansion</h1><p>上面的图割法显然只对二分类有用，经常用于分离背景和目标物，但如果要处理多个标签，就要用到𝝰-expansion了。\<br>这种方法简单说就是迭代地使用图割找到能量(或者对数后验)地局部最优解。\<br>基本思想是：</p>
<blockquote>
<ol>
<li>初始化disparity map，比如全部设0；\</li>
<li>以随机顺序反复扫过所有的disparity；\</li>
<li>将中间解决方案视为一个disparity level，当前提出的disparity level视为另一个；\</li>
<li>解决二进制的图割问题；\</li>
<li>重复进行，直到在一次扫描中没有像素被更新为止。　</li>
</ol>
</blockquote>
<p>用图表示的话就是：<img src="/images/326.png" alt=""><br>简单解释一下：𝝰-expansion将多标签问题表示成一系列的二元子问题，在每一步里，我们选择一个标签 α 并拓展：对于每个像素，要么保持标签原样，要么用 α 替换。每个子问题都能以全局最优的方式得到解决。 a) 初始标签。 b) 橙色标签扩大：每个标签保持不变或变成橙色。 c) 黄色标签扩大。 d) 红色标签扩大。</p>
<h1 id="作业解释"><a href="#作业解释" class="headerlink" title="作业解释"></a>作业解释</h1><p>(作业代码依旧在GitHub里，如果没有，那就是还没整理好)\</p>
<p>𝝰-expansion 提供了一种对具有多个标签的马尔科夫随机网格（MRF）进行推理的方法，将解决方案分解为一系列二进制问题，其中每个问题都可以被精确解决。其主要思想是对每个标签进行迭代，在每次迭代中，所有节点都可以选择保留其当前标签或切换到新标签，如上图所示。\<br>这里实现一种近似版本的𝝰-expansion，现假设图像被标准差为$\sigma_N$的高斯噪音破坏，并且这个高斯噪音是像素独立的(pixelwise independent)。\<br>对于尺寸为$H\times W$的图像，放弃一些常数项，我们可以写出一个适当的似然模型：</p>
<script type="math/tex; mode=display">
p(y|x) \propto \prod_{i,j}exp\left\{-\frac{1}{2\sigma^2_N}(x_{i,j}-y_{i,j})^2\right\}</script><p>这里$x,y\in R^{H\times W}$分别是原始图像和被破坏的图像。\</p>
<p>不写下去了，到时候直接在代码里加注释吧。。。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://seineo.github.io/%E5%9B%BE%E8%AE%BA%EF%BC%9A%E6%9C%80%E5%A4%A7%E6%B5%81%E6%9C%80%E5%B0%8F%E5%89%B2%E8%AF%A6%E8%A7%A3.html">参考1</a> \<br><a href="https://zhuanlan.zhihu.com/p/58185005">参考2</a>\</p>
]]></content>
      <categories>
        <category>图像处理</category>
      </categories>
  </entry>
  <entry>
    <title>三维重建基础与极几何</title>
    <url>/posts/28.html</url>
    <content><![CDATA[<h1 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h1><p>一般做三维重建我们是采用多视图的，单视图虽然也能做，但需要有场景的先验信息，比如点的对应关系，线、面几何等，所以在条件允许的情况下，基于多视图的三维重建适用范围是要优于单视图的，这里介绍两视图的情况，包括它们包含的几何关系。<br><span id="more"></span></p>
<h1 id="三角测量-Triangulation"><a href="#三角测量-Triangulation" class="headerlink" title="三角测量(Triangulation)"></a>三角测量(Triangulation)</h1><p>我们想要解决的问题是：</p>
<blockquote>
<p>现给定一个三维点在两个或多个图像(相机矩阵已知)中的投影，希望找到该点的坐标。</p>
</blockquote>
<p>以两图像投影为例，如下图：<img src="/images/286.png" alt=""><br>正常来说，由于相机矩阵是已知的，$O_1x_1$和$O_2x_2$我们都能表示出来，也就很容易求出这个空间三维点$X$了，然而这有个前提，就是那两条视线应该相交，但由于噪音和数值误差，这个条件一般都不能满足。\<br>也就是说一般情况下是这样子的：<img src="/images/287.png" alt=""><br>我们的想法是：找到连接两条视线的最短线段，并让 X 为该线段的中点。具体操作为：</p>
<blockquote>
<ol>
<li>寻找线段的方向(即垂直于两条射线);</li>
<li>构造两个平面，每个平面都包含该线段以及一条射线;</li>
<li>将该平面与其它射线相交以产生线段的端点;</li>
<li>找到中点。</li>
</ol>
</blockquote>
<p>找是找到了这个点，怎么求呢，我们现在已知的条件有：$x_1,x_2$，内参矩阵$K_1,K_2$，旋转平移$R,T$(也就是知道投影矩阵)，这里主要有两种方法，线性法和非线性法。</p>
<h2 id="线性法"><a href="#线性法" class="headerlink" title="线性法"></a>线性法</h2><p>线性法的话，精度不高但比较容易。\<br>由共线可知：</p>
<script type="math/tex; mode=display">
x_1\times P_1X=0</script><script type="math/tex; mode=display">
x_2\times P_2X=0</script><p>这明显就是个齐次线性方程组，用SVD分解很快就做出来了，这在之前相机那篇已经介绍过，这里就不赘述了，<a href="https://superzlw.github.io/posts/26.html">链接在这</a>。 \<br>对于多于两个视图的情况，无非就是多列几个方程。</p>
<h2 id="非线性法"><a href="#非线性法" class="headerlink" title="非线性法"></a>非线性法</h2><p>这种方法简单说就是希望找到个$X$，使其最小化下面这个式子：</p>
<script type="math/tex; mode=display">
d^2(x_1,P_1,X)+d^2(x_2,P_2,X)</script><p>如图：<img src="/images/288.png" alt=""><br>也即是说，我们要找的那个三维点，它投影到图像上的点，要跟我们观测点距离最小。也就是图中的<script type="math/tex">x_1'x_1+x_2'x_2</script>最小。\<br>假设我们把世界坐标系设在$O_1$的位置，那么两个投影矩阵分别为：</p>
<script type="math/tex; mode=display">
P_1=K_1[I|0]</script><script type="math/tex; mode=display">
P_2=K_2[R|T]</script><p>至此条件都足够了，然后通过迭代求解就行，一般是用高斯牛顿法或者L-M法求解，这个在另一篇非线性优化里也已经介绍过了，<a href="https://superzlw.github.io/posts/15.html">链接在这</a>。 \</p>
<h1 id="极几何"><a href="#极几何" class="headerlink" title="极几何"></a>极几何</h1><p>接下来我们想知道的就是同一场景的两个不同视图之间的关系。\<br>这里先给几个后面需要用到的概念：<img src="/images/289.png" alt=""></p>
<blockquote>
<ol>
<li>极平面(Epipolar plane): 过点$P$与两相机中心的平面;</li>
<li>基线(baseline): $O_1$与$O_2$的连线;</li>
<li>极线(epipolar lines): 极平面与成像平面的交线，也就是$p_1e_1$和$p_2e_2$;</li>
<li>极点(Epipole): 基线与成像平面的交点，也就是$e_1$和$e_2$。(可以在可见区域之外)</li>
</ol>
</blockquote>
<p>现在来看这个图：<img src="/images/290.png" alt=""><br>关于这个极几何有几个性质需要提一下(第3,4点比较重要):</p>
<blockquote>
<ol>
<li>极平面相交于基线;</li>
<li>极线相交于极点;</li>
<li>$p_1$的对应点落在极线$p_2e_2$上;</li>
<li>$p_2$的对应点落在极线$p_1e_1$上;</li>
</ol>
</blockquote>
<p>最后两个性质表明，我们要找一个像点在另一个平面的对应点时，不用整张图去找，只要找极线就行了。\<br>来看几个特例：\<br><strong>(1). 平行视图：</strong><img src="/images/291.png" alt=""></p>
<blockquote>
<p>两个图像平行；\<br>基线平行于图像平面，两极点位于无穷远处；\<br>极线平行于图像坐标轴的其中一条轴。\</p>
</blockquote>
<p><strong>(2). 前后平移：</strong><img src="/images/292.png" alt=""></p>
<blockquote>
<p>两幅图极点位置相同，极点称为展开焦点。</p>
</blockquote>
<h2 id="极几何约束"><a href="#极几何约束" class="headerlink" title="极几何约束"></a>极几何约束</h2><p>现在我们想要知道的是，一个图中的点和另一幅图中点的对应关系。\<br>如下图：<img src="/images/293.png" alt=""><br>明显，我们有如下关系(这些关系都是等价的，一个意思)：</p>
<blockquote>
<ol>
<li>$\overrightarrow{O_1P},\overrightarrow{O_2P},\overrightarrow{O_1O_2}$ 共面;\</li>
<li>$\overrightarrow{O_1p_1},\overrightarrow{O_2p_2},\overrightarrow{O_1O_2}$ 共面;\</li>
<li>$\overrightarrow{O_1p_1}\cdot [\overrightarrow{O_1O_2}\times \overrightarrow{O_2p_2}]=0$。</li>
</ol>
</blockquote>
<h3 id="本质矩阵情况下"><a href="#本质矩阵情况下" class="headerlink" title="本质矩阵情况下"></a>本质矩阵情况下</h3><p>这种情况我们考虑的是规范化相机，规范化相机是指相机内参数矩阵为单位矩阵的相机，即是：</p>
<script type="math/tex; mode=display">
K=
\begin{bmatrix}
1&0&0\\
0&1&0\\
0&0&1
\end{bmatrix}</script><p>假设$p_1=(u_1,v_1,1)^T$，$p_2=(u_2,v_2,1)^T$，上面最后一个关系(3)就等价于：</p>
<script type="math/tex; mode=display">
p_1^T[t\times (Rp_2)]=0</script><p>上面这个呢就是外极性约束了。\<br>叉乘的话可以表示成矩阵形式，比如说</p>
<script type="math/tex; mode=display">
a\times b = 
\begin{bmatrix}
0&-a_z&a_y\\
a_z&0&-a_x\\
-a_y&a_x&0
\end{bmatrix}
\begin{bmatrix}
b_x\\
b_y\\
b_z
\end{bmatrix}=
[a]_{\times}b</script><p>所以本质矩阵：$E=[t]_{\times}R$\<br>将上面那个外极性约束用本质矩阵描述的话是:</p>
<script type="math/tex; mode=display">
p_1^T[t\times (Rp_2)]=p_1^TEp_2=0</script><p>也就是说空间一个三维点在双视图中必须满足 $p_1^TEp_2=0$。\<br>随着这个本质矩阵带来的，还有下面一系列的重要性质：</p>
<blockquote>
<ol>
<li>极线用本质矩阵可表示为：$l_1=Ep_2$, $l_2=E^Tp_1$;</li>
<li>$e_1^TE=E^Te_1=0$与$Ee_2=0$;</li>
<li>本质矩阵是奇异的，秩为2; 剩下的两个特征值相等;</li>
<li>本质矩阵有5个自由度(三个旋转+三个平移，但$det(E)=0$去掉了一个，或者说由于比例是任意的，所以去掉了一个)。</li>
</ol>
</blockquote>
<p>(上面4个性质都容易证明，就不写了，但有一个性质需要记住：<strong>反对称矩阵的秩是偶数的</strong>)\</p>
<p>举个平行相机的例子，如图：<img src="/images/294.png" alt=""><br>由于是平行的，$R=I$，平移量为$b$，则$t=[-b,0,0]$，所以本质矩阵为：</p>
<script type="math/tex; mode=display">
E=[t]_{\times}R=
\begin{bmatrix}
0&0&0\\
0&0&b\\
0&-b&0
\end{bmatrix}</script><p>由约束条件 $p_{1}^TEp_2=0$ 得：</p>
<script type="math/tex; mode=display">
\begin{split}
0 &=p_{1}^TEp_2 \\\\
&= [x_1,y_1,1]\begin{bmatrix}
0&0&0\\
0&0&b\\
0&-b&0
\end{bmatrix}\begin{bmatrix}
x_2\\
y_2\\
1
\end{bmatrix}\\\\
&= [x_1,y_1,1]\begin{bmatrix}
0\\
b\\
-by_2
\end{bmatrix}\\\\
&= by_1-by_2
\end{split}</script><p>解得：$y_1=y_2$, $x_2$ 任意。</p>
<h3 id="基础矩阵情况下"><a href="#基础矩阵情况下" class="headerlink" title="基础矩阵情况下"></a>基础矩阵情况下</h3><p>一般情况下我们的相机都不是规范化的相机，而是普通的一般化相机，不适用于上面的情况，这时候就引出了与本质矩阵相对的“基础矩阵”。\<br>依旧是这个图：<img src="/images/295.png" alt=""><br>但这时候的$K$不再是单位矩阵了，怎么做？\<br>最直接的想法就是把它转化为规范化相机。\<br>我们假设两个内参矩阵分别是:$K_1,K_2$，这两个是已知的。然后有如下关系：</p>
<script type="math/tex; mode=display">
x_1=K[I\;0]P</script><p>即是:</p>
<script type="math/tex; mode=display">
K^{-1}x_1=K^{-1}K[I\;0]P=
\begin{bmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&1&0
\end{bmatrix}P</script><p>设：$K^{-1}x_1=x_{1c}$, 所以有：</p>
<script type="math/tex; mode=display">
x_{1c}=\begin{bmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&1&0
\end{bmatrix}P</script><p>同理，$K_2^{-1}x_2=x_{2c}$<br>如此得出来的$x_{1c},x_{2c}$就是规范化相机下$P$的对应点。\<br>文字描述的话就是：把成像平面上实际观测点乘以该相机内参数矩阵的逆矩阵，就可以得到对应的规范化相机下的点。\<br>由本质矩阵的性质有:</p>
<script type="math/tex; mode=display">
x_{2c}^TEx_{1c}=x_{2c}^T[t]_{\times}Rx_{1c}=(K_2^{-1}x_2)^T\cdot [t]_{\times}RK_{1}^{-1}x_1=x_2^TK_2^{-T}[t]_{\times}RK_{1}^{-1}x_1=0</script><p>中间这串就是我们要引出的基础矩阵$F$：$F=K_2^{-T}[t]_{\times}RK_{1}^{-1}$，然后我们就可以有如下结论：</p>
<script type="math/tex; mode=display">
x_2^TFx_1=0</script><p>基础矩阵有与本质矩阵类似的性质，如下：</p>
<blockquote>
<ol>
<li>极线分别是：$l_1=Fx_1$和$l_2=F^Tx_2$;</li>
<li>$Fe_1=0$与$F^Te_2=e_2^TF=0$;</li>
<li>$F$是奇异的，秩为2;</li>
<li>$F$有7个自由度(尺度任意，$det(F)=0$)。</li>
</ol>
</blockquote>
<h2 id="估计基础矩阵：归一化八点算法"><a href="#估计基础矩阵：归一化八点算法" class="headerlink" title="估计基础矩阵：归一化八点算法"></a>估计基础矩阵：归一化八点算法</h2><p>$F$有7个自由度，理论上7个点就可以求解了，但这样做的话解是非线性的，很麻烦，所以这里用8个点，俗称“八点算法”。\<br>现假设一个三维点在两成像平面上的观测点分别为：$[x’,y’,1]$和$[x,y,1]$，按照上面的结论我们有：</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
x'&y'&1
\end{bmatrix}
\begin{bmatrix}
F_{11}&F_{12}&F_{13}\\
F_{21}&F_{22}&F_{23}\\
F_{31}&F_{32}&F_{33}
\end{bmatrix}
\begin{bmatrix}
x\\y\\1
\end{bmatrix}
=0</script><p>展开的话即是：</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
xx'&yx'&x'&xy'&yy'&y'&x&y&1
\end{bmatrix}
\begin{bmatrix}
F_{11}\\F_{12}\\F_{13}\\F_{21}\\F_{22}\\F_{23}\\F_{31}\\F_{32}\\F_{33}\\
\end{bmatrix}=0</script><p>简写成：</p>
<script type="math/tex; mode=display">
Af=0</script><p>由于$f$不能为0，所以添加一个约束：$||f||=1$，这，又是带约束的齐次方程，正经8个点的话直接算，但一般我们会用多于8个点去估计，以减少误差，这时候就可以用SVD分解，相关计算看相机那篇，<a href="https://superzlw.github.io/posts/26.html">在这</a>。\<br>但是在计算之前我们不能忘了数值稳定性，也就是说方程组的系数应该在同一数量级，比如像素是$1E6$，也就是说要预处理，不然会由于数值差异很大导致最后结果精度很低。方法过程如下：<img src="/images/296.png" alt=""><br>之后就可以求解了。<br>但用这种方法算出来的还不是我们要求的基础矩阵，因为基础矩阵的秩应该是2，而这种方法算出来的矩阵秩通常为3，即满秩。\<br>要解决这个问题也简单(且粗暴)，加入在上面我们求出来的满秩“基础矩阵”为 $\widetilde{F}$，我们对其再做一次SVD分解：</p>
<script type="math/tex; mode=display">
\widetilde{F}=U_F\widetilde{D}_FV_{F}^T</script><p>令：</p>
<script type="math/tex; mode=display">
D_F=\widetilde{D}_F \quad , \quad D_{F,33}=0</script><p>接着回构：</p>
<script type="math/tex; mode=display">
\overline{F}=U_FD_FV_F^T</script><p>由于上面我们对坐标进行了处理，最后一步把$\overline{F}$返回去：</p>
<script type="math/tex; mode=display">
F=T'^T\overline{F}T</script><p>用这种奇异值置零的方法我们可以保证求出的$F$满足在约束 $det(F)=0$ 的情况下最小化 $||F-\widetilde{F}||_F$\<br>至此求解就结束了。\<br>上述过程可以用下面这张图总结：<img src="/images/297.png" alt=""></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>【1】 <a href="https://www.youtube.com/watch?v=BKVEvj9F_H4&amp;list=LL&amp;index=42">https://www.youtube.com/watch?v=BKVEvj9F_H4&amp;list=LL&amp;index=42</a></p>
]]></content>
      <categories>
        <category>图像处理</category>
      </categories>
  </entry>
  <entry>
    <title>光流三部曲---1. LK光流</title>
    <url>/posts/25.html</url>
    <content><![CDATA[<h1 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h1><p>目前关于光流一共学过三种方法，分别为LK光流，HS光流以及概率法，一步一步来吧，这里先讲LK光流，之后如果学了其它方法再来更新。<br><span id="more"></span></p>
<h1 id="光流"><a href="#光流" class="headerlink" title="光流"></a>光流</h1><p>光流（optical flow）是指空间运动物体在观测平面上的像素运动的瞬时速度。一般用图像序列中像素在不同时间上的变化来确定相邻帧之间的相关关系，从而计算出相邻帧图像的运动。当相邻帧时间间隔足够短时，我们可以用目标点的位移来代指这个速度。\<br>简言之：</p>
<blockquote>
<p><strong>Optical flow = 2D velocity field describing the apparent motion in the images.</strong></p>
</blockquote>
<p>光流场的话，是指图像中所有像素点构成的一种二维瞬时速度场，如下图的例子所示：<img src="/images/298.png" alt=""></p>
<h1 id="光流估计"><a href="#光流估计" class="headerlink" title="光流估计"></a>光流估计</h1><p>光流估计即是通过给定的两帧图片，估计它们之间的$u(x,y)$和$v(x,y)$。\<br>为了计算光流，我们会首先做以下两个假设：</p>
<blockquote>
<p><strong>假设1</strong>：对于图像中的小区域，尽管它们的位置可能发生变化，但小区域中的图像测量值(例如亮度)保持不变，称为亮度恒定假设。\<br><strong>假设2</strong>：场景中的相邻点通常属于同一表面，因此通常具有相似的3D运动。称为空间连续性假设。</p>
</blockquote>
<p>对于假设1，用数学表示的话可以写成如下形式：</p>
<script type="math/tex; mode=display">
I(x+u(x,y),y+v(x,y),t+1)=I(x,y,t)</script><p>其中$u(x,y)$和$v(x,y)$分别称为水平流位移和垂直流位移。\<br>计算这个光流最直接的想法是最小化亮度差异，亮度差异可表示为：</p>
<script type="math/tex; mode=display">
E_{SSD}(u,v)=\sum_{(x,y)\in R}(I(x+u,y+v,t+1)-I(x,y,t))^2</script><p>也就是计算一个区域像素在某一方向的亮度差异。\<br>再具体一点的话，就是对每个像素，取其邻近区域并尝试所有可能的运动，取一个最小的$SSD$，该方向即为该区域像素的光流方向。\<br>但这么算的话效率很低，而且把运动空间分散了，这跟现实不太符。\<br>所以我们希望找到另一个方法来近似。</p>
<h2 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h2><p>对于上面那个差异公式，右边第一项可以写为：</p>
<script type="math/tex; mode=display">
I(x+u,y+v,t+1)=I(x+\Delta_x,y+\Delta_y,t+\Delta_t)</script><p>继续用泰勒级数展开可表示为：</p>
<script type="math/tex; mode=display">
I(x,y,t)+\Delta_x\frac{\partial}{\partial x}I(x,y,t)+\Delta_y\frac{\partial}{\partial y}I(x,y,t)+\Delta_t\frac{\partial}{\partial t}I(x,y,t)+\epsilon(\Delta_x^2,\Delta_y^2,\Delta_t^2)</script><p>最后一项$\epsilon$是近似误差，当$\Delta_t$足够小时，这个误差近似为0。\<br>所以上面的SSD近似可写为：</p>
<script type="math/tex; mode=display">
\begin{split}
E_{SSD}(u,v) &\approx \sum_{(x,y)\in R}(I(x,y,t)+u\frac{\partial}{\partial x}I(x,y,t)+v\frac{\partial}{\partial y}I(x,y,t)+\frac{\partial}{\partial t}I(x,y,t)-I(x,y,t))^2\\\\
&= \sum_{(x,y)\in R}(u\frac{\partial}{\partial x}I(x,y,t)+v\frac{\partial}{\partial y}I(x,y,t)+\frac{\partial}{\partial t}I(x,y,t))^2\\\\
&= \sum_{(x,y)\in R}(u\cdot I_x(x,y,t)+v\cdot I_y(x,y,t)+I_t(x,y,t))^2
\end{split}</script><p>这个SSD近似方程相较于更上面的那个差异性方程来说，这个方程对于运动$u,v$是凸的，所以更容易计算，但是，这只适用于<mark><strong>小运动</strong></mark>的情况。\<br>当$\Delta_t$足够小趋近于0的时候，我们有下面这个式子：</p>
<script type="math/tex; mode=display">
u\cdot I_x+v\cdot I_y+I_t=0</script><p>这就是光流约束方程(optical flow constraint equation, OFCE)，也成为线性化亮度恒定约束。\<br>上面这个约束方程可改写成下面形式：</p>
<script type="math/tex; mode=display">
\Delta I^Tu=-I_t</script><p>其中:</p>
<script type="math/tex; mode=display">
u=
\begin{pmatrix}
u\\v
\end{pmatrix}
\quad
\Delta I=
\begin{pmatrix}
I_x\\
I_y
\end{pmatrix}</script><p><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>*</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong>\<br>另外，对于这个公式的推导，有些地方会这么写，都一个意思：</p>
<script type="math/tex; mode=display">
\frac{dI}{dt}=\frac{\partial I}{\partial x}\frac{dx}{dt}+\frac{\partial I}{\partial y}\frac{dy}{dt}+\frac{\partial I}{\partial t}</script><p>当假设每个场景点的图像强度随时间推移而不变时(例如亮度)，则有：</p>
<script type="math/tex; mode=display">
\frac{dI}{dt}=0</script><p>则：</p>
<script type="math/tex; mode=display">
u\cdot I_x+v\cdot I_y+I_t=0</script><p>两种推导一个意思。\<br><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>*</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong>\<br>用这个约束的话有个问题，也就是下面提到的孔径问题(The Aperture Problem)。</p>
<h3 id="The-Aperture-Problem"><a href="#The-Aperture-Problem" class="headerlink" title="The Aperture Problem"></a>The Aperture Problem</h3><p>孔径问题(The Aperture Problem)的话，表现为下图这种情况<img src="/images/299.png" alt=""><br><img src="/images/300.png" alt=""><br>也就是说对于第一幅图，虽然线是往右平移的，但我们的判断却是下面这样：<img src="/images/301.png" alt=""><br>简单解释一下，现假设我们取一个单一的像素点，并得到了下面这个约束方程：</p>
<script type="math/tex; mode=display">
u\cdot I_x+v\cdot I_y+I_t=0</script><p>画出来的话如下图：<img src="/images/302.png" alt=""><br>这里有两个未知数，却只有一条线，我们没法确认具体的$u,v$，所以我们就取图中”Normal flow”方向的值(如果还有一条线确定一个交点，那就不用纠结了)。\<br>再举个例子，如下图：<img src="/images/303.png" alt=""><br>与上面情况类似，对于光圈中观察到的直线的移动，有无数种可能，上图只取其中三种，列出下面这个约束方程：</p>
<script type="math/tex; mode=display">
u\cdot I_x+v\cdot I_y+I_t=0</script><p>由于我们这种情况下我们只研究垂直方向，也就是中间这种(这是人为定的)，因为其它可能性我们没法研究，除非有先验。也就是说$v$方向没运动，所以我们有：</p>
<script type="math/tex; mode=display">
u\cdot I_x+0\cdot I_y+I_t=0</script><p>进而：</p>
<script type="math/tex; mode=display">
u=-\frac{I_t}{I_x}</script><p>现在反过来考虑，由于我们只研究$u$方向，$v$方向没运动的话，即是:$I_y=0$，此时$v$可为任意值。\<br>这就是常见到的两个结论：</p>
<blockquote>
<ol>
<li>梯度方向的光流是确定的；</li>
<li>但平行于边缘的光流是未知的。</li>
</ol>
</blockquote>
<p>(这部分不太好用文字说明，很多地方直接说“由约束方程直觉上可知”上面两个结论，图也不给，神它喵的“直觉”……)\<br>这部分知道是怎么回事就行。\<br>也就是说运动场跟光流不一定一样，举个更简单的例子，理发店门口的颜色柱<img src="/images/304.png" alt=""><br>其运动场和光流分别是：<img src="/images/305.png" alt=""></p>
<h2 id="多个约束"><a href="#多个约束" class="headerlink" title="多个约束"></a>多个约束</h2><p>上面说到，因为两个未知数一条线所以我们只能估计法线方向的光流，那现在我们找多几个点，就会有下面这种情况：<img src="/images/306.png" alt=""><br>这就确定下来了。我们能这么做是因为上面有了假设2，也就是空间连续性假设，也就是说假设光流在一个区域内是恒定的。\<br>回到上面那个式子：</p>
<script type="math/tex; mode=display">
E_{SSD}(u,v) \approx \sum_{(x,y)\in R}(u\cdot I_x(x,y,t)+v\cdot I_y(x,y,t)+I_t(x,y,t))^2</script><p>要计算运动的话这里分别对$u,v$求导并使其为0：</p>
<script type="math/tex; mode=display">
\frac{\partial}{\partial u}E_{SSD}(u,v) \approx
2\sum_{(x,y)\in R}(u\cdot I_x(x,y,t)+v\cdot I_y(x,y,t)+I_t(x,y,t))I_x(x,y,t)=0</script><script type="math/tex; mode=display">
\frac{\partial}{\partial v}E_{SSD}(u,v) \approx
2\sum_{(x,y)\in R}(u\cdot I_x(x,y,t)+v\cdot I_y(x,y,t)+I_t(x,y,t))I_y(x,y,t)=0</script><p>即：<img src="/images/307.png" alt=""><br>写成矩阵形式：<img src="/images/308.png" alt=""><br>左边第一个矩阵是对称正定矩阵，可逆，它的特征向量给出了局部图像变化的主要方向。特征值则表示其强度。\<br>此时求个逆光流$u,v$就算出来了。<br>这种方法就是LK光流法。</p>
<h2 id="Coarse-to-fine-estimation"><a href="#Coarse-to-fine-estimation" class="headerlink" title="Coarse-to-fine estimation"></a>Coarse-to-fine estimation</h2><p>这里还有个问题，LK-model要求是小运动。为解决这个问题，可以用Coarse-to-fine estimation，基本操作是：</p>
<blockquote>
<ol>
<li>建立一个高斯金字塔;</li>
<li>从最低分辨率开始，此时运动很小;</li>
<li>使用这个运动来 pre-warp 下一个更精细的尺度;</li>
<li>只计算运动增量</li>
</ol>
</blockquote>
<p>具体过程见<a href="https://blog.csdn.net/sgfmby1994/article/details/68489944">这里</a></p>
<h2 id="密集LK光流-Dense-LK-Flow"><a href="#密集LK光流-Dense-LK-Flow" class="headerlink" title="密集LK光流(Dense LK Flow)"></a>密集LK光流(Dense LK Flow)</h2><p>要计算密集LK光流的话，我们可以只把区域R看作是每个像素周围的一个小区域，并为每个像素计算一个流动矢量。\<br>对于老问题：small motions，可以用以下两种方法解决：</p>
<blockquote>
<ol>
<li>迭代估计</li>
<li>Coarse-to-fine estimation</li>
</ol>
</blockquote>
<h3 id="迭代估计"><a href="#迭代估计" class="headerlink" title="迭代估计"></a>迭代估计</h3><p>首先通过两张图计算其LK光流，如下：<img src="/images/309.png" alt=""><br>然后将第二副图根据上面计算出来的光流，向第一幅图靠拢，如下：<img src="/images/310.png" alt=""><br>再计算第一副图与warp后的第二幅图的光流，如下：<img src="/images/311.png" alt=""><br>将上面计算出来的光流跟之前的光流叠加，再将第二副图按照这个新的光流向第一幅图靠拢，如下：<img src="/images/312.png" alt=""><br>如此往复一直下去直到收敛。</p>
<h3 id="Coarse-to-fine-estimation-1"><a href="#Coarse-to-fine-estimation-1" class="headerlink" title="Coarse-to-fine estimation"></a>Coarse-to-fine estimation</h3><p>如果用高斯金字塔的话，结果会是这样子：<img src="/images/313.png" alt=""><br>跟Ground Truth做对比的话，如下：<img src="/images/314.png" alt=""><br>效果明显不太好，有些地方窗口太大了，以至于一些不连续的地方都被磨平了，有些地方窗口则太小，估计结果很差，因为窗口中没有足够的图像信息。\<br>终究，LK光流法只是一种局部光流法，至于全局光流法，见另外两篇。</p>
<h1 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h1><p>这个相关代码都会上传到我的Github上，但由于攒了十几个代码都要修改上传，最近又很多事，所以如果Github上没有，那就是还没上传。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>【1】 <a href="https://blog.csdn.net/sgfmby1994/article/details/68489944">https://blog.csdn.net/sgfmby1994/article/details/68489944</a></p>
]]></content>
      <categories>
        <category>图像处理</category>
      </categories>
  </entry>
  <entry>
    <title>拟合优度检验</title>
    <url>/posts/23.html</url>
    <content><![CDATA[<h1 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h1><p>拟合优度检验是用来检测提出的模型与数据是否一致，这在数据科学领域很重要，因为通过这种检测可以知道：</p>
<blockquote>
<ol>
<li>数据是否符合我们提出的假设（比如高斯性）;\</li>
<li>两个数据集的分布是否可以假定为相同。</li>
</ol>
</blockquote>
<span id="more"></span>
<p>一般来说：</p>
<blockquote>
<ol>
<li><strong>离散分布</strong>对应于<strong>Chi-square test</strong> ;\</li>
<li><strong>连续分布</strong>对应于<strong>Kolmogorov Smirnov</strong>。</li>
</ol>
</blockquote>
<h1 id="Chi-square-test"><a href="#Chi-square-test" class="headerlink" title="Chi-square test"></a>Chi-square test</h1><p>这个就是经常能听到的卡方检测，卡方检测一般有两种，分别为拟合度的卡方检验和卡方独立性检验，这里重点介绍第一种。\</p>
<h2 id="拟合度卡方检测"><a href="#拟合度卡方检测" class="headerlink" title="拟合度卡方检测"></a>拟合度卡方检测</h2><p>简单定义的话，就是使用样本数据检验总体分布形态或者比例的假说，或者说得再清楚一点，检验该样本的分布比例与总体分布比例的拟合程度。\<br>拟合度卡方检测又分为如下两种情况：\</p>
<h3 id="case-1-所有参数都是确定的"><a href="#case-1-所有参数都是确定的" class="headerlink" title="case 1: 所有参数都是确定的"></a>case 1: 所有参数都是确定的</h3><p>由于这是检验分布比例的，所以零假设和备选假设可以写成这样：</p>
<blockquote>
<ol>
<li>$H_0$: $P\left\{ Y=i\right\}=p_i$ vs $H_1$: $P\left\{ Y=i\right\}\neq p_i$, $i\in$ {$k$ discrete outcomes}。\</li>
</ol>
</blockquote>
<p>定义如下变量：\</p>
<blockquote>
<p>$X_i$ 为输出结果 $i$ 在 $n$ 次重复试验中出现的次数，单看结果 $i$ 的话，可以看成是二项分布，所以有 $E[X_i]=np_i$。\</p>
</blockquote>
<p>检测统计量为：</p>
<script type="math/tex; mode=display">
T=\sum_i\frac{(x_i-np_i)^2}{np_i}</script><p>当次数 $n$ 很大，且零假设成立时，$T$ 可近似看作是 $k-1$ 个自由度的卡方分布，就可以查阅卡方表进行比较了。\</p>
<p>最后根据设置的 $\alpha$ 值，当 $T \geq \chi_{\alpha,k-1}^2$ 时，拒绝零假设，否则接受。\</p>
<h4 id="一个例子"><a href="#一个例子" class="headerlink" title="一个例子"></a>一个例子</h4><p>在一个试验中共有6个可能的输出分别是 $a,b,c,d,e,f$，现假设其出现概率分别为：0.1，0.1，0.05，0.4，0.2，0.15，试验重复40次所得到的各结果频数分别是：3，3，5，18，4，7。问：一开始那个频率假设是否合理？\<br><strong>零假设$H_0$</strong>：\</p>
<script type="math/tex; mode=display">
P\left\{ Y=a\right\}=0.1, \quad P\left\{ Y=b\right\}=0.1, \quad P\left\{ Y=c\right\}=0.05</script><script type="math/tex; mode=display">
P\left\{ Y=d\right\}=0.4, \quad P\left\{ Y=e\right\}=0.2, \quad P\left\{ Y=f\right\}=0.15</script><p><strong>计算统计量</strong>\<br>将 $n=40$，6个$p_i$代入 $T=\sum_i\frac{(x_i-np_i)^2}{np_i}$ 求出 $T=7.416666667$。\<br><strong>查表</strong>\<br>设$\alpha=0.05$，自由度$df=6-1=5$，查表得: $\chi_{0.05,5}^2=11.070$。\<br><strong>决策</strong>\<br>由于 $T \leq \chi_{0.05,5}^2$，所以接受零假设。</p>
<h3 id="case-2-某些参数都是未确定的"><a href="#case-2-某些参数都是未确定的" class="headerlink" title="case 2: 某些参数都是未确定的"></a>case 2: 某些参数都是未确定的</h3><p>与上面$n$和$p_i$都确定的情况不同，这里有些参数是不确定的，需要我们根据其它办法(比如MLE)近似确定。\<br>假设我们有 $m$ 个未指定的参数，需要我们用MLE来确定，比如说泊松分布 $Y\sim Pois(\lambda)$ 的均值的MLE估计为: $\hat{\lambda}=\frac{1}{n}\sum_{j=1}^ny_j$;\<br>测试统计量为：</p>
<script type="math/tex; mode=display">
T=\sum_i\frac{(x_i-n\hat{p}_i)^2}{n\hat{p}_i}</script><p>这里的$\hat{p}_i$是估计参数。\<br>比如说泊松分布的$pmf$为： $P(Y=i)=\frac{e^{-\lambda}\lambda^i}{i!}$\<br>将数据分为$k$组，每组进行分布统计。\<br>若试验次数$n$足够大且零假设为真，则$T$近似为卡方分布，自由度为 $k-1-m$。\<br>对于设定值$\alpha$，若 $T \geq \chi_{\alpha,k-1-m}^2$，则拒绝零假设，否则接受。</p>
<h4 id="一个例子-1"><a href="#一个例子-1" class="headerlink" title="一个例子"></a>一个例子</h4><p>我们有下面30个数据，表明30周里每周车祸发生的次数，现在需要验证一周事故发生次数服从泊松分布。<img src="/images/265.png" alt=""><br>我们将数据分成如下5组：</p>
<script type="math/tex; mode=display">
(1).Y=0,\quad (2).Y=1,\quad (3).Y=2\;or\; 3, \quad (4). Y=4\;or\;5,\quad (5). Y>5</script><p><strong>步骤1</strong>\<br>首先我们对数据根据分组进行统计，结果如下：<br><img src="/images/266.png" alt=""><br><strong>步骤2</strong>\<br>计算估计参数 $\hat{\lambda}$；\<br>根据上面泊松分布的MLE计算，这个参数就是数据的均值了，等于：$\hat{\lambda}=3.167$；\<br><strong>步骤3</strong>\<br>根据求出的估计参数，代入泊松分布的 $pmf$ 中，计算各个组的概率：</p>
<script type="math/tex; mode=display">
\hat{p}_1=0.042,\quad \hat{p}_2=0.133,\quad \hat{p}_3=0.434,\quad \hat{p}_4=0.288, \quad \hat{p}_5=0.102</script><p><strong>步骤4</strong>\<br>计算验证统计量：</p>
<script type="math/tex; mode=display">
T=\sum_{i=1}^5=\frac{(x_i-30\hat{p}_i)^2}{30\hat{p}_i}=21.99</script><p><strong>步骤5</strong>\<br>查卡方表，根据 $\alpha=0.05$，自由度$df=k-1-m=5-1-1=3$ 查得：$\chi_{0.05,3}^2=7.815$。\<br><strong>步骤6</strong>\<br>决策，由于$21.99\geq 7.815$，所以拒绝零假设，一周事故频数不服从泊松分布。\</p>
<h2 id="卡方独立性检验"><a href="#卡方独立性检验" class="headerlink" title="卡方独立性检验"></a>卡方独立性检验</h2><p>这里只是简单介绍怎么用这个检验，要具体推导的话看<a href="https://zhuanlan.zhihu.com/p/131286213">这里</a>。\<br>卡方独立性检验是为了检验两个变量是否独立，比如说性别与考试通过率是否独立，或者会不会说英语与收到的offer是否独立等。\<br>这里直接用链接里的这个例子，两个变量分别是性别与分期与否。\<br>观测值如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">分期</th>
<th style="text-align:center">不分期</th>
<th style="text-align:center"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">男</td>
<td style="text-align:center">90</td>
<td style="text-align:center">110</td>
<td style="text-align:center">200</td>
</tr>
<tr>
<td style="text-align:center">女</td>
<td style="text-align:center">30</td>
<td style="text-align:center">70</td>
<td style="text-align:center">100</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center">120</td>
<td style="text-align:center">180</td>
</tr>
</tbody>
</table>
</div>
<p>这里的 90，110，30，70分别用$o_1,o_2,o_3,o_4$表示。\<br>做零假设：两变量独立。\<br>根据这个零假设，我们期望的值应该是（就是比例相同）：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">分期</th>
<th style="text-align:center">不分期</th>
<th style="text-align:center"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">男</td>
<td style="text-align:center">80</td>
<td style="text-align:center">120</td>
<td style="text-align:center">200</td>
</tr>
<tr>
<td style="text-align:center">女</td>
<td style="text-align:center">40</td>
<td style="text-align:center">60</td>
<td style="text-align:center">100</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center">120</td>
<td style="text-align:center">180</td>
</tr>
</tbody>
</table>
</div>
<p>这里的80，120，40，60分别用$e_1,e_2,e_3,e_4$表示。\<br>计算卡方统计量：</p>
<script type="math/tex; mode=display">
X=\sum_{i=1}^4\frac{(o_i-e_i)^2}{e_i}=6.25</script><p>查表验证。</p>
<h1 id="Kolmogorov-Smirnov"><a href="#Kolmogorov-Smirnov" class="headerlink" title="Kolmogorov Smirnov"></a>Kolmogorov Smirnov</h1><p>这种检测方法是基本思路就是，比较理论的经验累积分布与观测的经验累积分布，求出最大偏离值，然后判断这种偏离值是不是偶然出现的。\<br>比如说现在有一个连续分布的样本数据：$y_1,…,y_n$。\<br>我们做出零假设：$F$ 是总体连续分布。\<br>现在验证这个零假设，有两种方法：</p>
<blockquote>
<ol>
<li>将该分布分成不同区间，然后用上面的卡方检测；\</li>
<li>用 K-S 检验。</li>
</ol>
</blockquote>
<p>举个例子，现在有$n=5$个数据：$y_1,y_2,y_3,y_4,y_5$，根据这5个数据得出的经验累积函数为：</p>
<script type="math/tex; mode=display">
F_e(x)=\frac{\#i:y_i \leq x}{n}</script><p>零假设对应的函数为$F$，画图如下：<img src="/images/267.png" alt=""><br>如果零假设成立，则$F_e(x)$应该很接近于$F(x)$。\<br>K-S 检测的统计量为（就是最大距离）：</p>
<script type="math/tex; mode=display">
D \equiv Maximum_x |F_e(x)-F(x)|</script><p>下图是sample test: Use the One sample Kolmogorov Smirnov table：<img src="/images/268.png" alt=""><br>(至于sample tests use the two sample Kolmorogov Smirnov table ，在<a href="https://www.real-statistics.com/statistics-tables/two-sample-kolmogorov-smirnov-table/">这里</a>)\<br>根据$\alpha$和$n$就可以读出临界值，两者比较，当最大值小于临界值时，接受零假设，否则拒绝。\<br><strong>简单说一下双样本的</strong><br>双样本检测也差不多，只是多了一组样本，表也多了一个维度，计算距离也不太一样，就，样本数据很大时，距离公式为：</p>
<script type="math/tex; mode=display">
D_{\alpha}=c(\alpha)\sqrt{\frac{n_1+n_2}{n_1n_2}}</script><p>系数$c(\alpha)$为：<img src="/images/269.png" alt=""></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>直接附图了这里 <img src="/images/270.png" alt=""></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>【1】：<a href="https://zhuanlan.zhihu.com/p/131286213">https://zhuanlan.zhihu.com/p/131286213</a></p>
<p>【2】：<a href="https://qinqianshan.com/math/significance_test/kolmogorovsmirnov/">https://qinqianshan.com/math/significance_test/kolmogorovsmirnov/</a></p>
]]></content>
      <categories>
        <category>Data Science</category>
      </categories>
  </entry>
  <entry>
    <title>RANSAC算法</title>
    <url>/posts/27.html</url>
    <content><![CDATA[<h1 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h1><p>当我们有一组数据需要拟合模型的时候，最简单的是用最小二乘法，又叫线性回归去拟合，然而这种方法是用所有数据去拟合，如果所有数据都是跟模型有关的话，那效果应该还行，但如果我们的数据集有很多数据是属于”outliers”的话，效果就会很差，这时候可以用这篇的这个算法：RANSAC算法。<br><span id="more"></span></p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>用这个算法的前提是，我们假设：我们的数据是由”inliers”和”outliers”组成的，并且这个”outliers”还不少(少的话也可以用，看有没有必要而已)，而且我们需要知道这个”inliers”大概占了多少，这个量属于先验，需要事先知道的。实际实现算法时我们还需要确定的有以下几个参数</p>
<blockquote>
<ol>
<li>得到正确模型的概率：字面意思，就是说有多大概率能得到正确模型，即是选出的用于拟合模型的点都是”inliers”，这个参数是用来算最大迭代次数的;</li>
<li>迭代次数：这个可以根据上面的概率来算，下面会解释;</li>
<li>模型参数的数目：也就是说我们要知道拟合的这个模型我们需要多少参数，比如直线是2个，Homography矩阵是4个;</li>
<li>阈值：即是判断点属于”inliers”还是”outliers”。</li>
</ol>
</blockquote>
<h2 id="计算迭代次数"><a href="#计算迭代次数" class="headerlink" title="计算迭代次数"></a>计算迭代次数</h2><p>比如说，所有数据中”inliers”占的比重为$w$，拟合模型需要$d$个参数，也就是说$d$个数据点，那么我们从数据中选出$d$个数据都是”inliers”的概率是: $w^d$，至少有一个”outliers”的概率是 $(1-w^d)$，$k$次循环每次都至少有一个”outliers”的概率是：</p>
<script type="math/tex; mode=display">
(1-w^d)^k</script><p>那么选到正确$d$个”inliers”的概率是：</p>
<script type="math/tex; mode=display">
P=1-(1-w^d)^k</script><p>求得：</p>
<script type="math/tex; mode=display">
k=\frac{\log(1-P)}{\log(1-w^d)}</script><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p>(因为这个算法不难，这个代码也是随手写的，所以这个代码风格很不好，参数命名也不太行。所以这东西，大概看看就好)。\<br>这里是用最简单的直线拟合，有40个”inliers”，都添加了高斯噪音，并且有60个随机的”outliers”，代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Gauss_noise</span>(<span class="params">data</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        data: ndarray, (2,n)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    noise_x = np.random.normal(<span class="number">0</span>,<span class="number">1</span>,<span class="built_in">len</span>(data[<span class="number">0</span>]))</span><br><span class="line">    noise_y = np.random.normal(<span class="number">0</span>,<span class="number">1</span>,<span class="built_in">len</span>(data[<span class="number">1</span>]))</span><br><span class="line">    data[<span class="number">1</span>] += noise_y</span><br><span class="line">    data[<span class="number">0</span>] += noise_x</span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">real_model</span>(<span class="params">x</span>):</span></span><br><span class="line">    a = <span class="number">4</span></span><br><span class="line">    b = <span class="number">6.9</span></span><br><span class="line">    y = a * x + b</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">outliers</span>(<span class="params">nums = <span class="number">60</span></span>):</span></span><br><span class="line">    down = <span class="number">0</span></span><br><span class="line">    up = <span class="number">40</span></span><br><span class="line">    outliers = np.random.rand(<span class="number">2</span>, nums)*<span class="number">60</span></span><br><span class="line">    <span class="keyword">return</span> outliers</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">inliers_nums = <span class="number">40</span></span><br><span class="line">outliers_nums = <span class="number">60</span></span><br><span class="line">x = np.linspace(<span class="number">0</span>,<span class="number">20</span>,inliers_nums).reshape(<span class="number">1</span>,-<span class="number">1</span>)</span><br><span class="line">y = real_model(x)</span><br><span class="line">inliers = np.concatenate((x,y), axis=<span class="number">0</span>)</span><br><span class="line">inliers = Gauss_noise(inliers)</span><br><span class="line">outliers = outliers(outliers_nums)</span><br><span class="line"></span><br><span class="line">data = np.concatenate((inliers, outliers),axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">data_nums = <span class="built_in">len</span>(data[<span class="number">0</span>])</span><br><span class="line">    </span><br><span class="line">estimate_a = <span class="number">0.</span></span><br><span class="line">estimate_b = <span class="number">0.</span></span><br><span class="line"></span><br><span class="line">accuracy = <span class="number">0.99</span></span><br><span class="line">error = <span class="number">0.8</span></span><br><span class="line">current_inliers = <span class="number">0</span></span><br><span class="line">curr_iter = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">accept_nums = <span class="built_in">int</span>(<span class="number">0.4</span> * data_nums)</span><br><span class="line"></span><br><span class="line">max_iter = np.log(<span class="number">1</span> - accuracy) / np.log(<span class="number">1</span> - (inliers_nums / data_nums)**<span class="number">2</span>)</span><br><span class="line"><span class="keyword">while</span> current_inliers &lt; accept_nums <span class="keyword">and</span> curr_iter &lt; max_iter:</span><br><span class="line">    curr_iter += <span class="number">1</span></span><br><span class="line">    ids = np.random.choice(<span class="built_in">range</span>(data_nums), <span class="number">2</span>, replace = <span class="literal">False</span>)</span><br><span class="line">    x_1 = data[<span class="number">0</span>,ids[<span class="number">0</span>]]</span><br><span class="line">    x_2 = data[<span class="number">0</span>,ids[<span class="number">1</span>]]</span><br><span class="line">    y_1 = data[<span class="number">1</span>,ids[<span class="number">0</span>]]</span><br><span class="line">    y_2 = data[<span class="number">1</span>,ids[<span class="number">1</span>]]</span><br><span class="line"></span><br><span class="line">    a = (y_2 - y_1) / (x_2 - x_1)</span><br><span class="line">    b = y_1 - a * x_1</span><br><span class="line"></span><br><span class="line">    tmp_inliers = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(data_nums):</span><br><span class="line">        y = a * data[<span class="number">0</span>,i] + b</span><br><span class="line">        <span class="keyword">if</span> np.<span class="built_in">abs</span>(data[<span class="number">1</span>,i] - y) &lt; error:</span><br><span class="line">            tmp_inliers += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> tmp_inliers &gt;= current_inliers:</span><br><span class="line">        estimate_a = a</span><br><span class="line">        estimate_b = b</span><br><span class="line">        current_inliers = tmp_inliers</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.scatter(data[<span class="number">0</span>],data[<span class="number">1</span>])</span><br><span class="line">plt.plot(data[<span class="number">0</span>,:<span class="number">40</span>], estimate_a * data[<span class="number">0</span>,:<span class="number">40</span>] + estimate_b, color=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>效果如下：<br><img src="/images/285.png" alt=""></p>
<p>再提一个，既然这个算法可以用来区分”inliers”和”outliers”，那要是我先用这个算法把”inliers”分出来，然后用线性回归，效果会不会更好？\<br>然而试验发现，好是有好那么一点点吧，主要是那个bias更接近了，但区别真的不大，代码写得很乱就不贴了，反正貌似也没啥意义。。。</p>
]]></content>
      <categories>
        <category>未分类</category>
      </categories>
  </entry>
  <entry>
    <title>相机模型与标定</title>
    <url>/posts/26.html</url>
    <content><![CDATA[<h1 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h1><p>相机模型化的话，说白了就是一个函数，把一个三维场景转化为二维图像。这里主要是介绍基本相机模型，世界、相机、图像坐标系之间的转换以及相机的标定。<br><span id="more"></span></p>
<h1 id="相机模型"><a href="#相机模型" class="headerlink" title="相机模型"></a>相机模型</h1><p>这里用最简单也是最广泛的针孔相机模型，单目的，也就是下面这个：<img src="/images/272.png" alt=""><br>从3D到2D的转换中我们遗失了<strong>角度</strong>以及<strong>距离</strong>信息。\<br>假设点$P$的三维坐标是$[x,y,z]$，其对应图像坐标为$[x^,,y^,]$，有如下关系：<img src="/images/273.png" alt=""><br>（就是简单的相似三角形）\<br>也就是说:</p>
<script type="math/tex; mode=display">
(x,y,z) \rightarrow (f'\frac{x}{z},f'\frac{y}{z})</script><p>用齐次坐标表示的话为：<img src="/images/274.png" alt=""><br>在很多地方左边的那个等式会写成这样子：</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
    f'&0&0&0\\
    0&f'&0&0\\
    0&0&1&0
\end{pmatrix}
\begin{pmatrix}
    x\\
    y\\
    z\\
    1
\end{pmatrix}
=z
\begin{pmatrix}
    f'\frac{x}{z}\\
    f'\frac{y}{z}\\
    1
\end{pmatrix}</script><p>虽然看上去只是把系数提出来而已，但表达的意思却多了一个：对于同一个图像坐标，其三维坐标有无数种可能，只要在投影线上就行。这是引进齐次坐标的一个很大的特点。</p>
<h1 id="坐标系转变"><a href="#坐标系转变" class="headerlink" title="坐标系转变"></a>坐标系转变</h1><p>上面那种情况是最简单的，但一般来说，相机在世界坐标系中的位置是不定的，所以要在中间引入一个新的坐标系————相机坐标系，又叫归一化坐标系。\<br>所以这里要用到三个坐标系，分别为：世界坐标系，相机坐标系和图像坐标系，它们的转变过程如下：<img src="/images/275.png" alt=""><br>也就是：</p>
<blockquote>
<p>首先将世界坐标转为相机坐标\<br>然后将相机坐标转为图像坐标</p>
</blockquote>
<h2 id="世界坐标到相机坐标"><a href="#世界坐标到相机坐标" class="headerlink" title="世界坐标到相机坐标"></a>世界坐标到相机坐标</h2><p>(这个过程也就是归一化的过程)\<br>相机坐标与世界坐标是旋转加平移的关系，即是如下：</p>
<script type="math/tex; mode=display">
\widetilde{x}^C=R(\widetilde{x}^W-\widetilde{c})</script><p>其中：$\widetilde{c}$是相机中心在世界坐标系中的位置。\<br>写成齐次形式的话就是：<img src="/images/276.png" alt=""><br>这是一种线性变换的形式，这时候维度还没有遗失\<br>这个转换矩阵写简洁一点即是：</p>
<script type="math/tex; mode=display">
x^C=
\begin{bmatrix}
    R&t\\
    0&1
\end{bmatrix}
x^W</script><p>其中$t=-R\widetilde{c}$</p>
<h2 id="相机坐标到图像坐标"><a href="#相机坐标到图像坐标" class="headerlink" title="相机坐标到图像坐标"></a>相机坐标到图像坐标</h2><p>这里先给三个定义：</p>
<blockquote>
<ol>
<li>主轴(Principal axis)：从相机中心垂直于图像平面的线；\</li>
<li>归一化(相机)坐标系(Normalized (camera) coordinate system)：以相机中心为原点，z轴为主轴的坐标系\</li>
<li>主点(Principal point)：主轴与图像平面相交的点(归一化坐标系的原点）</li>
</ol>
</blockquote>
<p>即是下图这样：<img src="/images/277.png" alt=""></p>
<p>相机坐标系的原点在主点上，而图像坐标系的原点在角落(一般是左上角)，也就是说，两个坐标系存在着偏移量，以图像坐标系为参考的话，该偏移量为$(p_x,p_y)$。\<br>将最上面那个投影转换写下来，对于三维坐标点$(X,Y,Z)$，其图像坐标为$(f\frac{X}{Z},f\frac{Y}{Z})$，即：</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
    f&0&0&0\\
    0&f&0&0\\
    0&0&1&0
\end{pmatrix}
\begin{pmatrix}
    X\\
    Y\\
    Z\\
    1
\end{pmatrix}
=
\begin{pmatrix}
    fX\\
    fY\\
    Z
\end{pmatrix}</script><p>写成紧凑一些的形式为：</p>
<script type="math/tex; mode=display">
x^I=K\cdot [I|0]\cdot x^C=K\cdot \widetilde{x}^C</script><p>其中：</p>
<script type="math/tex; mode=display">
K=diag\left\{f,f,1\right\}</script><p>(注意，这里有个非齐次向齐次转换的计算)\<br>然后考虑这个中心偏移量，即对于三维坐标$(X,Y,Z)$，其图像坐标应为$f\frac{X}{Z}+p_x,f\frac{Y}{Z}+p_y$，写成矩阵计算形式如下：</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
    f&0&p_x&0\\
    0&f&p_y&0\\
    0&0&1&0
\end{pmatrix}
\begin{pmatrix}
    X\\
    Y\\
    Z\\
    1
\end{pmatrix}
=
\begin{pmatrix}
    fX+Zp_x\\
    fY+Zp_y\\
    Z
\end{pmatrix}</script><p>这里对应的的$K$为标定矩阵(calibration matrix)，即：</p>
<script type="math/tex; mode=display">
K=
\begin{pmatrix}
    f&0&p_x\\
    0&f&p_y\\
    0&0&1
\end{pmatrix}</script><p>也就是输入一个归一化的世界坐标，通过矩阵$K$可以得到其对应的图像坐标。\<br>但是这里还有一个问题，就是说对于世界坐标系，我们用的单位一般会是“米”，“厘米”，“英寸”之类的，但在图像坐标系中我们统一使用“像素”为单位，所以这还需要一个转换过程：</p>
<blockquote>
<p>假设在水平方向上以某一单位(m,mm,inch,…)为基准共有$m_x$个像素，在垂直方向上有$m_y$个像素。</p>
</blockquote>
<p>就有如下关系：</p>
<script type="math/tex; mode=display">
K=
\begin{pmatrix}
    m_x&0&0\\
    0&m_y&0\\
    0&0&1
\end{pmatrix}
\begin{pmatrix}
    f&0&p_x\\
    0&f&p_y\\
    0&0&1
\end{pmatrix}
=
\begin{pmatrix}
    \alpha_x&0&\beta_x\\
    0&\alpha_y&\beta_y\\
    0&0&1
\end{pmatrix}</script><p>以上这些就是相机的主要内参数，包括有：</p>
<blockquote>
<ol>
<li>主点坐标;\</li>
<li>焦距;\</li>
<li>像素放大系数;\</li>
<li>对于非矩形像素，还有偏斜系数(Skew)，这里不讨论。\</li>
</ol>
</blockquote>
<p>常见的还有畸变参数，这个是为了补充内参数的，下面会讨论。\<br>这些参数是相机在出厂时就设定好的，一般不会轻易改变。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>用一张图简单总结下：<img src="/images/278.png" alt=""><br>这里的矩阵$P$是投影矩阵，即是给一个齐次世界坐标，通过矩阵$P$，可以得到对应的齐次图像坐标，其中$K\in R^{3\times 3}$，$P\in R^{3\times 4}$。</p>
<h1 id="正投影模型"><a href="#正投影模型" class="headerlink" title="正投影模型"></a>正投影模型</h1><p>关于这个简单说一下就好，正投影，也就是平行投影，形式如下：<img src="/images/279.png" alt=""><br>其投影矩阵为：<img src="/images/280.png" alt=""></p>
<h1 id="相机标定"><a href="#相机标定" class="headerlink" title="相机标定"></a>相机标定</h1><p>相机标定就是根据某些方法确定出相机的内参数和畸变参数。</p>
<h2 id="不考虑畸变"><a href="#不考虑畸变" class="headerlink" title="不考虑畸变"></a>不考虑畸变</h2><p>先假设我们有三维坐标点$X_i$，以及对应的图像坐标点$x_i$，都是齐次的，现在想要估计投影矩阵$P$。\<br>由于共线性，有如下关系：</p>
<script type="math/tex; mode=display">
\lambda x_i=PX_i</script><p>即：</p>
<script type="math/tex; mode=display">
\lambda
\begin{pmatrix}
    x_i\\
    y_i\\
    1
\end{pmatrix}=
\begin{bmatrix}
    p_1^T\\
    p_2^T\\
    p_3^T
\end{bmatrix}X_i</script><p>这里$p_j^T$表示矩阵$P$的第$j$行。\<br>用叉积：</p>
<script type="math/tex; mode=display">
x_i \times PX_i=0</script><p>即：</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
    y_ip_3^TX_i-p_2^TX_i\\
    p_1^TX_i-x_ip_3^TX_i\\
    x_ip_2^TX_i-y_ip_1^TX_i
\end{bmatrix}=0</script><p>写成矩阵乘积形式为：</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
    0&-X_i^T&y_iX_i^T\\
    X_i^T&0&-x_iX_i^T\\
    -y_iX_i^T&x_iX_i^T&0
\end{bmatrix}
\begin{pmatrix}
    p_1\\
    p_2\\
    p_3
\end{pmatrix}=0</script><p>这个矩阵$P$虽然有12个参数，但由于尺度是随意的，所以实际只要考虑11个自由度就行，然后由于上面那个矩阵等式只提供了两个线性独立等式(因为秩为2)，所以为了求解矩阵$P$，我们最少需要6个 3D/2D 对应点。如果我们有多于6个数据对的话，那就更好了，可以用最小二乘法估计。\<br>假设我们现在有很多个数据对，根据上面的方法写成如下形式：<img src="/images/281.png" alt=""><br>简单记为：</p>
<script type="math/tex; mode=display">
Ap=0</script><p>由于当$p=0$时虽然符合方程，但明显不是我们想要的，要排除，所以加个约束：$||p||=1$，也就是如下齐次最小二乘形式：</p>
<script type="math/tex; mode=display">
Ap=0\quad s.t. \quad ||p||=1</script><p>先说解法步骤：</p>
<blockquote>
<ol>
<li>进行SVD分解：$A=USV^T$</li>
<li>假设奇异值是排好序了的，即$S=diag(s_1,…,s_{12}),s_{i+1}\leq s_i$</li>
<li>取最后一个右奇异值向量，此时$p=v_{12}$</li>
</ol>
</blockquote>
<p>为什么是这样？下面简单说明。</p>
<h3 id="齐次最小二乘：简介"><a href="#齐次最小二乘：简介" class="headerlink" title="齐次最小二乘：简介"></a>齐次最小二乘：简介</h3><p>现假设向量$p\in R^n$满足：</p>
<script type="math/tex; mode=display">
Ap=0</script><p>其中$A\in R^{m\times n}$且$0\in R^m$，进一步假设$m\geq n$且$rank(A)=n$。\<br>由于$A$是测量矩阵，就难免有噪音，所以我们会最小化下面这个：</p>
<script type="math/tex; mode=display">
||Ap||^2</script><p>为了避免$p=0$这个解，我们在这里加个约束$||p||=1$，也就是说现在我们的问题变为：\</p>
<blockquote>
<p>在约束$||p||=1$的条件下，寻找$p$，使得$||Ap||^2$最小。</p>
</blockquote>
<p>现在开始解这个问题。\<br>首先我们改写这个约束条件为：$1-p^Tp=0$，然后用拉格朗日乘子：</p>
<script type="math/tex; mode=display">
\frac{\partial}{\partial p}(p^TA^TA^Tp+\lambda (1-p^Tp))=0</script><p>即：</p>
<script type="math/tex; mode=display">
2A^TAp-2\lambda p=0 \quad \rightarrow \quad (A^TA)p=\lambda p</script><p>这里$p$是$(A^TA)$的特征向量，$\lambda$是特征值。\<br>这就引出了特征值和特征向量了，但现在还有个问题，我们要选哪个特征值/向量？\<br>回到我们原来的问题，我们要最小化如下的量：</p>
<script type="math/tex; mode=display">
||Ap||^2=p^TA^TAp</script><p>用约束$||p||=1$改写一下如下：</p>
<script type="math/tex; mode=display">
||Ap||^2=p^T\lambda p=\lambda</script><p>这就出来了，我们要选的$p$就是$A^TA$最小特征值对应的特征向量。</p>
<p>然后继续，毕竟上面我们的对象是$A^TA$，不是$A$，现在要引进SVD分解。</p>
<h3 id="齐次最小二乘：SVD"><a href="#齐次最小二乘：SVD" class="headerlink" title="齐次最小二乘：SVD"></a>齐次最小二乘：SVD</h3><p>SVD，即是奇异值分解，全称为singular value decomposition，表示成如下形式：</p>
<script type="math/tex; mode=display">
A=USV^T</script><p>其中$U\in R^{m\times n}$，$V\in R^{n\times n}$是正交的(即是逆等于转置，行列式绝对值为1的矩阵)，$S\in R^{n\times n}$是对角矩阵，且值沿对角线递减。\<br>现在我们有：</p>
<script type="math/tex; mode=display">
A^TA=(USV^T)^TUSV^T=VS^TU^TUSV^T=VS^2V^T</script><p>上面这个式子就是一个特征值分解(正交性)，所以我们可以看到：</p>
<blockquote>
<ol>
<li>$A^TA$的特征向量就是$A$的右奇异向量；</li>
<li>$A^TA$的特征值是$A$奇异值的平方。</li>
</ol>
</blockquote>
<p>所以，要找$A^TA$的最小特征值，就是要计算$A$的最右边的奇异向量。\<br>至此就回答完上面的问题了。\<br>关于SVD分解，详细的可以参看<a href="https://shartoo.github.io/2016/10/25/SVD-decomponent/">这篇</a>。</p>
<h3 id="通过-P-求-K"><a href="#通过-P-求-K" class="headerlink" title="通过$P$求$K$"></a>通过$P$求$K$</h3><p>至此我们就求出了投影矩阵$P$，它包含了相机的外参和内参，但相机的标定只要求内参，所以我们要继续往下做。\<br>这里矩阵$P$是一个$3\times 4$的矩阵，我们把它按如下方式分解：</p>
<script type="math/tex; mode=display">
P=[M|m]=[KR|-KR\widetilde{c}]</script><p>这里矩阵$M\in R^{3\times 3},m\in R^{3\times 1}$，然后将矩阵$M$进行QR分解成一个上三角矩阵$K$和一个正交的矩阵$R$，这里的$K$就是我们要的标定矩阵，$R$是旋转矩阵。最后，通过SVD找到c作为P的零空间。\<br>至此，这种不考虑畸变的标定就结束了。</p>
<h2 id="考虑畸变参数"><a href="#考虑畸变参数" class="headerlink" title="考虑畸变参数"></a>考虑畸变参数</h2><p>上面是特殊情况，一般在现实中我们是不能忽略畸变参数的，在进行标定前，先对畸变参数作简要说明。\</p>
<h3 id="畸变参数"><a href="#畸变参数" class="headerlink" title="畸变参数"></a>畸变参数</h3><p>根据畸变的方式不同畸变参数可分为两部分，径向畸变和切向畸变(还有其他的比如薄透镜畸变，这里不考虑)。</p>
<h4 id="径向畸变"><a href="#径向畸变" class="headerlink" title="径向畸变"></a>径向畸变</h4><p>径向畸变是由于透镜形状的制造工艺导致。且越向透镜边缘移动径向畸变越严重。\<br>比如说这是我们正常的图像：<img src="/images/282.png" alt=""><br>其中的像素点坐标我们用极坐标来表示，为$(r,\theta)$，径向畸变的话，就是$r$的缩放，其又分为两种，分别为桶形畸变和枕形畸变，如下：<img src="/images/283.png" alt=""><br>其中桶形畸变大都发生在使用广角镜头或使用变焦镜头的广角端时，枕形畸变则是使用长焦镜头或使用变焦镜头的长焦端时发生。\<br>用数学形式表示的话，我们先假定归一化平面上有一点$p$，坐标为$[x,y]^T$，对应的极坐标形式为$[r,\theta]^T$，其中$r$表示与坐标系原点的距离，$\theta$表示与水平轴的夹角。径向畸变的话就是$r$发生了变化，畸变后的坐标可表示为：</p>
<script type="math/tex; mode=display">
x_{distorted}=x(1+k_1r^2+k_2r^4+k_3r^6)</script><script type="math/tex; mode=display">
y_{distorted}=y(1+k_1r^2+k_2r^4+k_3r^6)</script><p>这里的$k_1,k_2,k_3$即是径向畸变参数。\<br>这里的多项式不一定非得是这几个，可以多写几项或少些几项，看自己需求吧，不过大都用这几个表示。\</p>
<h4 id="切向畸变"><a href="#切向畸变" class="headerlink" title="切向畸变"></a>切向畸变</h4><p>切向畸变是由于透镜本身与相机传感器平面（成像平面）或图像平面不平行而产生的，其畸变来源示意图如下：<img src="/images/284.png" alt=""><br>同样可用数学表示如下：</p>
<script type="math/tex; mode=display">
x_{distorted}=x+2p_1xy+p_2(r^2+2x^2)</script><script type="math/tex; mode=display">
y_{distorted}=y+p_1(r^2+2y^2)+2P_2xy</script><p>这里的$p_1,p_2$就是切向畸变参数。</p>
<h4 id="求解畸变参数"><a href="#求解畸变参数" class="headerlink" title="求解畸变参数"></a>求解畸变参数</h4><p>首先，根据上面两类畸变可知，一个畸变后的坐标应该是这样的：</p>
<script type="math/tex; mode=display">
x_{distorted}=x(1+k_1r^2+k_2r^4+k_3r^6)+2p_1xy+p_2(r^2+2x^2)</script><script type="math/tex; mode=display">
y_{distorted}=y(1+k_1r^2+k_2r^4+k_3r^6)+p_1(r^2+2y^2)+2P_2xy</script><p>这里涉及到5个畸变参数，分别为：$k_1,k_2,k_3,p_1,p_2$。\<br>畸变的话不能直接求解，但可以用优化的思想，目标函数选用最小化重投影误差，也就是将空间坐标点按照估计的投影方程投影到图像上，得到像素估计值，使该值与实际观测值之间的误差最小。\<br>其中变量初始值设为：畸变参数为0，其余参数用上面无畸变标定的方法的结果。\<br>注意，这里的优化变量不只是畸变参数，还有其余内参。\<br>至于其中的优化细节，一大堆公式还没看，留着以后吧。\<br>其实直接用OpenCV或者Matlab就可以弄出来了，尤其是Matlab，真-傻瓜式操作，不过还是得知道这些内部算法的。这个优化细节，以后有心情的话看完再补充了。</p>
<h1 id="附"><a href="#附" class="headerlink" title="附"></a>附</h1><p>关于坐标系的投影变换，可以看<a href="https://superzlw.github.io/posts/1.html">这个</a>，有直接的例子。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>【1】 视觉SLAM 14讲\<br>【2】 <a href="https://shartoo.github.io/2016/10/25/SVD-decomponent/">https://shartoo.github.io/2016/10/25/SVD-decomponent/</a> \<br>【3】 <a href="https://www.qinxing.xyz/posts/b7ea425d/">https://www.qinxing.xyz/posts/b7ea425d/</a> \<br>【4】 <a href="https://zhuanlan.zhihu.com/p/24651968">https://zhuanlan.zhihu.com/p/24651968</a> \<br>【5】 <a href="https://zhuanlan.zhihu.com/p/87334006">https://zhuanlan.zhihu.com/p/87334006</a></p>
]]></content>
      <categories>
        <category>图像处理</category>
      </categories>
  </entry>
  <entry>
    <title>支持向量机(SVM)</title>
    <url>/posts/13.html</url>
    <content><![CDATA[<h1 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h1><p>对于上一篇提到的风险（Risk）：</p>
<script type="math/tex; mode=display">
R(w)\leq R_{emp}(w)+\epsilon(N,p^*,h)</script><p>其中$N$是训练数据量，$p^*$是到达边界的概率，$h$是VC维度。</p>
<span id="more"></span>
<p>为了最小化风险，经典的机器学习算法是：</p>
<blockquote>
<p>固定$\epsilon(…)$，最小化经验风险$R_{emp}$<br>其中$\epsilon(…)$是通过保证一些模型参数不变以固定的，比如说神经网络的隐藏层数量。</p>
</blockquote>
<p>支持向量机（SVM）的做法则是：</p>
<blockquote>
<p>保证$R_{emp}(w)$固定以最小化$\epsilon$<br>当数据可分时，$R_{emp}(w)=0$。<br>通过改变VC维度以控制$\epsilon$。</p>
</blockquote>
<p>支持向量机一般分为三类：</p>
<blockquote>
<ol>
<li>线性可分支持向量机（linear support vector machine in linearly separable case ），采用的方法是硬间隔最大化（hard margin maximization）</li>
<li>线性支持向量机（linear supportvector machine），是指训练数据近似线性可分时，采用的方法是软间隔最大化（soft margin maximization）</li>
<li>非线性支持向量机（non-linear support vector machine），是指训练数据线性不可分时，采用核技巧（kernel trick）及软间隔最大化</li>
</ol>
</blockquote>
<p>对于二分类问题：</p>
<blockquote>
<p>输入空间：欧式空间或离散集合<br>特征空间：欧式空间或希尔伯特空间</p>
</blockquote>
<p>线性可分支持向量机、线性支持向量机：</p>
<blockquote>
<p>假设这两个空间的元素一一对应，并将输入空间中的输入映射为特征空间中的特征向量；</p>
</blockquote>
<p>非线性支持向量机：</p>
<blockquote>
<p>利用一个从输入空间到特征空间的非线性映射将输入映射为特征向量；</p>
</blockquote>
<p><strong>支持向量机的学习是在特征空间进行的</strong>。</p>
<p>（这篇会有不少补充推导，这些过程其实看不看都没事，不想看的话就直接记结论。看了，能减少学习过程中的疑惑，不看，也不会对SVM的理解产生大的影响）</p>
<h1 id="线性SVM"><a href="#线性SVM" class="headerlink" title="线性SVM"></a>线性SVM</h1><h2 id="硬间隔最大化"><a href="#硬间隔最大化" class="headerlink" title="硬间隔最大化"></a>硬间隔最大化</h2><p>直白地说，就是找一个（超）平面，使其距离最近数据点的距离最大，就是最大化下图这个$margin$。<br><img src="https://img-blog.csdnimg.cn/2021072008164997.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" title="" alt="在这里插入图片描述" data-align="center"><br>为了最大化这个margin，我们首先要找到一个超平面，使数据得到线性分离：</p>
<script type="math/tex; mode=display">
y_i(\pmb{w}^T\pmb{x}_i+b)\ge1 \quad \forall i</script><p>并且让至少一个点满足$y_i(\pmb{w}^T\pmb{x}_i+b)=1$。<br>然后这个间隔就是$\frac{1}{||w||}$。</p>
<p>———————————————————————————————————————</p>
<p>补充1</p>
<p><strong>上面的（不）等式怎么来的，这里的1又是什么，为什么间隔会是这个？</strong><br>我们要找到最大间隔分类器，就是要找一个合适的参数$w,b$，使其满足：</p>
<script type="math/tex; mode=display">
\max margin(w,b)</script><script type="math/tex; mode=display">
s.t.\quad y_i(\pmb{w}^T\pmb{x}_i+b)>0</script><p>稍微解释一下这个约束条件：这里有个前提是我们已经把数据分类好了，因为线性可分，其经验风险为0，$y_i$与括号里的式子同号，相乘大于0。<br>再多说一点，点到超平面的距离表示分类预测的确信程度，$w^Tx+b$与标签符号是否一致表示分类的准确性，所以$y(w^Tx+b)$表示分类的正确性和确信度。<br>这里定义一下这个$margin$函数如下：</p>
<script type="math/tex; mode=display">
margin(w,b)=\min\limits_{w,b,x_i,i=1...N} distant(w,b,x_i)</script><p>也就是说我们要找到距离（超）平面最近的那个点所对应的距离。<br>然后点到（超）平面的距离为（距离公式之前那篇分类问题里有用到）：</p>
<script type="math/tex; mode=display">
distant=\frac{1}{||\pmb{w}||}|\pmb{w}^Tx_i+b|</script><p>也就是说，间隔公式可以写为：</p>
<script type="math/tex; mode=display">
margin(w,b)=\min\limits_{w,b,x_i,i=1...N} \frac{1}{||\pmb{w}||}|\pmb{w}^Tx_i+b|</script><p>即是说，最上面我们要处理的问题就变为：</p>
<script type="math/tex; mode=display">
\max\limits_{w,b} \min\limits_{x_i,i=1...N}\frac{1}{||\pmb{w}||}|\pmb{w}^Tx_i+b|</script><script type="math/tex; mode=display">
s.t.\quad y_i(\pmb{w}^T\pmb{x}_i+b)>0</script><p>因为绝对值符号有点碍眼，根据其约束条件我们可以将其改写成：</p>
<script type="math/tex; mode=display">
\max\limits_{w,b} \min\limits_{x_i,i=1...N}\frac{1}{||\pmb{w}||}y_i(\pmb{w}^Tx_i+b)</script><p>因为我们要求的是最大值对应的参数，所以这里加个系数也无所谓（或许这里写成$argmax$之类的会更好理解？）。<br>继续改写！<br>前面那个$\frac{1}{||\pmb{w}||}$跟$\min$无关，所以可以前移，然后对于约束条件，因为左式大于0，所以我们肯定能找到一个$\gamma$，使左式最小值等于$\gamma$，即是：</p>
<script type="math/tex; mode=display">
\max\limits_{w,b} \frac{1}{||\pmb{w}||}\min\limits_{x_i,i=1...N}y_i(\pmb{w}^Tx_i+b)</script><script type="math/tex; mode=display">
s.t.\quad \exists \gamma \min\limits_{x_i,y_i,i=1,...,N} y_i(\pmb{w}^T\pmb{x}_i+b)=\gamma</script><p>令$\gamma=1$，将约束条件带入我们要处理的式子，就可以写成：</p>
<script type="math/tex; mode=display">
\max\limits_{w,b} \frac{1}{||\pmb{w}||}</script><script type="math/tex; mode=display">
s.t.\quad \min\limits_{x_i,y_i,i=1,...,N} y_i(\pmb{w}^T\pmb{x}_i+b)=1</script><p>为什么这里可以让$\gamma=1$呢？因为这相当于超平面的等比例缩放，取1只是为了方便计算而已，要有闲情逸致的话，取个3.1415926都行。<br>继续改写约束条件!<br>这个最小值的等式我们可以用不等式代替，然后这个求最大值的我们也可以用最小值代替，分子分母换位而已，即：</p>
<script type="math/tex; mode=display">
arg\min\limits_{w,b} \frac{1}{2}w^Tw</script><script type="math/tex; mode=display">
s.t.\quad y_i(\pmb{w}^T\pmb{x}_i+b)\ge1 \quad for \quad \forall i=1,...,N</script><p>这里乘上$\frac{1}{2}$，又把根号去掉，只是为了方便后面计算，毕竟我们要的是$w,b$，不要求计算实际最大值，所以这无所谓。这是一个典型的凸二次规划问题：有$N$个约束条件，目标函数是二次的。<br>至此，我们就知道上面的1和（不）等式是什么东西，以及间隔为什么会是$\frac{1}{||w||}$了。</p>
<p>——————————————————————————————————————</p>
<p><strong>支持向量（Support vectors）</strong>：</p>
<p>所有位于边缘的点，即满足：$y_i(\pmb{w}^T\pmb{x}_i+b)=1$的点。<br>现在计算以下优化问题(写法上跟上面补充内容有一丁点不一样，但意思是一样的)：</p>
<script type="math/tex; mode=display">
arg\min\limits_{w,b} \frac{1}{2}||w||^2</script><script type="math/tex; mode=display">
s.t. \quad y_i(\pmb{w}^T\pmb{x}_i+b)-1\ge0 \quad \forall i</script><p>带约束条件的优化问题依旧是拉格朗日,如下：<br><img src="https://img-blog.csdnimg.cn/20210720095426527.png#pic_center" alt="在这里插入图片描述"><br>要最小化上面这个$L(w,b,\alpha)$，即是要同时对$b,w$求偏导并令偏导等于0，如下：<br><img src="https://img-blog.csdnimg.cn/20210720100108960.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>现在还剩个$\alpha_i$。<br>在这里，只有在边缘点$\alpha_i$是不为0的，其余点的$\alpha_i$都等于0，比如下面这张图：<br><img src="https://img-blog.csdnimg.cn/20210720100607658.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>只有圈出的那三个点的$\alpha_i$是有意义的。所以SVM是一个稀疏（sparse）的学习机，分类只取决于少数的点。<br>要求解这个$\alpha_i$，我们就要引进这个问题的对偶问题。</p>
<p>———————————————————————————————————————</p>
<p>补充2——拉格朗日对偶</p>
<p>在约束最优化问题中，常常利用拉格朗日对偶性(Lagrange duality)将原始问题转换为对偶问题，通过解对偶问题得到原始问题的解。<br>比如说，现有原始问题：<br>设$f(x),c(x),h(x)$是定义在$\mathcal{R}^n$上的连续可微函数：</p>
<script type="math/tex; mode=display">
\min\limits_{x\in \mathcal{R}^n}f(x)</script><script type="math/tex; mode=display">
s.t. \quad c_i(x)\leq0,\quad i=1,...,k</script><script type="math/tex; mode=display">
h_j(x)=0,\quad j=1,...,l</script><p>引入拉格朗日函数$\alpha_i,\beta_j$为乘子$\alpha_i\ge0$:</p>
<script type="math/tex; mode=display">
L(x,\alpha,\beta)=f(x)+\sum_{i=1}^{k}\alpha_ic_i(x)+\sum_{j=1}^{l}\beta_jh_j(x)</script><p>考虑$x$的函数，$P$为原始问题：</p>
<script type="math/tex; mode=display">
\theta_P(x)=\max\limits_{\alpha,\beta,\alpha_i\ge0}L(x,\alpha,\beta)</script><p>假定给定一个$x$，如果$x$违反约束条件，即：</p>
<script type="math/tex; mode=display">
c_i(x)>0\quad h_j(x)\ne0</script><p>则 </p>
<script type="math/tex; mode=display">
\theta_P(x)=\max\limits_{\alpha,\beta,\alpha_i\ge0}[f(x)+\sum_{i=1}^{k}\alpha_ic_i(x)+\sum_{j=1}^{l}\beta_jh_j(x)]=+\infty</script><p>即（这个结论下面需要用到）：<br><img src="https://img-blog.csdnimg.cn/20210720162958878.png#pic_center" alt="在这里插入图片描述"><br>考虑极小问题：</p>
<script type="math/tex; mode=display">
\min\limits_{x}\theta_P(x)=\min\limits_{x}\max\limits_{\alpha,\beta,\alpha_i\ge0}L(x,\alpha,\beta)</script><p>这个极小问题与原始的最优化问题等价</p>
<script type="math/tex; mode=display">
p^*=\min\limits_{x}\theta_P(x)</script><p><strong>原始问题</strong>变为：</p>
<script type="math/tex; mode=display">
\min\limits_{x}\theta_P(x)=\min\limits_{x}\max\limits_{\alpha,\beta,\alpha_i\ge0}L(x,\alpha,\beta)</script><p>称为广义拉格朗日函数的极小极大问题，定义最优解为$p^*=\min\limits_{x}\theta_P(x)$。</p>
<p><strong>对偶问题</strong>为：<br>定义：$\theta_D(\alpha,\beta)=\min L(x,\alpha,\beta)$<br>所以：</p>
<script type="math/tex; mode=display">
\max\limits_{\alpha,\beta,\alpha_i\ge0}\theta_D(\alpha,\beta)=\max\limits_{\alpha,\beta,\alpha_i\ge0}\min\limits_xL(x,\alpha,\beta)</script><p>称为广义拉格朗日函数的极大极小问题，约束条件为$\alpha_i\ge0,i=1,…,k$，这个称为原始问题的对偶问题，对偶问题的最优值为：</p>
<script type="math/tex; mode=display">
d^*=\max\limits_{\alpha,\beta,\alpha_i\ge0}\theta_D(\alpha,\beta)</script><p>若原始问题和对偶问题都有最优值，则：</p>
<script type="math/tex; mode=display">
d^*=\max\limits_{\alpha,\beta,\alpha_i\ge0}\min\limits_xL(x,\alpha,\beta)\leq \min\limits_{x}\max\limits_{\alpha,\beta,\alpha_i\ge0}L(x,\alpha,\beta)=p^*</script><p>在SVM这里，对偶问题其实就可以表示为很直接的一个式子：</p>
<script type="math/tex; mode=display">
\max\min f(x)\leq \min\max f(x)</script><p>这个式子直观上就能理解，所以这里不解释。<br>这个不等式称为弱对偶关系，但在SVM我们要强对偶关系，也就是：</p>
<script type="math/tex; mode=display">
\max\min f(x)=\min\max f(x)</script><p>而凸优化的二次规划问题恰恰就满足强对偶条件。</p>
<p>————————————————————————————————————————</p>
<p>也就是说，现在问题变成求以下优化问题：</p>
<script type="math/tex; mode=display">
\max\limits_{\alpha}\min\limits_{w,b}L(w,b,\alpha)</script><script type="math/tex; mode=display">
s.t. \quad \alpha\ge0</script><p>现在问题就好算了。<br>在继续往下之前先对上面做个总结（上面的看着挺乱）：</p>
<ol>
<li><p>首先我们要求解以下这个优化问题：</p>
<script type="math/tex; mode=display">
arg \min\limits_{w,b}\frac{1}{2}w^Tw</script><script type="math/tex; mode=display">
s.t. \quad y_i(\pmb{w}^T\pmb{x}_i+b)\ge1 \quad for \quad \forall i=1,...,N</script></li>
<li><p>用拉格朗日法进行去约束，变为：</p>
<script type="math/tex; mode=display">
\min\limits_{w,b}\max\limits_{\alpha}L(w,b,\alpha)</script></li>
</ol>
<script type="math/tex; mode=display">
s.t. \quad \alpha\ge0</script><p>   这里再简单说一下，这个优化跟$w,b$已经无关了（这么说好像不太准确。。。），就是说，如果$y_i(\pmb{w}^T\pmb{x}_i+b)&lt;1$的话，根据上面补充（关于$+\infty$）的内容，值是等于$+\infty$的，再根据前面的最小约束，怎么都取不到这个值，所以不管$w,b$的值怎么取，结果都是等价于是在约束$y_i(\pmb{w}^T\pmb{x}_i+b)\ge1$里取，这是被上面拉格朗日的方程限制死的，所以我们不再需要考虑上面的约束了。</p>
<ol>
<li>根据强对偶关系，优化问题等价于：</li>
</ol>
<script type="math/tex; mode=display">
\max\limits_{\alpha}\min\limits_{w,b}L(w,b,\alpha)</script><script type="math/tex; mode=display">
s.t. \quad \alpha\ge0</script><p>总结结束，继续!<br>对于这个优化问题就很容易求了，两次求导即可。上面我们已经有过一次求导了，把上面的结论写下来即是：<br><img src="https://img-blog.csdnimg.cn/20210720193528375.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>将其带入下面这个最开始的拉格朗日方程里：<br><img src="https://img-blog.csdnimg.cn/20210720195830686.png#pic_center" alt="在这里插入图片描述"><br>得到下面这个式子：<br><img src="https://img-blog.csdnimg.cn/20210720195959244.png#pic_center" alt="在这里插入图片描述"><br>这就是拉格朗日方程的最小值，再由于：<br><img src="https://img-blog.csdnimg.cn/20210720200724523.png#pic_center" alt="在这里插入图片描述"><br>继续改写成如下的式子<br><img src="https://img-blog.csdnimg.cn/20210720200849578.png#pic_center" alt="在这里插入图片描述"><br>这就是我们要最大化的东西。<br>这里我们要这么大费周章地推导求解对偶问题，主要是因为：</p>
<blockquote>
<ol>
<li>对偶问题往往更容易求解；</li>
<li>引入核函数，即为非线性SVM做准备。</li>
</ol>
</blockquote>
<h2 id="软间隔最大化"><a href="#软间隔最大化" class="headerlink" title="软间隔最大化"></a>软间隔最大化</h2><p>软间隔最大化问题跟硬间隔的差不多，只是训练集中会有一些异常点（outlier）不能满足约束条件$y_i(\pmb{w}^T\pmb{x}_i+b)\ge1$。如下图：<br><img src="https://img-blog.csdnimg.cn/20210720211011111.png#pic_center" alt="在这里插入图片描述"></p>
<p>一个简单的思路就是，将这些数据点映射到新的特征空间，然后就可以线性区分了，比如用小半径的RBF核函数（这在下面非线性SVM会介绍）。<br>但这会产生一个问题，就是$VC$维度特别大，可能会导致过拟合。</p>
<p>另一种解决方法是：选择性地忽略一些点。对每个样本点$(x_i,y_i)$引进一个松弛变量（slack variables）$\xi_i\ge0$，使：<br><img src="https://img-blog.csdnimg.cn/20210721215433434.png#pic_center" alt="在这里插入图片描述"></p>
<p>函数间隔加上松弛变量大于等于1，即约束条件变为：</p>
<script type="math/tex; mode=display">
y_i(\pmb{w}^T\pmb{x}_i+b)\ge1-\xi_i</script><p>目标函数就变为：</p>
<script type="math/tex; mode=display">
\frac{1}{2}||w||^2+C\sum_{i=1}^{N}\xi_i</script><p>其中$C&gt;0$为惩罚参数。<br>由此可得，线性不可分的线性支持向量机的学习问题为：<br><img src="https://img-blog.csdnimg.cn/20210721215658777.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p>其中$w$的解是唯一的，$b$不是（不作证明）。</p>
<p>最大化边际（margin），同时最小化所有不在边际之<strong>外</strong>的数据点的惩罚权重 C 允许我们指定权衡。 通常通过交叉验证确定, 即使数据是可分离的，最好允许偶尔的惩罚（penalty）。</p>
<p>剩下的解优化问题跟硬间隔几乎是一模一样了，不过还是再过一遍吧。<br>回到我们上面的学习问题，带约束的优化，用拉格朗日函数如下：</p>
<script type="math/tex; mode=display">
L(w,b,\xi,\alpha,\mu)=\frac{1}{2}||w||^2+C\sum_{i=1}^{N}\xi_i-\sum_{i=1}^{N}\alpha_i(y_i(wx_i+b)-1+\xi_i)-\sum_{i=1}^{N}\mu_i\xi_i</script><p>其中: $\alpha_i\ge 0,\mu_i\ge0$。对偶问题是这个函数的极大极小问题，首先求$L(w,b,\xi,\alpha,\mu)$对$w,b,\xi$的极小，即是求导等于0，得：<br><img src="https://img-blog.csdnimg.cn/20210720212504986.png#pic_center" alt="在这里插入图片描述"><br>即是说，极小值为：<br><img src="https://img-blog.csdnimg.cn/20210720212541576.png#pic_center" alt="在这里插入图片描述"><br>再对$\alpha$求极大：<br><img src="https://img-blog.csdnimg.cn/20210720212652248.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>这里$alpha_i\leq C$称为box constraint(箱体约束)。</p>
<blockquote>
<p>设$\alpha^<em>=(\alpha_1^</em>,…,\alpha_N^<em>)^T$是上面对偶问题的一个解，若存在$\alpha$的一个分量$\alpha_j$，$0&lt;\alpha_j^</em>&lt;C$，则原始问题的解$w,b$为：</p>
<script type="math/tex; mode=display">
w^*=\sum_{i=1}^{N_s}\alpha_i^*y_ix_i</script><script type="math/tex; mode=display">
b^*=y_j-\sum_{i=1}^{N_s}y_i\alpha_i^*(x_ix_j)</script></blockquote>
<p>(注：在硬间隔中求出的$\alpha$也是一个向量。)</p>
<p>———————————————————————————————————————</p>
<p>补充3——合页损失函数（hinge loss function）</p>
<p>线性支持向量机学习还有另一种解释，就是最小化下面这个目标函数：</p>
<script type="math/tex; mode=display">
\sum_{i=1}^{N}[1-y_i(wx_i+b)]_++\lambda||w||^2</script><p>第一项：$L(y(wx+b))=[1-y(wx+b)]_+$就称为合页损失函数，其中：</p>
<script type="math/tex; mode=display">
[z]_+=z,\quad z>0</script><script type="math/tex; mode=display">
[z]_+=0,\quad z\le0</script><p>最优化问题就可以等价为：<br><img src="https://img-blog.csdnimg.cn/20210720215356556.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p>———————————————————————————————————————</p>
<h1 id="非线性SVM"><a href="#非线性SVM" class="headerlink" title="非线性SVM"></a>非线性SVM</h1><p>然而很多时候我们得到的数据并不是直接用硬或者软间隔进行线性分类的，比如下面这个：<br><img src="https://img-blog.csdnimg.cn/20210721163854894.png#pic_center" alt="在这里插入图片描述"><br>明显，不管怎么画直线都没办法把这两类分开，这时候我们可以增加一个维度，从二维变三维，变成下面这种情况：<br><img src="https://img-blog.csdnimg.cn/20210721164054328.png#pic_center" alt="在这里插入图片描述"><br>这个就可以用（超）平面进行线性划分了。记住一点：<strong>相对于低维，高维更可能可以用线性进行分类</strong>。<br>也就是说在这里，数据的输入空间不再直接充当特征空间，我们要新构造一个特征空间，就跟之前的线性回归那里的思想是一样的。<br>我们的输入是$\pmb{x}$，然后就要找到一个非线性变换$\phi$，将输入空间映射到特征空间（但这个映射也不能太过分了，不然容易过拟合）。<br>现在回顾上面的对偶形式：<br><img src="https://img-blog.csdnimg.cn/20210721165319361.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>通过非线性变换，可以得到下面的方程：<br><img src="https://img-blog.csdnimg.cn/20210721165401767.png#pic_center" alt="在这里插入图片描述"><br>这个式子可以看出，当两个特征空间的元素$\phi(x_i)$和$\phi(x_j)$内积时，就会产生一个标量。<br>再看判别函数：</p>
<script type="math/tex; mode=display">
y(x)=w^T\phi(x)+b</script><p>由于</p>
<script type="math/tex; mode=display">
w=\sum_{i=1}^{N_s}\alpha_iy_i\phi(x_i)</script><p>则非线性判别函数可以写为：</p>
<script type="math/tex; mode=display">
y(x)=\sum_{i=1}^{N_s}\alpha_iy_i\phi(x_i)^T\phi(x)+b</script><p>其中$N_s$是支持向量的数目。<br>又发现，判别函数也可以只用非线性特征的标量积来表示。<br>但是我们想直接计算它们内积是不太可能的，因为这个非线性转换可能会出现无限维的情况。<br>这就引出了核技巧（Kernel Trick），即使用核函数取代每一个标量积：</p>
<script type="math/tex; mode=display">
K(x_i,x_j)=\phi(x_i)^T\phi(x_j)</script><p>如果我们能找到这么一个核函数，那就可以避免高维空间的映射，转而直接计算这个标量积。<br>但该怎么判断是否存在这样的核函数呢？</p>
<h2 id="多项式核（Polynomial-Kernel）"><a href="#多项式核（Polynomial-Kernel）" class="headerlink" title="多项式核（Polynomial Kernel）"></a>多项式核（Polynomial Kernel）</h2><p>以以下二阶核为例：</p>
<script type="math/tex; mode=display">
K(x,y)=(x^Ty)^2</script><p>等同于点积：</p>
<script type="math/tex; mode=display">
K(x,y)=(x^Ty)^2=x_1^2y_1^2+2x_1x-2y_1y_2+y_1^2y_2^2</script><p>对应的非线性转换为：<br><img src="https://img-blog.csdnimg.cn/2021072117411341.png#pic_center" alt="在这里插入图片描述"><br>以这个为例，核方法的一个优点是核计算量的简便，比如这里的计算数为：3（$x,y$的点积）+1（结果的平方）=4<br>另外需要注意，一个核函数的非线性变换不是唯一的，比如上面这个核函数，其对应的变换还可能是：<br><img src="https://img-blog.csdnimg.cn/20210721174440506.png#pic_center" alt="在这里插入图片描述"><br><strong>转换空间维度以及VC维度的确定</strong><br>令$C_d(x)$为将一个向量映射到所有阶数为$d$的有序单项式空间的变换。</p>
<script type="math/tex; mode=display">
K(x,y)=(x^Ty)^d=C_d(x)^TC_d(y)</script><p>转换空间$H$的维度为：$C_{d+N-1}^{d}$。<br>比如说：</p>
<script type="math/tex; mode=display">
N=16\times 16=256 \quad d=4</script><p>则转换空间维度为：</p>
<script type="math/tex; mode=display">
dim(H)=183181376</script><p>该分类器的$VC$维度为：$dim(H)+1$。</p>
<h2 id="径向基函数（Radial-Basis-Functions）"><a href="#径向基函数（Radial-Basis-Functions）" class="headerlink" title="径向基函数（Radial Basis Functions）"></a>径向基函数（Radial Basis Functions）</h2><p>径向基函数（RBF），通常定义为空间任一点到某一中心的$y$的欧氏距离的单调函数，最常用的是高斯核函数，定义为：</p>
<script type="math/tex; mode=display">
K(x,y)=\exp(-\frac{||x-y||^2}{2\sigma^2})</script><p>换种说法的话，这个核函数可以衡量$x$和$y$的相似度的。<strong>其对应的变换空间是无限维的</strong>，所以其<strong>VC维度也是无限维的</strong>。当$x$和$y$很接近时值为1，很远时值为0，$\sigma$为函数的宽度参数，控制函数的径向作用范围。<br>用这个核函数的效果如下：<br><img src="https://img-blog.csdnimg.cn/20210721204316240.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h2 id="Mercer’s-Condition"><a href="#Mercer’s-Condition" class="headerlink" title="Mercer’s Condition"></a>Mercer’s Condition</h2><p>既然核函数能极大地简便我们的计算量，那现在就有个问题，我们该如何判断给定的一个函数$K$是一个有效的核函数呢，或者说，我们如何判断这个$K$能够替代$\phi(x_i)^T\phi(x_j)$？<br>这就引出了Mercer定理。</p>
<blockquote>
<p>Mercer定理：<br>如果函数$K$是$R^n\times R^n\rightarrow R$上的映射（也就是从两个$n$维向量映射到实数域），当且仅当对于任意训练样例$[x_1,…,x_n]$，其相应的核函数矩阵是对称半正定的话，则$K$是一个有效核函数（也称为Mercer核函数）。</p>
</blockquote>
<p>——————————————————————————————————————</p>
<p>补充4——正定与半正定矩阵</p>
<blockquote>
<p>正定矩阵<br>给定一个大小为 $n\times n$的实对称矩阵 $A$ ，若对于任意长度为 $n$ 的非零向量 $x$ ，有$x^TAx&gt;0$ 恒成立，则矩阵 $A$ 是一个正定矩阵。<br>正定矩阵的特征值都大于0。</p>
<p>半正定矩阵<br>给定一个大小为 $n\times n$的实对称矩阵 $A$ ，若对于任意长度为 $n$ 的非零向量 $x$ ，有$x^TAx\ge0$ 恒成立，则矩阵 $A$ 是一个半正定矩阵。<br>也就是说，半正定包含正定，其特征值均为非负，主子式大于等于0。</p>
</blockquote>
<p>———————————————————————————————————————</p>
<p>也就是说，为了证明$K$是有效核函数，我们不需要去找相应的映射函数$\phi$，只需要在训练集上求出各个$K_ij$，判断其是否为半正定即可。<br>上面的Mercer定理用数学表达的话可以写为：</p>
<blockquote>
<p>对于满足$\int g(x)^2dx&lt;0$的$g(x)$，若$K(x,y)$满足$\int \int K(x,y)g(x)g(y)dxdy\ge0$，则$K(x,y)$是一个有效核函数。</p>
</blockquote>
<p>判断一个函数是否满足Mercer定理并不容易，但我们可以根据现有的有效核函数构造出新的，也就是说，如果$K_1(x,y),K_2(x,y)$是有效核函数，则下面这些也是：</p>
<script type="math/tex; mode=display">
cK_1(x,y)\\K_1(x,y)+K_2(x,y)\\K_1(x,y)K_2(x,y)\\f(x)K_1(x,y)f(y)\\...</script><h2 id="相关补充"><a href="#相关补充" class="headerlink" title="相关补充"></a>相关补充</h2><p>除了上面的，非齐次多项式核函数也可以用来表示$d$阶多项式，形式如下：</p>
<script type="math/tex; mode=display">
K(\pmb{x},\pmb{y})=(\pmb{x}^T\pmb{y}+c)^d</script><p>除了Gaussian RBF 核函数：</p>
<script type="math/tex; mode=display">
K(\pmb{x},\pmb{y})=\exp(-\frac{||\pmb{x}-\pmb{y}||^2}{2\sigma^2})</script><p>还有其它比如Hyperbolic tangent核函数：</p>
<script type="math/tex; mode=display">
K(\pmb{x},\pmb{y})=tahn(a\pmb{x}^T\pmb{y}+b)</script>]]></content>
      <categories>
        <category>学习总结</category>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>数据分析中的一些常用Python指令</title>
    <url>/posts/24.html</url>
    <content><![CDATA[<h1 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h1><p>这里就是杂七杂八地记，方便自己查询。顺序基本没什么规律，或者以后可能会整理，看心情~~。总之就善用”ctrl+f”查询。<br><span id="more"></span></p>
<h1 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h1><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>【网址1】：<a href="https://www.xiaoheidiannao.com/70120.html">https://www.xiaoheidiannao.com/70120.html</a></p>
]]></content>
      <categories>
        <category>Data Science</category>
      </categories>
  </entry>
  <entry>
    <title>卡尔曼滤波（Kalman Filter）</title>
    <url>/posts/20.html</url>
    <content><![CDATA[<h1 id="写在前面的说明"><a href="#写在前面的说明" class="headerlink" title="写在前面的说明"></a>写在前面的说明</h1><p>本文总结自B站博主DR_CAN，他是整个B站我最喜欢的博主没有之一，推荐大家直接去看他的视频，这篇当作学后的回顾，链接在这$\rightarrow$<a href="https://space.bilibili.com/230105574/channel/detail?cid=139198&amp;ctype=0">【传送门】</a>。</p>
<span id="more"></span>
<h1 id="引入-——-递归算法"><a href="#引入-——-递归算法" class="headerlink" title="引入 —— 递归算法"></a>引入 —— 递归算法</h1><p>卡尔曼滤波器可看作是一种“Optimal Recursive Data Processing Algorithm”，即“最优的递归数字处理算法”。与滤波器相比它更像是一种观测器。<br>卡尔曼滤波器的应用非常广泛，尤其是导航中，这是因为现实生活中充满大量的不确定性，当我们描述一个系统的时候，这个不确定性主要体现在以下三个方面：</p>
<blockquote>
<ol>
<li>不存在完美的数学模型；</li>
<li>系统的挠动是不可控的，也很难建模；</li>
<li>测量传感器存在误差。</li>
</ol>
</blockquote>
<p>举个例子：<br>当我们要测量一枚硬币的直径时，设第$k$次测量结果为$z_k$，假如三次测量分别为：</p>
<script type="math/tex; mode=display">z_1=50.1mm</script><script type="math/tex; mode=display">z_2=50.4mm</script><script type="math/tex; mode=display">z_3=50.2mm</script><p>若要估计真实数据，很自然地，我们会用平均值来表示，即第$k$次的估计值$\hat{x}_k$为：</p>
<script type="math/tex; mode=display">
\begin{split}
\hat{x}_k &= \frac{1}{k}(z_1+...+z_k)\\\\ 
 &= \frac{1}{k}(z_1+...+z_{k-1})+\frac{1}{k}z_k\\\\ &=
\frac{1}{k}\frac{k-1}{k-1}(z_1+...+z_{k-1})+\frac{1}{k}z_k \\\\
&=\frac{k-1}{k}\hat{x}_{k-1}+\frac{1}{k}z_k\\\\
&=\hat{x}_{k-1}-\frac{1}{k}\hat{x}_{k-1}+\frac{1}{k}z_k
\end{split}</script><p>即是：</p>
<script type="math/tex; mode=display">
\hat{x}_k=\hat{x}_{k-1}+\frac{1}{k}(z_k-\hat{x}_{k-1})</script><p>由此可见，当$k$逐渐增加时，$\frac{1}{k}\rightarrow \infty$，也就是说$\hat{x}_k\rightarrow \hat{x}_{k-1}$。<br>用文字描述的话，就是说随着$k$的增加，之后测量结果也就不再重要了，也就是对前面的真实估计值已经非常有信心了。<br>相反，当$k$比较小时，$\frac{1}{k}$就会很大，也就是说此时$z_k$的作用会很大。<br>现在改写一下上面的式子为：</p>
<script type="math/tex; mode=display">
\hat{x}_k=\hat{x}_{k-1}+K_k(z_k-\hat{x}_{k-1})</script><p>在卡尔曼滤波器里，这个系数$K_k$就叫Kalman Gain，中文称为卡尔曼增益/因数。<br>由这个式子可以看出，跟马尔科夫链类似，当前估计值只与该次测量值以及上一次的估计值有关，这体现了一种递归的思想。<br>这里先对这个系数作简单的讨论。首先引入两个变量：</p>
<blockquote>
<ol>
<li>估计误差：$e_{EST}$；</li>
<li>测量误差：$e_{MEA}$。</li>
</ol>
</blockquote>
<p>卡尔曼增益就等于：</p>
<script type="math/tex; mode=display">
K_k=\frac{e_{EST_{k-1}}}{e_{EST_{k-1}}+e_{MEA_k}}</script><p><strong>这个公式是卡尔曼滤波的核心公式，详细推导见后文。</strong>\<br>这里先简单讨论在$k$时刻两个变量的作用：</p>
<blockquote>
<ol>
<li>当$e_{EST_{k-1}} \gg e_{MEA_k}$，即$k\rightarrow 1$时，$\hat{x}_k=\hat{x}_{k-1}+z_k-\hat{x}_{k-1}=z_{k}$；</li>
<li>当$e_{EST_{k-1}} \ll e_{MEA_k}$，即$k\rightarrow 0$时，$\hat{x}_k=\hat{x}_{k-1}$。</li>
</ol>
</blockquote>
<p>运用卡尔曼滤波器，真实估计值就可以表示为以下三个步骤：</p>
<blockquote>
<ol>
<li><strong>Step 1</strong>：计算卡尔曼增益：$K_k=\frac{e_{EST_{k-1}}}{e_{EST_{k-1}}+e_{MEA_k}}$；</li>
<li><strong>Step 2</strong>：计算当前估计值$\hat{x}_k=\hat{x}_{k-1}+K_k(z_k-\hat{x}_{k-1})$；</li>
<li><strong>Step 3</strong>：更新$e_{EST_k}=(1-K_k)e_{EST_{k-1}}$（这个式子后面推导）。</li>
</ol>
</blockquote>
<p>这里举个例子说明上面三个步骤：<br>假设我们要测量一个长度是$50mm$的物体，第0次的估计值，也就是相当于先验吧，是$\hat{x}_0=40mm$，第0次估计误差为$e_{EST_0}=5mm$，第一次测量为$z_1=51mm$，测量误差一直为$e_{MEA_k}=3mm$。<br>填入起始值，如下表：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">$k$</th>
<th style="text-align:center">$z_k$</th>
<th style="text-align:center">$e_{MEA_k}$</th>
<th style="text-align:center">$\hat{x}_k$</th>
<th style="text-align:center">$K_k$</th>
<th style="text-align:center">$e_{EST_k}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">40</td>
<td style="text-align:center"></td>
<td style="text-align:center">5</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">51</td>
<td style="text-align:center">3</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
</tbody>
</table>
</div>
<p>借助上面三个步骤：<br>$k=1$时：</p>
<script type="math/tex; mode=display">
K_k=\frac{5}{5+3}=0.625</script><script type="math/tex; mode=display">
\hat{x}_k=40+0.625(51-40)=46.875</script><script type="math/tex; mode=display">
e_{EST}=(1-0.625)\times 5 = 1.875</script><p>然后$k=2$时，设此次测量值为$z_2=48$，重复上面的步骤，就可以填满表格了。如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">$k$</th>
<th style="text-align:center">$z_k$</th>
<th style="text-align:center">$e_{MEA_k}$</th>
<th style="text-align:center">$\hat{x}_k$</th>
<th style="text-align:center">$K_k$</th>
<th style="text-align:center">$e_{EST_k}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">40</td>
<td style="text-align:center"></td>
<td style="text-align:center">5</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">51</td>
<td style="text-align:center">3</td>
<td style="text-align:center">46.88</td>
<td style="text-align:center">0.625</td>
<td style="text-align:center">1.875</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">48</td>
<td style="text-align:center">3</td>
<td style="text-align:center">47.31</td>
<td style="text-align:center">0.385</td>
<td style="text-align:center">1.154</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">47</td>
<td style="text-align:center">3</td>
<td style="text-align:center">47.22</td>
<td style="text-align:center">0.278</td>
<td style="text-align:center">0.833</td>
</tr>
</tbody>
</table>
</div>
<p>这样不断下去，最后这个真实估计值会收敛于$50mm$。<br>借用博主视频里的数值结果图片，其表现如下：<br><img src="D:\blog\source\images\250.png" alt=""><br>其中蓝色线表示测量值，红色线表示估计值。</p>
<h1 id="数据融合，协方差矩阵，状态空间方程"><a href="#数据融合，协方差矩阵，状态空间方程" class="headerlink" title="数据融合，协方差矩阵，状态空间方程"></a>数据融合，协方差矩阵，状态空间方程</h1><p>这部分是复习基础知识，可跳过。</p>
<h2 id="数据融合"><a href="#数据融合" class="headerlink" title="数据融合"></a>数据融合</h2><p>简单讲，就是从多个测量数据中找到一个最优的。<br>举个例子，我们在称量一个物体的时候，两次称量结果如下：</p>
<script type="math/tex; mode=display">
z_1=30g, \quad \sigma_1=2g</script><script type="math/tex; mode=display">
z_2=32g, \quad \sigma_2=4g</script><p>根据正态分布的性质，对于第一个测量结果，真实值落在区间[28,32]的概率是68.4%；对第二个测量结果，真实值落在区间[28,36]的概率是68.4%，如果这时候要我们估计真实值的话，我们会把真实值估计在区间[30,32]之间，而且由于第一个测量标准差较小，所以真实值会更靠近于30。<br>如果要我们在数学上得到一个比较准确的值，就可以用上面递归的方法，写出如下等式：</p>
<script type="math/tex; mode=display">
\hat{z}=z_1+K(z_2-z_1)</script><p>其中 $K\in [0,1]$，当$K=0$时，$\hat{z}=z_1$；当$K=1$时，$\hat{z}=z_2$。\<br>现在我们要求$K$，使得 $\sigma_{\hat{z}}$ 最小，即是要 $Var(\hat{z})$ 最小。<br>而这个方差我们可以写成：</p>
<script type="math/tex; mode=display">
\sigma_{\hat{z}}^2=Var(z_1+K(z_2-z_1))=Var((1-K)z_1+Kz_2)</script><p>由于括号里的两项 $(1-K)z_1$ 和 $Kz_2$ 相互独立(毕竟是两个不同的称，称量结果肯定不会相互影响)，根据方差的性质，上面式子可以继续往下写：</p>
<script type="math/tex; mode=display">
Var((1-K)z_1+Kz_2)=Var((1-K)z_1)+Var(Kz_2)=(1-K)^2Var(z_1)+K^2Var(z_2)</script><p>将上面两个方差代入得：</p>
<script type="math/tex; mode=display">
(1-K)^2Var(z_1)+K^2Var(z_2)=(1-K)^2\sigma_1^2+K^2\sigma_2^2</script><p>也就是说：</p>
<script type="math/tex; mode=display">
\sigma_{\hat{z}}^2=(1-K)^2\sigma_1^2+K^2\sigma_2^2</script><p>现在要求最小值，所以就是对$K$进行求导：</p>
<script type="math/tex; mode=display">
\frac{d\sigma_{\hat{z}}^2}{dK}=0</script><p>(中间过程自行计算)\<br>最后求出$K$的值为：</p>
<script type="math/tex; mode=display">
K=\frac{\sigma_1^2}{\sigma_1^2+\sigma_2^2}=0.2</script><p>把$K$代入上面式子，其最佳估计值为：</p>
<script type="math/tex; mode=display">
\hat{z}=z_1+K(z_2-z_1)=30+0.2(32-30)=30.4</script><p>其方差为：</p>
<script type="math/tex; mode=display">
\sigma_{\hat{z}}^2=(1-0.2)^22^2+0.2^24^2=3.2</script><p>标准差为：$\sqrt{3.2}=1.79$<br>这个计算过程就是数据融合。</p>
<h2 id="协方差矩阵"><a href="#协方差矩阵" class="headerlink" title="协方差矩阵"></a>协方差矩阵</h2><p>协方差矩阵是将矩阵，协方差在一个矩阵中表示出来，表明变量空间的联动关系。比如说下面这个表格：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">名字</th>
<th style="text-align:center">身高$(x)$</th>
<th style="text-align:center">体重$(y)$</th>
<th style="text-align:center">年龄$(z)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">A</td>
<td style="text-align:center">179</td>
<td style="text-align:center">74</td>
<td style="text-align:center">33</td>
</tr>
<tr>
<td style="text-align:center">B</td>
<td style="text-align:center">187</td>
<td style="text-align:center">80</td>
<td style="text-align:center">31</td>
</tr>
<tr>
<td style="text-align:center">C</td>
<td style="text-align:center">175</td>
<td style="text-align:center">71</td>
<td style="text-align:center">28</td>
</tr>
<tr>
<td style="text-align:center">均值</td>
<td style="text-align:center">180.3</td>
<td style="text-align:center">75</td>
<td style="text-align:center">30.7</td>
</tr>
</tbody>
</table>
</div>
<p>计算出变量$x,y,z$的方差为：</p>
<script type="math/tex; mode=display">
\sigma_x^2=24.89,\quad \sigma_y^2=14,\quad\sigma_z^2=4.22</script><p>计算协方差为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\sigma_x\sigma_y &= \frac{1}{3}((179-180.3)(74-75)+(187-180.3)(80-75)+(175-180.3)(71-75))\\
&= 18.7=\sigma_y\sigma_x
\end{aligned}</script><p>同理：</p>
<script type="math/tex; mode=display">
\sigma_x\sigma_z=4.4=\sigma_z\sigma_x, \quad \sigma_y\sigma_z=3.3=\sigma_z\sigma_y</script><p>(协方差矩阵是对称矩阵。)<br>协方差矩阵形式如下：</p>
<script type="math/tex; mode=display">
 \left[
 \begin{matrix}
   \sigma_x^2 & \sigma_{x,y} & \sigma_{x,z} \\
   \sigma_{y,x} & \sigma_y^2 & \sigma_{y,z} \\
   \sigma_{z,x} & \sigma_{z,y} & \sigma_z^2
  \end{matrix}
  \right]</script><p>代入数据：</p>
<script type="math/tex; mode=display">
 \left[
 \begin{matrix}
   24.89 & 18.7 & 4.4 \\
   18.7 & 14 & 3.3 \\
   4.4 & 3.3 & 4.22
  \end{matrix}
  \right]</script><p><strong>矩阵计算的方法</strong><br>在写代码的时候，用矩阵计算方法计算协方差明显更容易书写，其形式如下：\<br>过渡矩阵：</p>
<script type="math/tex; mode=display">
a=
 \left[
 \begin{matrix}
   x_1 & y_1 & z_1 \\
   x_2 & y_2 & z_2 \\
   x_3 & y_3 & z_3
 \end{matrix}
 \right]
-
\frac{1}{3}
 \left[
 \begin{matrix}
   1 & 1 & 1 \\
   1 & 1 & 1 \\
   1 & 1 & 1
  \end{matrix}
  \right]
   \left[
 \begin{matrix}
   x_1 & y_1 & z_1 \\
   x_2 & y_2 & z_2 \\
   x_3 & y_3 & z_3
  \end{matrix}
  \right]</script><p>协方差矩阵：</p>
<script type="math/tex; mode=display">
P=\frac{1}{3}a^Ta</script><p>现在我们观察上面得出的那个协方差矩阵，先看对角线，前两个数字都很大（其实第三个数字不应该这么小的，但毕竟只取了三个数据，偶然性太大），表明数据波动是比较大的，说明身高体重在这个群体里影响不大；接着看其它数据，比如这个18.7，说明身高跟体重是成正比的，且相关性较大，也符合常理；然后再看看4.4，说明身高跟年龄关系不大，也对，到一定年龄身高跟年龄关系就不大了……\</p>
<h2 id="状态空间方程"><a href="#状态空间方程" class="headerlink" title="状态空间方程"></a>状态空间方程</h2><p>比如说对于一个弹簧阻尼系统：<img src="/images/271.png" alt=""><br>我们可以很容易写出如下微分方程($x$为位移)：</p>
<script type="math/tex; mode=display">
m\ddot{x}+c\dot{x}+kx=F</script><p>状态量为：</p>
<script type="math/tex; mode=display">
x_1=x;\quad x_2=\dot{x}</script><p>观测量为：</p>
<script type="math/tex; mode=display">
z_1=x=x_1;位置\quad z_2=\dot{x}=x_2速度</script><p>设$F$为输入值，等于$u$。\<br>写成一阶微分矩阵形式如下：</p>
<script type="math/tex; mode=display">
   \left[
\begin{matrix}
   \dot{x_1}  \\
   \dot{x_2} 
  \end{matrix}
  \right]
=
   \left[
\begin{matrix}
   0 & 1  \\
   -\frac{k}{m} & -\frac{c}{m} 
  \end{matrix}
  \right]
     \left[
\begin{matrix}
   x_1  \\
   x_2
  \end{matrix}
  \right]
+
   \left[
\begin{matrix}
   0  \\
   \frac{1}{m} 
  \end{matrix}
  \right]
u</script><p>观测矩阵为：</p>
<script type="math/tex; mode=display">
\left[
\begin{matrix}
   z_1  \\
   z_2 
\end{matrix}
\right]
=
   \left[
\begin{matrix}
   1 & 0  \\
   0 & 1 
  \end{matrix}
  \right]
      \left[
\begin{matrix}
   x_1  \\
   x_2
  \end{matrix}
  \right]</script><p>以上两个矩阵表达式就是状态空间表达式，归纳一下可以写为：</p>
<script type="math/tex; mode=display">
\dot{X}_t=AX_t+Bu_t</script><script type="math/tex; mode=display">
Z_t=HX_t</script><p>这是一种连续的表达式，左边$X$是对时间求导。\<br>写成离散形式为：</p>
<script type="math/tex; mode=display">
X_k=AX_{k-1}+Bu_{k-1}</script><script type="math/tex; mode=display">
Z_k=HX_k</script><p>其下标$k.k+1,k+2…$为时间单位，即是采样。\<br>在实际生活中到处充满着不确定性，上面那个离散的状态空间表达式可以写为：</p>
<script type="math/tex; mode=display">
X_k=AX_{k-1}+Bu_{k-1}+w_{k-1}</script><script type="math/tex; mode=display">
Z_k=HX_k+v_k</script><p>其中$w_{k-1}$为过程噪音，$v_k$为测量噪音。\<br>根据这些如何去估计一个$\hat{X}_k$呢？这就是卡尔曼滤波器要解决的问题。</p>
<h1 id="卡尔曼增益的数学推导"><a href="#卡尔曼增益的数学推导" class="headerlink" title="卡尔曼增益的数学推导"></a>卡尔曼增益的数学推导</h1><p>在一开始我们用了卡尔曼增益，但却没有解释为什么这个增益是这种形式，这部分就是对它的推导。\<br>将上面那个离散情况下带噪音的状态空间方程写下来如下：</p>
<script type="math/tex; mode=display">
X_k=AX_{k-1}+Bu_{k-1}+w_{k-1}</script><script type="math/tex; mode=display">
Z_k=HX_k+v_k</script><p>两个方程的噪音部分是我们不可测的，但在自然界中我们可以把这噪声假设成正态分布(why? 只能说自然界就是这么神奇)。所以我们可以把过程噪音写成：</p>
<script type="math/tex; mode=display">
P(w) \sim (0,Q)</script><p>这里0是期望，$Q$是协方差矩阵。这个$Q$可以用下面的式子计算：</p>
<script type="math/tex; mode=display">
Q=E[ww^T]</script><p>举个例子，假如现在$X_k$由两部分组成，即是：</p>
<script type="math/tex; mode=display">
X_k=
\left[
  \begin{matrix}
   x_1  \\
   x_2
  \end{matrix}
\right]</script><p>则对应的过程噪音就是：</p>
<script type="math/tex; mode=display">
\left[
\begin{matrix}
   w_1  \\
   w_2
  \end{matrix}
  \right]</script><p>这个噪音的协方差矩阵为：</p>
<script type="math/tex; mode=display">
E[
\left[
\begin{matrix}
   w_1  \\
   w_2
  \end{matrix}
  \right]
\left[
\begin{matrix}
   w_1 & w_2
  \end{matrix}
  \right]
]
=
\left[
\begin{matrix}
   E[w_1^2] & E[w_1w_2]  \\
   E[w_2w_1] & E[w_2^2]
  \end{matrix}
  \right]</script><p>另外，对于方差我们有：</p>
<script type="math/tex; mode=display">
Var(x)=E(x^2)-E^2(x)</script><p>在这里噪音的期望为0，所以：$Var(x)=E(x^2)$，上面的矩阵可以写为：\</p>
<script type="math/tex; mode=display">
E[
\left[
\begin{matrix}
   w_1  \\
   w_2
  \end{matrix}
  \right]
\left[
\begin{matrix}
   w_1 & w_2
  \end{matrix}
  \right]
]
=
\left[
\begin{matrix}
   \sigma_{w_1}^2 & \sigma_{w_1,w_2}  \\
   \sigma_{w_2,w_1} & \sigma_{w_2}^2
  \end{matrix}
  \right]</script><p>噪音的协方差矩阵就出来了，所以这个式子：</p>
<script type="math/tex; mode=display">
Q=E[ww^T]</script><p>是正确的。<br>同理，对于测量噪音$v_k$，也是一样的：</p>
<script type="math/tex; mode=display">
p(v)\sim (0,R)</script><p>$R$是协方差矩阵，同样用上面的方法也可以计算。\<br>在实际建模中，这两个噪音我们是无法建模出来的，我们能计算的只是下面这个东西：</p>
<script type="math/tex; mode=display">
X_k^-=AX_{k-1}+Bu_{k-1} \tag{1}</script><p>少了噪音那项，像这种我们直接计算出来的我们称为先验估计值，表示为估计值上面带个减号。\<br>对于测量部分，我们有：</p>
<script type="math/tex; mode=display">
\hat{X}_{k,mea}=H^{-1}Z_{k} \tag{2}</script><p>这是测量估计值。\<br>现在我们有两个结果，分别是先验估计值和测量估计值，但这两个都不具备噪声的影响，都是不准确的，然后卡尔曼滤波器的作用就出来了：通过两个不太准确的结果得出一个比较准确的结果。\<br>根据上面【数据融合】那部分的知识，后验估计值(就是我们想要的)可以写成：</p>
<script type="math/tex; mode=display">
\hat{X}_k=\hat{X}_{k}^-+G(H^{-1}Z_k-\hat{X}_{k}^-)</script><p>这里$G\in [0,1]$，当$G=0$时，$\hat{X}_k=\hat{X}_{k}^-$，表明更相信计算的结果，也就是先验估计值；如果$G=1$，$\hat{X}_k=H^{-1}Z_{k}$，表明更相信测量的结果。\<br>在其它地方上面的式子会写成这样：</p>
<script type="math/tex; mode=display">
\hat{X}_k=\hat{X}_k^-+K_k(Z_k-H\hat{X}_{k}^-)</script><p>这里的$G=K_kH$，其实一个一声，但要注意的是，这里的$K\in[0,H^{-1}]$。\<br>现在我们要做的就是寻找这个$K_k$，使得$\hat{X}_k$趋近于$X_{k}$。这里我们定义一个误差：</p>
<script type="math/tex; mode=display">
e_k=X_k-\hat{X}_k</script><p>这里的误差$e_k$也是一个符合正态分布的矩阵：$P(e_{k})\sim (0,P)$(这个应该很容易理解)，依旧按照上面的方法求这个协方差矩阵：</p>
<script type="math/tex; mode=display">
P=E[ee^T]=
\left[
\begin{matrix}
   \sigma_{e_1}^2 & \sigma_{e_1,e_2}  \\
   \sigma_{e_2,e_1} & \sigma_{e_2}^2
  \end{matrix}
  \right]</script><p>我们这个估计值$\hat{X}_k$越接近于实际值，就说明这个误差的方差越小，也就说明越接近于0。现在问题就变成了我们需要选取适当的$K_k$，使协方差矩阵的迹最小，即$tr(P)$最小。(元素分开考虑，所以是对角线)。</p>
<script type="math/tex; mode=display">
tr(P)=\sigma_{e_1}^2+\sigma_{e_2}^2</script><p>现在我们首先把协方差矩阵求出来，对于误差有：</p>
<script type="math/tex; mode=display">
e_k=X_k-\hat{X}_k</script><p>将其代入协方差矩阵的式子得：</p>
<script type="math/tex; mode=display">
\begin{aligned}
P  &= E[ee^T]\\
    &=E[(X_k-\hat{X}_k)(X_k-\hat{X}_{k})^T]
\end{aligned}</script><p>将</p>
<script type="math/tex; mode=display">
\hat{X}_k=\hat{X}_k^-+K_k(Z_k-H\hat{X}_{k}^-)</script><script type="math/tex; mode=display">
Z_k=HX_k+v_k</script><p>代入得：</p>
<script type="math/tex; mode=display">
\begin{aligned}
X_k-\hat{X}_k &= (X_k-\hat{X}_{k}^-)-K_kH(X_k-\hat{X}_{k}^-)-K_kv_k \\
&=(I-K_kH)(X_k-\hat{X}_{k}^-)-K_kv_k
\end{aligned}</script><p>定义先验误差 $e_k^-=(X_k-\hat{X}_{k}^-)$。\<br>所以上面的协方差矩阵可以写为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
P_k  &= E[ee^T]\\
    &= E[(X_k-\hat{X}_k)(X_k-\hat{X}_{k})^T]\\
    &= E[[(I-K_kH)e_k^--K_kv_k][(I-K_kH)e_k^--K_kv_k]^T]\\
    &= E[(I-K_kH)e_k^-{e_k^-}^T(I-K_kH)^T]-E[(I-K_kH)e_k^-v_k^TK_k^T]
        -E[K_kv_k{e_k^-}^T(I-K_kH)^T]+E[K_kv_kv_k^TK_k^T]
\end{aligned}</script><p>由于$(I-K_kH)$是常数，且对于独立的$A,B$，有$E(AB)=E(A)E(B)$，$E(e_K^-)=E(v_k^T)=0$，所以上式的中间两项为0。上式可以继续化简：</p>
<script type="math/tex; mode=display">
\begin{aligned}
P_k  &= E[ee^T]\\
    &= E[(X_k-\hat{X}_k)(X_k-\hat{X}_{k})^T]\\
    &= E[[(I-K_kH)e_k^--K_kv_k][(I-K_kH)e_k^--K_kv_k]^T]\\
    &= E[(I-K_kH)e_k^-{e_k^-}^T(I-K_kH)^T]-E[(I-K_kH)e_k^-v_k^TK_k^T]
        -E[K_kv_k{e_k^-}^T(I-K_kH)^T]+E[K_kv_kv_k^TK_k^T]\\
    &= (I-K_kH)E[(e_k^-{e_k^-}^T)](I-K_kH)^T+K_kE[v_kv_k^T]K_k^T
\end{aligned}</script><p>由于$E[(e_k^-{e_k^-}^T)]$为先验误差的协方差矩阵$P_k^-$，$R=E[v_kv_k^T]$上面等式就可以继续化简：</p>
<script type="math/tex; mode=display">
\begin{aligned}
P_k  &= E[ee^T]\\
    &= E[(X_k-\hat{X}_k)(X_k-\hat{X}_{k})^T]\\
    &= E[[(I-K_kH)e_k^--K_kv_k][(I-K_kH)e_k^--K_kv_k]^T]\\
    &= E[(I-K_kH)e_k^-{e_k^-}^T(I-K_kH)^T]-E[(I-K_kH)e_k^-v_k^TK_k^T]
        -E[K_kv_k{e_k^-}^T(I-K_kH)^T]+E[K_kv_kv_k^TK_k^T]\\
    &= (I-K_kH)E[(e_k^-{e_k^-}^T)](I-K_kH)^T+K_kE[v_kv_k^T]K_k^T\\
    &=(P_k^--K_kHP_k^-)(I-H^TK_k^T)+K_kRK_k^T\\
    &=P_k^--K_kHP_k^--P_k^-H^TK_k^T+K_kHP_k^-H^TK_k^T+K_kRK_k^T
\end{aligned}</script><p>现在就求出这个协方差矩阵了，接下来是求迹。我们看右边第三项，对其转置：</p>
<script type="math/tex; mode=display">
((P_k^-H^T)K_k^T)^T=K_k(P_k^-H^T)^T=K_kH{P_k^-}^T</script><p>也就是说，第三项的转置等于第二项，所以第三项的迹与第二项相同。所以协方差矩阵的迹为：</p>
<script type="math/tex; mode=display">
tr(P_k)=tr(P_k^-)-2tr(K_kHP_k^-)+tr(K_kHP_k^-H^TK_k^T)+tr(K_kRK_k^T)</script><p>现在要求迹最小时对应的$K_k$，即是求导等于0。\<br>这里先说个东西：</p>
<script type="math/tex; mode=display">
\frac{dtr(AB)}{dA}=B^T</script><script type="math/tex; mode=display">
\frac{d(ABA^T)}{dA}=2AB</script><p>(这个自己验证)\<br>所以：</p>
<script type="math/tex; mode=display">
\frac{dtr(P_k)}{dK_k}=0-2(HP_k^-)^T+2K_kHP_k^-H^T+2K_kR=0</script><p>即：</p>
<script type="math/tex; mode=display">
-P_k^-H^T+K_k(HP_k^-H^T+R)=0</script><script type="math/tex; mode=display">
K_k=\frac{P_k^-H^T}{HP_k^-H^T+R}</script><p>这就是卡尔曼滤波器最核心的公式，分析一下，当$R$特别大时，即测量噪声特别大时，$K_k$就趋近于0，此时估计值$\hat{X}_k$就等于先验估计$\hat{X}_{k}^-$，我们就更愿意相信计算的先验估计；当$R$很小时，$K_k$就趋近于$H^{-1}$，估计值就等于测量估计。</p>
<h1 id="误差协方差矩阵"><a href="#误差协方差矩阵" class="headerlink" title="误差协方差矩阵"></a>误差协方差矩阵</h1><p>上面我们得出了卡尔曼增益的表达式：</p>
<script type="math/tex; mode=display">
K_k=\frac{P_k^-H^T}{HP_k^-H^T+R}</script><p>在这个表达式中$P_k^-$我们还不知道，这部分就是对这个的推导。\<br>我们已知：</p>
<script type="math/tex; mode=display">
P_k^-=E[e_k^--{e_k^-}^T]</script><script type="math/tex; mode=display">
\begin{aligned}
e_k^- &=X_k-\hat{X}_k^-\\
    &= AX_{k-1}+Bu_{k-1}+w_{k-1}-A\hat{X}_{k-1}-Bu_{k-1}\\
    &= A(X_{k-1}-\hat{X}_{k-1})+w_{k-1}\\
    &= Ae_{k-1}+w_{k-1}
\end{aligned}</script><p>代入上面的协方差矩阵：</p>
<script type="math/tex; mode=display">
\begin{aligned}
P_k^- &=E[e_k^--{e_k^-}^T]\\
    &= E[(Ae_{k-1}+w_{k-1})((Ae_{k-1})^T+(w_{k-1})^T)]\\
    &= E[Ae_{k-1}e_{k-1}^TA^T]+E[Ae_{k-1}w_{k-1}^T]+E[w_{k-1}e_{k-1}^TA^T]+E[w_{k-1}w_{k-1}^T]
\end{aligned}</script><p>这里，由于$e_{k-1}$和$w_{k-1}$是相互独立的（$e_{k-1}$与$w_{k-1}$无关而与$w_{k-2}$有关），所以相乘的期望等于期望的相乘，都等于0，上面式子中间两项等于0，可以写为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
P_k^- &=E[e_k^--{e_k^-}^T]\\
    &= E[(Ae_{k-1}+w_{k-1})((Ae_{k-1})^T+(w_{k-1})^T)]\\
    &= E[Ae_{k-1}e_{k-1}^TA^T]+E[Ae_{k-1}w_{k-1}^T]+E[w_{k-1}e_{k-1}^TA^T]+E[w_{k-1}w_{k-1}^T]\\
    &= AE[e_{k-1}e_{k-1}^T]+E[w_{k-1}w_{k-1}^T]\\
    &= AP_{k-1}A^T+Q
\end{aligned}</script><p>至此，就可以用卡尔曼滤波器来估计状态变量的值了。这个过程分为两步：</p>
<blockquote>
<p><strong>预测：</strong>\<br>&emsp;计算先验：<script type="math/tex">\hat{X}_k^-=A\hat{X}_{k-1}+Bu_{k-1} \tag{1}</script>\<br>&emsp;先验误差协方差：<script type="math/tex">P_k^-=AP_{k-1}A^T+Q \tag{2}</script></p>
<p><strong>校正：</strong>\<br>&emsp;卡尔曼增益：</p>
<script type="math/tex; mode=display">
K_k=\frac{P_k^-H^T}{HP_k^-H^T+R} \tag{3}</script><p>&emsp;后验估计：</p>
<script type="math/tex; mode=display">
\hat{X}_k=\hat{X}_{k}^-+K_k(Z_k-H\hat{X}_k^-) \tag{4}</script></blockquote>
<p>由于在预测中用了上一次的误差协方差，所以要更新这个参数，在上一部分我们得出了：</p>
<script type="math/tex; mode=display">
P_k=P_k^--K_kHP_k^--P_k^-H^TK_k^T+K_kHP_k^-H^TK_k^T+K_kRK_k^T</script><p>将算出的$K_k$代入得：</p>
<script type="math/tex; mode=display">
P_k=P_k^--K_kHP_k^-=(I-K_kH)P_k^- \tag{5}</script><p>上面公式(1) — (5)就是卡尔曼滤波器最重要的5个公式了，在最开始的时候我们要初始化$\hat{X}_0$ 和 $P_{0}$。</p>
]]></content>
      <categories>
        <category>学习总结</category>
      </categories>
  </entry>
  <entry>
    <title>方差分析</title>
    <url>/posts/22.html</url>
    <content><![CDATA[<h1 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h1><p>对于比较两个总体的均值，t-test 是个不错的选择，但假如我们现在有很多个总体呢，t-test 好像就变得不那么理想了，毕竟比较所有成分效率太低。\<br>我们需要一种检测方法，它可以告诉我们在这些总体的任意一个地方是否有任何显著的差异，如果没有的话，那我们就没必要进一步探究了。\<br>要分析均值间的方差，可以用 F test，或者又叫方差分析（Way Analysis of Variance，ANOVA）。<br><span id="more"></span></p>
<h1 id="单因素方差分析"><a href="#单因素方差分析" class="headerlink" title="单因素方差分析"></a>单因素方差分析</h1><p>在试验过程中，只有一个因素在改变，称为单因素试验，而单因素方差分析主要用来验证这种试验中两组或两组以上的样本均值是否有显著性差异。\<br>举个例子，工厂有5台机器生产同一种零件，我们想知道这几台机器生产的零件重量（或者其它指标）是否一致，就可以转化为验证这5个总体的均值是否一致，这里考察的就是机器这一因素对零件重量有无影响。\<br>如下图（这个图跟这个例子不对应，但意思是一样的，即<strong>组内方差和组间方差</strong>，因为现在还不懂在这里要怎么画图。。）：<img src="/images/259.png" alt=""><br>也就是说，F可以写为：</p>
<script type="math/tex; mode=display">
F=\frac{Between \quad group\quad  variability}{Within \quad group\quad variability}=\frac{MS_b}{MS_w}</script><p>当：</p>
<blockquote>
<ol>
<li>$F=1$: $H_0$ 为真，每组平均值之间有相同的变化量，与机器无关；</li>
<li>$F&gt;1$: $H_0$ 为假，平均值之间的巨大差异可能不是偶然造成的，表明至少存在一个机器与其它机器不同。</li>
</ol>
</blockquote>
<p>上面只是一个简单的例子，具体地，我们要根据样本的关系进行如下分类。</p>
<h2 id="样本大小相等"><a href="#样本大小相等" class="headerlink" title="样本大小相等"></a>样本大小相等</h2><p>现假设我们有 $m$ 组独立样本，每组样本有 $n$ 个元素，即可以写成：</p>
<script type="math/tex; mode=display">
X_{ij}\sim \mathcal{N}(\mu_i,\sigma^2)\quad i=1,...,m;\quad j=1,...,n</script><p>我们做出如下假设：</p>
<blockquote>
<p>$H_0$: $\mu_1=\mu_2=…=\mu_m$ vs $H_1$: 并非所有均值都相等。\</p>
</blockquote>
<p>继续往下，方差分析这里最重要的就是计算组内以及组间方差，为此需要计算如下两种均值：</p>
<blockquote>
<ol>
<li>组内均值：令 $\overline{x}_{i\cdot}$ 为第 $i$ 组样本的均值，即：<script type="math/tex; mode=display">
\overline{x}_{i\cdot}=\sum_{j=1}^n\frac{x_{ij}}{n}</script></li>
<li>总体均值：用$\overline{x}_{\cdot \cdot}$表示对$\mu$的估计，即：<script type="math/tex; mode=display">
\overline{x}_{\cdot \cdot} = \frac{\sum_{i=1}^m\sum_{j=1}^nx_{ij}}{nm}=\frac{\sum_{i=1}^m\overline{x}_{i\cdot}}{m}</script></li>
</ol>
</blockquote>
<p>然后通过下面表格就可以计算出F的值：<img src="/images/260.png" alt=""></p>
<h2 id="样本大小不等"><a href="#样本大小不等" class="headerlink" title="样本大小不等"></a>样本大小不等</h2><p>现假设我们有 $m$ 组独立样本，每组样本有 $n_1,…,n_m$ 个元素，即可以写成：</p>
<script type="math/tex; mode=display">
X_{ij}\sim \mathcal{N}(\mu_i,\sigma^2)\quad i=1,...,m;\quad j=1,...,n</script><p>同样我们做出如下假设：</p>
<blockquote>
<p>$H_0$: $\mu_1=\mu_2=…=\mu_m$ vs $H_1$: 并非所有均值都相等。\</p>
</blockquote>
<p>依旧计算两种均值：</p>
<blockquote>
<ol>
<li>组内均值：$\overline{x}_{i\cdot}=\sum_{j=1}^{n_i}\frac{x_{ij}}{n_i}$\<br>\</li>
<li>总体均值：$\overline{x}_{\cdot \cdot} = \frac{\sum_{i=1}^m\sum_{j=1}^{n_i}x_{ij}}{\sum_{i=1}^mn_i}$</li>
</ol>
</blockquote>
<p>令 $N = \sum_in_i-m$。\<br>然后通过下面表格就可以计算出F的值：<img src="/images/261.png" alt=""></p>
<h2 id="一个例子"><a href="#一个例子" class="headerlink" title="一个例子"></a>一个例子</h2><p>下图表示从5种不同的方法中采集的数据，现在我们要判断这5种不同的方法是否会给出不同的结果。\<br><img src="/images/262.png" alt=""><br>均值之类的都已经算好了，直接带入上面表格里的式子可得：</p>
<script type="math/tex; mode=display">
SS_b=150.50 \quad SS_w=87.43</script><p>则均方根(mean Squares) 为：</p>
<script type="math/tex; mode=display">
MS_b=\frac{150.50}{4}=37.63 \quad MS_w=\frac{87.43}{26}=3.36</script><p>测试统计量为：$\frac{MS_b}{MS_w}=11.19$\<br>通过查找F分布表可得：$F_{4,26,0.05}=2.743$\<br>由于11.19 &gt; 2.743，所以拒绝零假设。</p>
<h1 id="双因素方差分析"><a href="#双因素方差分析" class="headerlink" title="双因素方差分析"></a>双因素方差分析</h1><p>字面意思，这里的方差分析涉及到两个因素，比如说，我们要探讨不同温度及营养元素对培养皿微生物繁殖的影响，就可以使用双因素方差分析。\<br>另一个例子，如下图：<img src="/images/263.png" alt=""><br>给出的是不同温度与材料下发动机的寿命(月)。\<br>计算跟单方差分析还是挺类似的，定义以下变量：$\mu,\alpha_i,\beta_j,\gamma_{ij},i=1,…,m,j=1,…,n$，并有如下关系：</p>
<blockquote>
<ol>
<li>$\mu=\mu_{\cdot\cdot}$：总体平均值（即所有元素）;\</li>
<li>$\alpha_i=\mu_{i\cdot}-\mu_{\cdot\cdot}$：第$i$行的影响;\</li>
<li>$\beta_j=\mu_{\cdot j}-\mu_{\cdot\cdot}$：第$j$列的影响;\</li>
<li>$\gamma_{ij}=\mu_{ij}-(\mu+\alpha_i+\beta_j)=\mu_{ij}-\mu_{i\cdot}-\mu_{\cdot j}+\mu_{\cdot\cdot}$：行𝑖和列𝑗的相互作用。\<br>（这里 $\mu_{ij}=E[X_{ij}]$）</li>
</ol>
</blockquote>
<p>零假设这里有三个：</p>
<blockquote>
<ol>
<li>$H_0^r: \alpha_i=0$, for all i（即是行没有影响）;\</li>
<li>$H_0^c: \beta_j=0$, for all j（即是列没有影响）;\</li>
<li>$H_0^{int}: \gamma_{ij}=0$, for all i, j（即是行列没有相互作用）。</li>
</ol>
</blockquote>
<p>更一般的情况，也就是每行每列多个观测值。\<br>现假设每行每列有 $l$ 个观测值，比如上面那张图里 $l=3$，并假设所有观测值都是独立的正态随机变量，方差均为 $\sigma^2$。\<br>假设数据表示为 $X_{ijk}$, $i=1,…,m \quad j=1,…,n \quad k=1,…,l$，则无偏估计为：</p>
<blockquote>
<p>$\hat{\mu}=\overline{x}_{\cdot\cdot\cdot}$\<br>$\hat{\alpha}_i=\overline{x}_{i\cdot\cdot}-\overline{x}_{\cdot\cdot\cdot}$\<br>$\hat{\beta}_j=\overline{x}_{\cdot j \cdot}-\overline{x}_{\cdot \cdot\cdot}$\<br>$\hat{\gamma}_{ij}=\overline{x}_{ij\cdot}-\overline{x}_{\cdot j \cdot}+\overline{x}_{\cdot\cdot\cdot}$</p>
</blockquote>
<p>最后就可以根据下图，计算，查表然后判断了：<img src="/images/264.png" alt=""><br>可以拿上面那个发动机的数据计算，这里就不写了，就只是代公式而已，最后结果会是：拒绝$H_0^c$, 接受另外两个零假设。\</p>
]]></content>
      <categories>
        <category>Data Science</category>
      </categories>
  </entry>
  <entry>
    <title>假设性检测 (t-test, z-test)</title>
    <url>/posts/21.html</url>
    <content><![CDATA[<h1 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h1><p>在实际生活中，我们处理的往往是很大量的数据，分析时经常要对这些数据的某种规律提出一个假设，但我们有时没法直接用总数据来验证这个假设，而是通过样本数据来推断，决定是否拒绝这一假设，这样的统计活动成为假设检验。\<br>假设检测方法的均值对比方法主要有t-test和z-test，以下介绍这两个检测以及中间涉及到的F-test。<br><span id="more"></span></p>
<p>该检测一般可以简单概况为以下4个步骤：</p>
<blockquote>
<ol>
<li>提出零假设$H_0$以及对应的备选假设$H_1$(分为以下两种): \<br> &emsp; Non-directional(two tailed), e.g. $H_0:\mu=10,H_1:\mu_0\neq 10$;\<br> &emsp; Directional(one tailed), e.g. $H_0:\mu=10,H_1:\mu_0&gt;10$(or $\mu_0&lt;10$);\</li>
<li>设定拒绝$H_0$的标准:\<br> &emsp; Set the significance level $\alpha$, e.g. 0.05;\<br> &emsp; Find the criteria for a decision: the critical value in z- or t-distribution;\<br> &emsp; Two tailed for non directional alternative hypothesis;\<br> &emsp; One tailed for directional alternative hypothesis;\</li>
<li>计算测试统计数据:\<br> &emsp; $\sigma$已知：z-score;\<br> &emsp; $\sigma$未知：t-score;\<br> &emsp; $\sigma$未知但样本数据非常大：z-score。\</li>
<li>决定是否拒绝零假设。\</li>
</ol>
</blockquote>
<h1 id="One-sample-case-for-the-mean"><a href="#One-sample-case-for-the-mean" class="headerlink" title="One sample case for the mean"></a>One sample case for the mean</h1><h2 id="z-test"><a href="#z-test" class="headerlink" title="z-test"></a>z-test</h2><p>首先给一个图：<img src="/images/251.png" alt=""><br>如果 $\overline{x}$ 是正态分布的话，则$z=\frac{\overline{x}-\mu_0}{\sigma_{\overline{x}}}$也是正态分布的（这里$\sigma_{\overline{x}}$是确定或已知的）。\<br>对于z检验，我们使用（累积）正态分布的表格来找出$z_{\alpha}$的值。\</p>
<h3 id="一个例子"><a href="#一个例子" class="headerlink" title="一个例子"></a>一个例子</h3><p>现用一个例子来说明z-test的步骤。\<br>以下20个样本是从已知标准差为5的正态分布中产生的。<img src="/images/252.png" alt=""><br>现要检测该群体的均值是否大于6。\</p>
<blockquote>
<ol>
<li><strong>Step 1</strong>: 设置假设：$H_0: \mu=6$ vs $H_1:\mu&gt;6$;\</li>
<li><strong>Step 2</strong>: 设置$\alpha$，比如说等于0.05，然后从下表中查出其临界值为：$z_{\alpha}=1.645$ <img src="/images/253.png" alt=""></li>
<li><strong>Step 3</strong>: 计算样本的均值和标准差：<img src="/images/254.png" alt=""></li>
<li><strong>Step 4</strong>: 计算检测统计值：<script type="math/tex; mode=display">
z=\frac{\overline{x}-\mu_0}{\sigma_{\overline{x}}}=\frac{5.572-6}{6.287/\sqrt{20}}=-0.304</script></li>
<li><strong>Step 5</strong>: 做决策：由于$z &lt; z_{\alpha}$，所以不拒绝零假设$H_0$。</li>
</ol>
</blockquote>
<h2 id="t-test"><a href="#t-test" class="headerlink" title="t-test"></a>t-test</h2><p>依旧先给一个图：<img src="/images/255.png" alt=""><br>如果 $\overline{x}$ 是正态分布的话，则$t=\frac{\overline{x}-\mu_0}{s_x}$服从 t 分布（因为$s_x$是一个估计值，因此是另一个随机变量的输出）。\<br>这意味着，对于t检验，我们使用t分布的表格来找出𝑡的值。这个表比上面那个累积正态表复杂一丢丢，这里大概说一下怎么查。\<br>首先这个t分布表长这样：<img src="/images/256.png" alt=""><br>$t_{\alpha}$是基于$(n-1)$的，这个值称为自由度degree of freedom (df)。\<br>举个例子，比如说样本尺寸是7，单尾，则$t_{0.05}$为1.9432。\</p>
<h1 id="Two-sample-case-for-the-mean"><a href="#Two-sample-case-for-the-mean" class="headerlink" title="Two sample case for the mean"></a>Two sample case for the mean</h1><p>上面的都是单样本的情况，这里开始介绍双样本，字面意思，就是两个样本（好像在说废话。。。。）。\<br>跟上面的区别还是有点大的，主要有以下两个方面：\</p>
<blockquote>
<ol>
<li>假设不同，举两个例子：\<br> &emsp; 女学生的GPA平均值是否要高于男学生；\<br> &emsp; 在数据科学领域，女性的工资平均值是否高于男性；\</li>
<li>情况不同：\<br> &emsp; 样本独立：\<br> &emsp; &emsp; 方差已知；\<br> &emsp; &emsp; 方差未知：\<br> &emsp; &emsp; &emsp; 总体方差未知但总体方差相等；\<br> &emsp; &emsp; &emsp; 总体方差未知但总体方差不相等；\<br> &emsp; &emsp; &emsp; 总体方差未知，也不知道它们的关系。\<br> &emsp; 样本不独立。</li>
</ol>
</blockquote>
<p>以下对上面情况进行分别讨论。</p>
<h2 id="样本独立，方差已知"><a href="#样本独立，方差已知" class="headerlink" title="样本独立，方差已知"></a>样本独立，方差已知</h2><p>方差已知的话就是z-test了。\<br>两总体均值差异为：$\overline{X}-\overline{Y}\sim \mathcal{N}(\mu_X-\mu_Y,\frac{\sigma_X^2}{n}+\frac{\sigma_Y^2}{m})$，其中$n,m$分别为$X,Y$两样本的大小。\<br>上面这个复杂的正态分布也可以写为：$\frac{\overline{X}-\overline{Y}-(\mu_X-\mu_Y)}{\sqrt{\frac{\sigma_X^2}{n}+\frac{\sigma_Y^2}{m}}}\sim \mathcal{N}(0,1)$，其实就是z-score。\<br>所以，当$H_0$为真时，即 $\mu_X-\mu_Y=0$ 时，z-test的统计量为：</p>
<script type="math/tex; mode=display">
z=\frac{\overline{X}-\overline{Y}}{\sqrt{\frac{\sigma_X^2}{n}+\frac{\sigma_Y^2}{m}}}</script><h2 id="样本独立，方差未知"><a href="#样本独立，方差未知" class="headerlink" title="样本独立，方差未知"></a>样本独立，方差未知</h2><p>方差未知即是用t-test。</p>
<h3 id="case-1：方差相等"><a href="#case-1：方差相等" class="headerlink" title="case 1：方差相等"></a>case 1：方差相等</h3><p>方差相等即：$\mu_X^2=\mu_Y^2$。\<br>测试统计量为：$t=\frac{(\overline{x}-\overline{y})}{s_{\overline{x}-\overline{y}}}$，其中：</p>
<script type="math/tex; mode=display">
s_{\overline{x}-\overline{y}}=\sqrt{s^2(\frac{1}{n}+\frac{1}{m})}</script><script type="math/tex; mode=display">
s^2=\frac{\sum^n_{i=1}(x_i-\overline{x})^2+\sum^m_{j=1}(y_j-\overline{y})^2}{n+m-2}</script><p>或：</p>
<script type="math/tex; mode=display">
s^2=\frac{(n-1)s_x^2+(m-1)s_y^2}{n+m-2}</script><script type="math/tex; mode=display">
df=n+m-2</script><h3 id="case-2：方差不相等"><a href="#case-2：方差不相等" class="headerlink" title="case 2：方差不相等"></a>case 2：方差不相等</h3><p>方差不相等即：$\mu_X^2\neq\mu_Y^2$。\<br>测试统计量为：$t=\frac{(\overline{x}-\overline{y})}{s_{\overline{x}-\overline{y}}}$，其中：</p>
<script type="math/tex; mode=display">
s_{\overline{x}-\overline{y}}=\sqrt{\frac{s_x^2}{n}+\frac{s_y^2}{m}}=\sqrt{s_{\overline{x}}^2+s_{\overline{y}}^2}</script><script type="math/tex; mode=display">
df = \frac{(s_{\overline{x}}^2+s_{\overline{y}}^2)^2}{\frac{(s_{\overline{x}}^2)^2}{n-1}+\frac{(s_{\overline{y}}^2)^2}{m-1}}</script><p>\<br><strong>上面两个case中：</strong>\<br>$s_{\overline{x}}^2=\frac{s_x^2}{n};\quad s_{\overline{y}}^2=\frac{s_y^2}{m}; \quad s_x^2=\frac{\sum_{i=1}^n(x_i-\overline{x})^2}{n-1}; \quad s_y^2=\frac{\sum_{j=1}^m(y_j-\overline{y})^2}{m-1}$</p>
<h3 id="case-3：方差关系未知"><a href="#case-3：方差关系未知" class="headerlink" title="case 3：方差关系未知"></a>case 3：方差关系未知</h3><p>这里我们不知道 $\sigma_X^2$ 和 $\sigma_Y^2$ 相不相等，首先用 $F_{\max} test$ 检测方差的相等关系。\<br>这里简单补充下 $F_{\max}test$ 的相关内容。(这里仅补充这里需要用到的，更具体的看另一篇【F-test 方差分析】)。\</p>
<h4 id="F-max-test"><a href="#F-max-test" class="headerlink" title="$F_{\max}test$"></a>$F_{\max}test$</h4><p>F 检测是一种方差差异性检测。\<br>测试统计量为：$F=\frac{s_x^2}{s_y^2}$，一般把较大的放分子上。\<br>F的临界值由以下三部分确定：\</p>
<blockquote>
<ol>
<li>置信度$\alpha$；</li>
<li>分子的自由度$df$；</li>
<li>分母的自由度$df$。</li>
</ol>
</blockquote>
<p>根据这三个条件就可以查表了，由于F检测更多的是确定置信区间，所以要记得下面这个等式。</p>
<script type="math/tex; mode=display">
\frac{1}{F_{1-\frac{\alpha}{2}},m,n}=F_{\frac{\alpha}{2},n,m}</script><p>(注意这里 $m,n$ 的顺序)。\<br>决策依据是：\</p>
<blockquote>
<ol>
<li>拒绝$H_0$: $F&gt;F_{\frac{\alpha}{2}} \quad \rightarrow s_x^2 &gt; s_y^2$；\</li>
<li>拒绝$H_0$: $F&lt; F_{1-\frac{\alpha}{2}} \quad \rightarrow s_x^2 &lt; s_y^2$；\</li>
</ol>
</blockquote>
<p>置信区域如下：<img src="/images/258.png" alt=""><br>这里简单推导一下这个等式的由来。\<br>设$F\sim F(n,m)$，则$\frac{1}{F}\sim F(m,n)$。若</p>
<script type="math/tex; mode=display">
P\left\{ F>F_{1-\frac{\alpha}{2}}(n,m)\right\}=1-\frac{\alpha}{2}</script><p>即：</p>
<script type="math/tex; mode=display">
P\left\{ F< F_{1-\frac{\alpha}{2}}(n,m)\right\}=\frac{\alpha}{2}</script><p>则有：</p>
<script type="math/tex; mode=display">
P\left\{ \frac{1}{F}>\frac{1}{F_{1-\frac{\alpha}{2}}(n,m)}\right\}=\frac{\alpha}{2}</script><p>由于$\frac{1}{F}\sim F(m,n)$，转换一下就有：</p>
<script type="math/tex; mode=display">
P\left\{ \frac{1}{F}>{F_{\frac{\alpha}{2}}(m,n)}\right\}=\frac{\alpha}{2}</script><p>所以就得到了上面的等式：</p>
<script type="math/tex; mode=display">
\frac{1}{F_{1-\frac{\alpha}{2}},m,n}=F_{\frac{\alpha}{2},n,m}</script><p><strong>回到 case 3</strong>\<br>运用F-test 进行方差检测，即：\</p>
<blockquote>
<ol>
<li><p>$H_0$: $\sigma_X^2=\sigma_Y^2$ 或 $\frac{\sigma_X^2}{\sigma_Y^2}=1$\</p>
</li>
<li><p>$H_1$: $\sigma_X^2 \neq \sigma_Y^2$ 或 $\frac{\sigma_X^2}{\sigma_Y^2}\neq 1$\<br>(non-directional / two-tailed)</p>
</li>
</ol>
</blockquote>
<p>然后根据 F-test 的结果用上面 case 1 或者 case 2 的方法继续下去。</p>
<h4 id="case-3-的一个例子"><a href="#case-3-的一个例子" class="headerlink" title="case 3 的一个例子"></a>case 3 的一个例子</h4><p>现在有两个导师A，B教同一门课，现在要探究哪个导数教的学生成绩更好。有以下数据：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">amount</th>
<th style="text-align:center">average</th>
<th style="text-align:center">Standard deviation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">A</td>
<td style="text-align:center">7</td>
<td style="text-align:center">87</td>
<td style="text-align:center">15</td>
</tr>
<tr>
<td style="text-align:center">B</td>
<td style="text-align:center">7</td>
<td style="text-align:center">80</td>
<td style="text-align:center">10</td>
</tr>
</tbody>
</table>
</div>
<p>A导师说他更厉害，那我们现在就想检测他说的是不是真的。\<br>首先我们要检测两总体的方差是否相等：$H_0:\sigma_A^2=\sigma_B^2$ vs $H_1:\sigma_A^2\neq \sigma_B^2$; \</p>
<blockquote>
<ol>
<li>$F_{\max}$test：$F=\frac{s_A^2}{s_B^2}=\frac{15^2}{10^2}=2.25$；\</li>
<li>A, B 两类均有7个样本，则自由度均为：$df=n-1=7-1=6$；\</li>
<li>选取 $\alpha$ 值，这里令 $\alpha = 0.05$，寻找寻找分子分母自由度均为6的边界值 $F_{cv}$；\</li>
<li>从下面这个表中读取边界值：<img src="/images/257.png" alt=""><br>边界值 $F_{\alpha/2}=5.82$，即$F_{1-\alpha/2}=\frac{1}{5.82}=0.17$</li>
<li>由于 $F$ 是位于0.17和5.82之间的，所以接受零假设，$\sigma_A^2=\sigma_B^2$。</li>
</ol>
</blockquote>
<p>我们得出了方差相等，现在继续往下，为验证导师A的话，做出如下新的假设：\</p>
<blockquote>
<ol>
<li>$H_0: \mu_A=\mu_B$ vs $H_1: \mu_A&gt;\mu_B$，用case 1的方法;\</li>
<li>计算：$s_{\overline{A}-\overline{B}}=\sqrt{\frac{(n-1)s_A^2+(m-1)s_B^2}{n+m-2}(\frac{1}{n}+\frac{1}{m})}=6.81$\</li>
<li>t 的统计量为：$t=\frac{87-80}{6.81}=1.03$，自由度为：7+7-2=12; \</li>
<li>边界值为（one tailed）：$t_{0.05,12}=1.782$；\</li>
<li>决策：由于 1.03 &lt; 1.782，我们接受零假设，导师A的说法不靠谱。</li>
</ol>
</blockquote>
<p>另外多说一点，如果上面的数据，其它不变，但样本都变成700的话，就会发现导师A的说法是可信的，这个自行计算。\<br>值得注意的是，如果样本 n 很大，我们也可以用 z-test，700已经足够大了，有700个自由度的 t 分布已经很接近于正态分布了。\</p>
]]></content>
      <categories>
        <category>Data Science</category>
      </categories>
  </entry>
  <entry>
    <title>【ML-2020/21】Generative Adversarial Network (GAN) --- Part-1 基本介绍</title>
    <url>/posts/18.html</url>
    <content><![CDATA[<h1 id="写在前面的说明"><a href="#写在前面的说明" class="headerlink" title="写在前面的说明"></a>写在前面的说明</h1><p>这个系列【ML-2020/21】大部分是课上内容的简单复述，之前上过但因为笔记写得很乱就忘了很多，所以重来一遍。与其看我这篇，不如直接去看视频，讲得还更生动。视频系列链接$\rightarrow$<a href="https://www.youtube.com/watch?v=OP5HcXJg2Aw">这里</a>。</p>
<span id="more"></span>
<h1 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h1><p>Generative Adversarial Network (GAN) 中文名是【生成式对抗网络】，用这个网络的目的是为了让机器生成东西（比如图片，文章等），其基本网络结构包括两大部分：</p>
<blockquote>
<ol>
<li>Generation：生成器</li>
<li>Discriminator：鉴别器</li>
</ol>
</blockquote>
<h2 id="Generation"><a href="#Generation" class="headerlink" title="Generation"></a>Generation</h2><p>在generation过程中我们需要做的就是训练一个生成器，比如在影像生成中，我们希望得到一个生成器，随便输入一个向量，通过这个生成器能得到一张对应图片，不同的向量对应不同的图片；在文字生成中也是一样，输入一个向量能得到对应的语句。<br>基本形式就像下图一样：</p>
<p><img src="../images/239.png" alt=""></p>
<p>一个Generation就是一个神经网络，更简单来说，就是一个方程，输入一个向量，就能得到一个更高维度的向量，将其进行通道，图片尺寸进行重组后就是一张新的图片。<br>以图像生成为例，通常输入向量的每一个dimension会对应到图像的某一个特征，比如头发长度，眼镜大小，颜色等。</p>
<h2 id="Discriminator"><a href="#Discriminator" class="headerlink" title="Discriminator"></a>Discriminator</h2><p>Discriminator也是一个神经网络，或者说方程，输入一张图片（或者一个句子），输出一个数值，这个数值越大，则说明这个输入的真实性越高。比如说下面这张图片所示：<br><img src="../images/240.png" alt=""></p>
<h2 id="Generation-vs-Discriminator"><a href="#Generation-vs-Discriminator" class="headerlink" title="Generation vs. Discriminator"></a>Generation vs. Discriminator</h2><p>在GAN中这两个神经网络可以看作是一种对抗的关系，比如说在第一个Generation中生成了一组图像，然后Discriminator根据某一个判断标准比如说图像是不是彩色的来判断这组图像是否足够真实；然后第二个Generation根据第一个Discriminator的结果产生了一组新的彩色图片，然而第二个Discriminator又根据另一个判断标准比如是否有眼睛来判断这组图片是否足够真实；再接着第三个Generation又根据第二个Discriminator的结果……这样一直“对抗”下去，直到生成的图片可以骗过Discriminator的时候，就完成了。</p>
<h2 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h2><p>根据上面它们两者的关系就可以大致写出这个算法的流程了：</p>
<blockquote>
<ol>
<li><p>初始化generator和discriminator</p>
</li>
<li><p>进入循环，每个循环进行如下步骤：</p>
</li>
</ol>
<ul>
<li><p><strong>step-1</strong>：固定住generator G，更新discriminator D（再说具体一点的话就是，用当前的G产生一组图片，在D中与真实的图片（已有的database）作对比，训练D，使真实图片高分，生成图低分。）</p>
</li>
<li><p><strong>step-2</strong>：固定住D，更新G（具体就是，把一个向量输入进G，产生一张图片，根据当前的D进行评分，训练G，使得到比较高评分）</p>
</li>
</ul>
</blockquote>
<p>（一般来说是把G和D一起当作是一个巨大的神经网络，即输入向量得到一个数值）</p>
<p>说得再具体一点就是：</p>
<blockquote>
<ol>
<li><p>初始化$D$和$G$的参数分别为$\theta_d$和$\theta_g$；</p>
</li>
<li><p>$D$的学习过程：</p>
<ul>
<li><p>从数据集database中选取$m$个样本 {$x^1,x^2,…,x^m$}；</p>
</li>
<li><p>从一个分布（高斯或均匀或其它）中采取$m$个噪音样本 {$z^1,z^2,…,z^m$}，其中噪音样本维度自己定，属于超参数；</p>
</li>
<li><p>利用$G$获取生成数据 {$\widetilde{x}^1,\widetilde{x}^2,…,\widetilde{x}^m$}，$\widetilde{x}^i = G(z^i)$；</p>
</li>
<li><p>更新$D$的参数$\theta_d$，以便最大化$\widetilde{V}$，其中：<script type="math/tex">\widetilde{V}=\frac{1}{m}\sum_{i=1}^m\log D(x^i)+\frac{1}{m}\sum_{i=1}^m\log(1-D(\widetilde{x}^i))</script> (简单说一下这个$\widetilde{V}$，第一项就是让真实图片的得分越大越好，第二项就是让生成的图片得分越小越好，其中$D(.)$一般是经过sigmoid的介于0~1的数)<script type="math/tex">\theta_d \leftarrow \theta_d + \eta \nabla \widetilde{V}(\theta_d)</script>(这里在说一句，一般这里更新不会是只update一次的，至于更新几次也属于超参数，要自己调的)</p>
</li>
</ul>
</li>
<li><p>$G$的学习过程：</p>
<ul>
<li><p>另外从一个分布（高斯或均匀或其它）中采取$m$个噪音样本 {$z^1,z^2,…,z^m$}；</p>
</li>
<li><p>更新参数$\theta_g$以最大化$\widetilde{V}$，其中<script type="math/tex">\widetilde{V}=\frac{1}{m}\sum_{i=1}^m\log(D(G(z^i)))$$$$\theta_g\leftarrow\theta_g+\eta\nabla\widetilde{V}(\theta_g)</script></p>
</li>
</ul>
</li>
<li><p>上面2，3步骤反复执行。</p>
</li>
</ol>
</blockquote>
<h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><p>GAN可以看作是一个Structured Learning的方法，基本可以看作下面两个部分组成：)<br><img src="../images/241.png" alt=""><br>Bottom Up 是指一个一个部件生成的办法，这种方法会失去全局观，也就是说不知道各个部件之间的联系；下面的Top Down 是直接观察整体，然后进行判断，generator没法用这种方法。</p>
<blockquote>
<p>问题1：能不能不用Discriminator？</p>
</blockquote>
<p>严格说来是可以的，用Auto-encoder的技术，但由于同个layer神经元之间没有联系，为了让它们产生联系，就必须多加layer，这样会把神经网络弄得很复杂，所以在同种效果下，GAN的技术会更实用。</p>
<blockquote>
<p>问题2：能不能不用Generation, 就只用Discriminator来做生成？</p>
</blockquote>
<p>也是可以的。但也很麻烦，要穷举所有可能的$x$，使<script type="math/tex">\widetilde{x}=arg\max D(x)</script>然后还要让Discriminator学会如何分别好的和坏的图片，但实际上我们的database里只有好的，不过这个问题有办法解决，如下：</p>
<blockquote>
<ol>
<li>随机产生一些坏的图片，比如随机噪音；</li>
<li>用好的图片和坏的图片去训练$D$；</li>
<li>用当前$D$产生一组新的图片，充当坏的图片继续与好的图片去训练$D$</li>
</ol>
</blockquote>
<p>步骤2-3循环。<br>效果如下：</p>
<p><img src="../images/242.png" alt=""><br>所以实际可以这么理解，generator就是用来解$arg\max$问题的，也就是用generator产生坏的图片。<br><strong>两者比较</strong><br>Generator：</p>
<blockquote>
<p>优点：<br>&emsp; 1. 容易生成东西。</p>
<p>缺点：<br>&emsp; 1. 不容易考虑部件（component）之间的关系；<br>&emsp; 2. 只是模仿表象。</p>
</blockquote>
<p>Discriminator</p>
<blockquote>
<p>优点：<br>&emsp; 1. 考虑整张图片。</p>
<p>缺点：<br>&emsp; 1. 不容易生成东西；<br>&emsp; 2. 解那个$arg\max$问题不容易。</p>
</blockquote>
]]></content>
      <categories>
        <category>ML-2020/21</category>
      </categories>
  </entry>
  <entry>
    <title>四元数（三维旋转）</title>
    <url>/posts/3.html</url>
    <content><![CDATA[<h1 id="四元数的基本知识："><a href="#四元数的基本知识：" class="headerlink" title="四元数的基本知识："></a>四元数的基本知识：</h1><p>与复数类似，四元数也由实部和虚部组成，但四元数有三个虚部，通常表示为：<script type="math/tex">q=q_{0} + q_{1}i + q_{2}j + q_{3}k</script><br>或:    <script type="math/tex">q=[s, v]^{T}, 其中: s=q_{0} \in R, v=[q_{1},q_{2},q_{3}]^{T} \in R^{3}</script><br><span id="more"></span></p>
<p>这三个虚部满足下面关系：<br><img src="https://img-blog.csdnimg.cn/20210402045509774.png#pic_center" alt=""><br>在复数中，乘以一个 $i$ 相当于旋转90度，但四元数中不同，乘以$i$ 表示对应旋转180度，这样才能保证 $ij = k$，而$i^2 = -1$表明，旋转360度后会得到一个相反的东西，<strong>要旋转两周才能回到原来的样子</strong>（看着挺反常识的。。。。。。）</p>
<h1 id="四元数的运算"><a href="#四元数的运算" class="headerlink" title="四元数的运算"></a>四元数的运算</h1><p>(这里介绍加减，乘，模，共轭，逆，数乘)\<br>现定义两个四元数<script type="math/tex">q_{a}= [s_{a},v_{a}]^{T}=s_{a}+x_{a}i+y_{a}j+z_{a}k</script> <script type="math/tex">q_{b}= [s_{b},v_{b}]^{T}=s_{b}+x_{b}i+y_{b}j+z_{b}k</script></p>
<h2 id="1-加减"><a href="#1-加减" class="headerlink" title="1).加减"></a>1).加减</h2><script type="math/tex; mode=display">q_{a} \pm q_{b} = [s_{a} \pm s_{b}, v_{a} \pm v_{b}]^{T}</script><h2 id="2-乘法"><a href="#2-乘法" class="headerlink" title="2). 乘法"></a>2). 乘法</h2><p>乘法是把$q_{a}$每一项都与$q_{b}$相乘，虚部部分的运算要按照上面图1的规则计算：</p>
<script type="math/tex; mode=display">q_{a}q_{b}=s_{a}s_{b}-x_{a}x_{b}-y_{a}y_{b}-z_{a}z_{b}\\+(s_{a}x_{b}+x_{a}s_{b}+y_{a}z_{b}-z_{a}y_{b})i\\ 
+(s_{a}y_{b}-x_{a}z_{b}+y_{a}s_{b}+z_{a}x_{b})j\\ 
+(s_{a}z_{b}+x_{a}y_{b}-y_{a}x_{b}+z_{a}s_{b})k</script><p>(看着挺复杂，但写成向量内外积的形式就会简洁很多: )</p>
<script type="math/tex; mode=display">q_{a}q_{b}=[s_{a}s_{b}-v_{a}^{T}v_{b},s_{a}v_{b}+s_{b}v_{a}+v_{a} \times v_{b}]^{T}</script><p>(由于最后一项外积的存在，这个乘法通常是不可交换的，除非$v_{a},v_{b}$在$R^{3}$中共线，此时外积项为0.)</p>
<h2 id="3-模长"><a href="#3-模长" class="headerlink" title="3). 模长"></a>3). 模长</h2><p>四元数的模长定义为：</p>
<script type="math/tex; mode=display">||q_{a}|| = \sqrt{s^{2}_{a}+x^{2}_{a}+y^{2}_{a}+z_{a}^{2}}</script><p>两个四元数乘积的模等于模的乘积：</p>
<script type="math/tex; mode=display">||q_{a}q_{b}|| =\parallel q_{a}\parallel \parallel q_{b}\parallel</script><h2 id="4-共轭"><a href="#4-共轭" class="headerlink" title="4). 共轭"></a>4). 共轭</h2><p>即是把虚部取相反数：</p>
<script type="math/tex; mode=display">q_{a}^{*}= [s_{a},-v_{a}]^{T}=s_{a}-x_{a}i-y_{a}j-z_{a}k</script><p>四元数共轭与其本身相乘会得到一个实四元数，其实部等于模长的平方：</p>
<script type="math/tex; mode=display">q_{a}q^{*}_{a}=q_{a}^{*}q_{a}=[s_{a}^{2}+v^{T}v,0]^{T}</script><h2 id="5-逆"><a href="#5-逆" class="headerlink" title="5). 逆"></a>5). 逆</h2><p>四元数的逆为：</p>
<script type="math/tex; mode=display">q^{-1}=q^{*}/ \parallel q \parallel ^{2}</script><p>若$q$为单位四元数，则其逆和共轭就是同一个量。另外，乘积的逆具有和矩阵相似的性质：</p>
<script type="math/tex; mode=display">(q_{a}q_{b})^{-1}=q^{-1}_{b}q^{-1}_{a}</script><h2 id="6-数乘"><a href="#6-数乘" class="headerlink" title="6). 数乘"></a>6). 数乘</h2><script type="math/tex; mode=display">kq_{a} = [ks_{a},kv_{a}]^{T}</script><h1 id="旋转表示"><a href="#旋转表示" class="headerlink" title="旋转表示"></a>旋转表示</h1><p>首先有一个待旋转点$p$，表示成一个纯四元数为：<script type="math/tex">p=[0,x,y,z]^{T}=[0,v]^{T}</script>其三个虚部分别对应$x,y,z$三个坐标轴，旋转四元数表示为$q$，是一个单位四元数。对于旋转后的点$p’$表示如下：<script type="math/tex">p'=qp</script><br>对于旋转有如下要求：</p>
<blockquote>
<ol>
<li>旋转前后的模应相等；\</li>
<li>旋转后的点$p’$应该也为纯四元数。</li>
</ol>
</blockquote>
<p>由于$q$为单位四元数，模为$1$，对应上面四元数模的性质可知$p’$满足条件1，但对于条件2则只有当$p$和$q$虚部正交时才能满足（原因的话网上有很多篇文章都有对其展开计算，这里就懒得写了……）。\<br>一般情况下会把旋转表示成下面这种形似：<script type="math/tex">p'=qpq^{-1}</script><br>（有些地方这个逆也会用共轭代替，这无所谓，因为$q$是单位四元数）\<br>至于为什么是这种形式，因为Hamilton发现，如果右乘一个$p$的逆，则可以同时满足上面两个条件（可具体展开乘一下或者直接用特殊值法）。</p>
<h1 id="四元数与其它旋转方式之间的转换"><a href="#四元数与其它旋转方式之间的转换" class="headerlink" title="四元数与其它旋转方式之间的转换"></a>四元数与其它旋转方式之间的转换</h1><p>任意一个单位四元数可以表示一个旋转，这个旋转也可以用旋转矩阵或者旋转向量来表示，这里主要讨论这三者的关系。    \<br>（现在把四元数乘法表示为矩阵乘法）\<br>设$q=[s,v]^{T}$，首先定义两种新的符号$^{+}$和$^{\oplus}$。<script type="math/tex">q^{+}=\begin{bmatrix} s & -v^{T} \\ v & sI+v^{\land} \end{bmatrix}</script> <script type="math/tex">q^{\oplus}=\begin{bmatrix} s & -v^{T} \\ v & sI-v^{\land} \end{bmatrix}</script><br>其中的符号$^{\land}$表示反对称矩阵，即：$A’=-A$，举个例子，设：<script type="math/tex">a=\begin{bmatrix} a_{1} \\ a_{2} \\ a_{3} \end{bmatrix}</script><br>则$a^{\land}$为：<script type="math/tex">a^{\land}=\begin{bmatrix} 0 & -a_{3} & a_{2} \\ a_{3} & 0 & -a_{1} \\ -a_{2} & a_{1} & 0 \end{bmatrix}</script><br>这两个符号将四元数映射成一个$4\times4$矩阵。\<br>所以，四元数乘法写成矩阵形式如下：<script type="math/tex">q_{1}^{+}q_{2}=\begin{bmatrix} s_{1} & -v_{1}^{T} \\ v_{1} & s_{1}I+v_{1}^{\land} \end{bmatrix} \begin{bmatrix} s_{2} \\ v_{2} \end{bmatrix}=\begin{bmatrix} -v_{1}^{T}v_{2}+s_{1}s_{2}  \\ s_{1}v_{2}+s_{2}v_{1}+v_{1}^{\land}v_{2} \end{bmatrix}=q_{1}q_{2}</script><br>同理：<script type="math/tex">q_{1}q_{2}=q_{1}^{+}q_{2}=q_{2}^{\oplus}q_{1}</script><br>进一步：<script type="math/tex">p'=qpq^{-1}=q^{+}p^{+}q^{-1}=q^{+}q^{-1 ^{\oplus}}p</script><br>代入对应矩阵：<script type="math/tex">q^{+}(q^{-1})^{\oplus}=\begin{bmatrix} s & -v^{T} \\ v & sI+v^{\land} \end{bmatrix} \begin{bmatrix} s & v^{T} \\ -v & sI+v^{\land} \end{bmatrix}=\begin{bmatrix} 1 & 0 \\ 0^{T} & vv^{T}+s^{2}I+2sv^{\land}+(v^{\land})^{2} \end{bmatrix}</script><br>由于$p$和$p’$都是纯四元数，所以上面矩阵的右下角即是从四元数到旋转矩阵的变换关系：<script type="math/tex">R=vv^{T}+s^{2}I+2sv^{\land}+(v^{\land})^{2}</script><br>然后现在是四元数到旋转向量的变换，对上式两侧求迹（对角线的总和）：<script type="math/tex">tr(R)=tr(vv^{T}+s^{2}I+2sv^{\land}+tr((v^{\land})^{2}))\\ =v_{1}^{2}+v_{2}^{2}+v_{3}^{2}+3s^{2}-2(v_{1}^{2}+v_{2}^{2}+v_{3}^{2}) \\ =(1-s^{2})+3s^{2}-2(1-s^{2}) \\ =4s^{2}-1</script><br>转角$\theta$为：<script type="math/tex">\theta=arccos\frac{tr(R-1)}{2}=arccos(2s^{2}-1)</script><br>即：<script type="math/tex">cos\theta =2s^{2}-1=2cos^{2}\frac{\theta}{2}-1</script><br>所以：<script type="math/tex">\theta = 2arccos s</script><br>然后是旋转轴：\<br>上面最后一个矩阵那，如果用$q$ 的虚部代替$p$，则表示$q$的虚部组成的向量在旋转时是不变的，即为旋转轴。此时只要把它除以它的模长即可。\<br>最后，四元数到旋转向量的转换如下：<script type="math/tex">\begin{cases} \theta = 2arccos q_{0} \\ [n_{x},n_{y},n_{z}]^{T}=[q_{1},q_{2},q_{3}]^{T} / sin\frac{\theta}{2}\end{cases}</script><br><strong><em>注：本文主要参考《视觉SLAM十四讲》</em></strong></p>
]]></content>
      <categories>
        <category>视觉SLAM相关</category>
      </categories>
  </entry>
  <entry>
    <title>【ML-2020/21】Generative Adversarial Network (GAN) --- Part-2 Conditional GAN</title>
    <url>/posts/19.html</url>
    <content><![CDATA[<h1 id="写在前面的说明"><a href="#写在前面的说明" class="headerlink" title="写在前面的说明"></a>写在前面的说明</h1><p>这个系列【ML-2020/21】大部分是课上内容的简单复述，之前上过但因为笔记写得很乱就忘了很多，所以重来一遍。与其看我这篇，不如直接去看视频，讲得还更生动。视频系列链接$\rightarrow$<a href="https://www.youtube.com/watch?v=OP5HcXJg2Aw">这里</a>。<br><span id="more"></span></p>
<h1 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h1><p>根据上一篇【基本介绍】的内容可知，给GAN输入是一个向量，但这在实际中很不实用，我们更习惯用的是输入常见的信息比如语言文字，这就是Conditional GAN 所研究的东西。<br>举个例子，比如输入‘The Dog is running’，我们就希望生成如下图片：<img src="/images/243.jpg" alt=""><br>这就是这篇要讲的东西。</p>
<h1 id="Text-to-Image"><a href="#Text-to-Image" class="headerlink" title="Text-to-Image"></a>Text-to-Image</h1><h2 id="Traditional-Supervised-Approach"><a href="#Traditional-Supervised-Approach" class="headerlink" title="Traditional Supervised Approach"></a>Traditional Supervised Approach</h2><p>首先，这可以看作是一个传统的监督学习，我们要的首先是一大堆数据集（图片），每张图片都需要一个对应的文字进行描述。简单来说流程如下：<img src="/images/244.png" alt=""><br>但用这种方法会产生一个问题，比如说我们现在有一个文字描述为“car”，可能同时对应到以下两张图片，它们都是汽车只是方向不同：<img src="/images/245.png" alt=""><br>我们用上面的方法训练出来的模型输出既要像左边也要像右边，最后就会产生这种现象：输入“car”，输出的图片是上面两种图片的平均，这可能会是一张没有意义的图。<br>所以这种传统的方法不行。</p>
<h2 id="Conditional-GAN"><a href="#Conditional-GAN" class="headerlink" title="Conditional GAN"></a>Conditional GAN</h2><p>然后现在就要引进今天的主角了。<br>回顾一下，如果按照之前介绍的最原始的GAN，这个训练流程大致是这样的：</p>
<blockquote>
<ol>
<li>首先输入一组随机向量给Generation，产生数据（图片）；\</li>
<li>将产生的图片输入Discriminator，借助真实图片，利用定义的function更新Discriminator的参数;\</li>
<li>随机再取一组噪音数据，利用上面更新后的Discriminator更新Generation的参数；\</li>
<li>重复上面两个步骤。</li>
</ol>
</blockquote>
<p>然而在Conditional-GAN中，我们输入给Generation的不再是单一的向量，同时还有（Condition）文字信息比如所“car”，借助这两个信息产生新的数据（图片）$x$；\<br>对于Discriminator，其输入除了之前产生的图片$x$，还有(Condition)文字信息$c$，输出是一个标量，这个标量描述以下两个方面：</p>
<blockquote>
<ol>
<li>$x$是否是一张真实的图片（这点跟最原始的GAN一样）；\</li>
<li>$c$和$x$是否符合。</li>
</ol>
</blockquote>
<p>这个标量的评分标准如下：</p>
<blockquote>
<ol>
<li>高分（1分）：\<br>&emsp; a). 正确的文字与真实的图片。\</li>
<li>低分（0分）：\<br>&emsp; a). 正确的文字与生成的图片；\<br>&emsp; b). 错误的文字与真实的图片。</li>
</ol>
</blockquote>
<p>基本的训练流程如下：</p>
<blockquote>
<p><strong>D-Learning:</strong>\<br>&emsp; 1. 从database中取$m$个样本：$\left\{ (c^1,x^1),(c^2,x^2),…,(c^m,x^m)\right\}$ (这个对于Discriminator来说是要给高分的)；\<br>&emsp; 2. 随机取样$m$个噪音样本：{$z^1,…,z^m$}，这$m$个样本加上上面的文字信息$c$组成新的数据$(c^i,z^i)$；\<br>&emsp; 3. 通过Generation获取生成数据：{$\widetilde{x}^1,…,\widetilde{x}^m$}，$\widetilde{x}^i=G(c^i,z^i)$\<br>&emsp; 4. 从database中取样新的$m$个数据{$\hat{x}^1,…,\hat{x}^m$}\<br>&emsp; 5. 更新Discriminator的参数$\theta_d$，以最大化下面这个方程：<script type="math/tex">\widetilde{V}=\frac{1}{m}\sum_{i=1}^m\log D(c^i,x^i)+\frac{1}{m}\sum_{i=1}^m\log(1 - D(c^i,\widetilde{x}^i))-\frac{1}{m}\sum_{i=1}^m\log(1 - D(c^i,\hat{x}^i))</script></p>
<script type="math/tex; mode=display">\theta_d \leftarrow \theta_d +\eta \nabla \widetilde{V}(\theta_d)</script><p><strong>G-Leaarning:</strong>\<br>&emsp; 1. 取样$m$个噪声样本：{$z^1,…,z^m$}；\<br>&emsp; 2. 从database中取$m$个condition：{$c^1,…,c^m$}；\<br>&emsp; 3. 更新Generator的参数$\theta_g$，以最大化下面这个方程：<script type="math/tex">\widetilde{V}=\frac{1}{m}\sum_{i=1}^m\log(D(G()c^i,z^i))</script></p>
<script type="math/tex; mode=display">\theta_g \leftarrow \theta_g +\eta \nabla \widetilde{V}(\theta_g)</script></blockquote>
<h2 id="Discriminator-架构"><a href="#Discriminator-架构" class="headerlink" title="Discriminator 架构"></a>Discriminator 架构</h2><p>常见的Discriminator架构如下：<br><img src="/images/246.png" alt=""><br>同时，又有人提出另一种架构：<br><img src="/images/247.png" alt=""></p>
<h2 id="更进一步-——-Stack-GAN"><a href="#更进一步-——-Stack-GAN" class="headerlink" title="更进一步 —— Stack GAN"></a>更进一步 —— Stack GAN</h2><p>Stack-GAN技术能让模型的性能更加优秀，其基本的想法是：先产生小张的图，再根据小张的图产生大张的图。\<br>比如说，我们要产生$256\times 256$的图，但如果直接生成这么大的图，图很可能会坏，所以Stack-GAN在train的时候把整个训练过程拆分为两部分，大概流程是：</p>
<blockquote>
<ol>
<li>输入一个文字以及一段噪声，拼接，通过第一个Generator产生一个$64\times 64$的图；\</li>
<li>这个图片进入第一个Discriminator，判断其和文字是否match；\</li>
<li>然后第二部分，输入一个文字信息以及$64\times 64$的图片，产生一张$256\times 256$的图片；\</li>
<li>最后第二个Discriminator判断这个$256\times 256$的图片是不是足够真实。</li>
</ol>
</blockquote>
<p><em>具体可参看paper：StackGAN:Text to Photo-realistic Image Synthesis with Stack Generative Adversarial Network. 【ICCV, 2017】</em></p>
<h1 id="Image-to-Image"><a href="#Image-to-Image" class="headerlink" title="Image-to-Image"></a>Image-to-Image</h1><p>这种即是：输入图片，输出也是图片。比如说黑白图片变彩色，白天变黑夜。</p>
<h2 id="Traditional-Supervised-Approach-1"><a href="#Traditional-Supervised-Approach-1" class="headerlink" title="Traditional Supervised Approach"></a>Traditional Supervised Approach</h2><p>理论上也是可以用传统的监督学习实现，但也会遇到跟上面类似的问题，导致最终产生出来的图片特别模糊。</p>
<h2 id="Conditional-GAN-1"><a href="#Conditional-GAN-1" class="headerlink" title="Conditional GAN"></a>Conditional GAN</h2><p>用GAN的话，以下图为例<img src="/images/249.png" alt=""><br>也就是输入左边简单的图片，希望产生右边真实复杂的图。\<br>其基本流程就是：将简单的图片与噪音一起输入Generator，产生一张图片；接着将这张图片与原来简单的图片一起输入Discriminator，产生一个标量。这个标量可参看上面的解释。\<br>单单这么做的话，产生的图像虽然清晰，正确率也不错，但图片里可能会产生一些奇奇怪怪的部分。为了消除这种现象，我们可以加额外的约束，比如：在Generator的输出中，我们将产生的图片与真实图片作对比，也就是说，我们希望Generator产生的图片，既能骗过Discriminator，又跟目标图片相差不要太大，如此以来，模型性能会好很多。\</p>
<h2 id="Patch-GAN"><a href="#Patch-GAN" class="headerlink" title="Patch GAN"></a>Patch GAN</h2><p>如果图片很大张，那么Discriminator就不能直接接受整张Image，因为参数会很多，容易过拟合。所以措施是分区域输入，每次检查一小块，看这一小块是好的还是坏的，至于区域设置多大，这个就自己调了。</p>
]]></content>
      <categories>
        <category>ML-2020/21</category>
      </categories>
  </entry>
  <entry>
    <title>学习记录_Computer Vision1_作业1_(1) Bayer Interpolation</title>
    <url>/posts/2.html</url>
    <content><![CDATA[<p>这次作业需要记录的有三道题，包括：</p>
<blockquote>
<ol>
<li>Bayer Interpolation\</li>
<li>Projective Transformation\</li>
<li>Image Filtering and Edge Detection</li>
</ol>
</blockquote>
<h1 id="（1）Bayer-Interpolation"><a href="#（1）Bayer-Interpolation" class="headerlink" title="（1）Bayer Interpolation"></a>（1）Bayer Interpolation</h1><span id="more"></span>
<p>作业要求：<br>    <img src="https://img-blog.csdnimg.cn/20201214062515791.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>简单来说，即是给出一张Bayer图片，将其转化为RGB图。\<br>作业给出的图片如下：<img src="https://img-blog.csdnimg.cn/20201214185058357.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p><strong>这里先对Bayer图进行简要说明。</strong>\<br>与一般RGB图不同，Bayer图的像素分布如下图所示：<br><img src="https://img-blog.csdnimg.cn/20201214063319235.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p>在Bayer图中，每个像素点都是由单一颜色所控制，而且绿色所占的比例是其他两种颜色的两倍，所以整张图片看起来会偏绿，我们所要做的就是通过插值将每个像素点的其他两种颜色算出来。</p>
<h2 id="步骤1："><a href="#步骤1：" class="headerlink" title="步骤1："></a>步骤1：</h2><p>将Bayer数据分成RGB通道，以便每个颜色通道仅保留Bayer模式给定的相应值，而缺失值则用零填充。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">separatechannels</span>(<span class="params">bayerdata</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Separate bayer data into RGB channels so that</span></span><br><span class="line"><span class="string">    each color channel retains only the respective</span></span><br><span class="line"><span class="string">    values given by the bayer pattern and missing values</span></span><br><span class="line"><span class="string">    are filled with zero</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        Numpy array containing bayer data (H,W)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        red, green, and blue channel as numpy array (H,W)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    H, W = bayerdata.shape</span><br><span class="line">    r = np.zeros((H, W))</span><br><span class="line">    g = np.zeros((H, W))</span><br><span class="line">    b = np.zeros((H, W))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(H):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(W):</span><br><span class="line">            <span class="keyword">if</span>((i + j) % <span class="number">2</span> == <span class="number">0</span>):</span><br><span class="line">                g[i][j] = bayerdata[i][j]</span><br><span class="line">            <span class="keyword">elif</span>(i % <span class="number">2</span> == <span class="number">0</span> <span class="keyword">and</span> (i + j) % <span class="number">2</span> != <span class="number">0</span>):</span><br><span class="line">                r[i][j] = bayerdata[i][j]</span><br><span class="line">            <span class="keyword">elif</span>(i % <span class="number">2</span> != <span class="number">0</span> <span class="keyword">and</span> (i + j) % <span class="number">2</span> != <span class="number">0</span>):</span><br><span class="line">                b[i][j] = bayerdata[i][j]</span><br><span class="line">    <span class="keyword">return</span> r, g, b</span><br></pre></td></tr></table></figure>
<h2 id="步骤2："><a href="#步骤2：" class="headerlink" title="步骤2："></a>步骤2：</h2><p>将单独的通道组合成图像。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">assembleimage</span>(<span class="params">r, g, b</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Assemble separate channels into image</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        red, green, blue color channels as numpy array (H,W)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Image as numpy array (H,W,3)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    H, W = r.shape</span><br><span class="line">    imgData = np.zeros((H, W, <span class="number">3</span>))</span><br><span class="line">    imgData[:,:,<span class="number">0</span>] = r[:,:]</span><br><span class="line">    imgData[:,:,<span class="number">1</span>] = g[:,:]</span><br><span class="line">    imgData[:,:,<span class="number">2</span>] = b[:,:]</span><br><span class="line">    <span class="keyword">return</span> imgData</span><br></pre></td></tr></table></figure><br>结果如下图：<br><img src="https://img-blog.csdnimg.cn/20201218023313111.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h2 id="步骤3："><a href="#步骤3：" class="headerlink" title="步骤3："></a>步骤3：</h2><p>通过使用双线性插值对Bayer模式中的缺失值进行插值。\<br>双线性插值，简单来说就是通过邻近4个点平均出中间点的数值。\<br>其实主要就是计算R,G,B三通道的卷积核。说之前先解释一个函数：scipy.ndimage.filters.convolve，（python的各种库里有非常多用于卷积的函数，改天做个对比总结）。</p>
<p><strong>scipy.ndimage.filters.convolve：</strong>\<br>(这里给个 <a href="https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.filters.convolve.html/">链接</a>，说得很详细了已经)\<br><em>scipy.ndimage.filters.convolve(input, weights, output=None,<br> mode=’reflect’, cval=0.0, origin=0)</em> </p>
<pre><code>    input: 待卷积的图片     
    weight: 卷积核       
    output:（这个未知，好像一直没用过这个，（待定））   
    mode: 边缘处理，有5种（关于各个边缘模式的选用现在还未理解）

    ‘reflect’ (d c b a | a b c d | d c b a)
    ‘constant’ (k k k k | a b c d | k k k k)
    ‘nearest’ (a a a a | a b c d | d d d d)
    ‘mirror’ (d c b | a b c d | c b a)
    ‘wrap’ (a b c d | a b c d | a b c d)
</code></pre><blockquote>
<p>cval：默认是0，当mode取constant时有效，这个的取值既是边缘外补什么值\<br>origin：默认0，应该是控制weight在input上的位置的，目前也还没用过</p>
</blockquote>
<p>这里有一个要特别注意的是，<strong>卷积核不是直接对应相乘然后相加，而是要先翻转</strong>，举个例子，比如一个卷积核设为：<br><img src="https://img-blog.csdnimg.cn/20201218094517319.png#pic_center" alt="在这里插入图片描述"><br>但实际上并不是上面这个与图片卷积，而是下面这个卷积核：<br><img src="https://img-blog.csdnimg.cn/20201218094610325.png#pic_center" alt="在这里插入图片描述"><br>这点在初始化卷积核的时候要注意（不过目前遇到的好像都是中心对称的）</p>
<p>回到题里，G通道对应的矩阵如下图：<br><img src="https://img-blog.csdnimg.cn/20201218104911416.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>按照双线性插值，很容易能得到卷积核四个角的值，又由于应该把原本的像素值保留下来，再根据边界插值条件，就可以确定出通道G的卷积核，其他两个通道同理可各自获得卷积核值。\<br>代码如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">interpolate</span>(<span class="params">r, g, b</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Interpolate missing values in the bayer pattern</span></span><br><span class="line"><span class="string">    by using bilinear interpolation</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        red, green, blue color channels as numpy array (H,W)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Interpolated image as numpy array (H,W,3)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    r_weight = np.array([[<span class="number">0.25</span>,<span class="number">0.5</span>,<span class="number">0.25</span>],[<span class="number">0.5</span>,<span class="number">1</span>,<span class="number">0.5</span>],[<span class="number">0.25</span>,<span class="number">0.5</span>,<span class="number">0.25</span>]])</span><br><span class="line">    g_weight = np.array([[<span class="number">0</span>,<span class="number">0.25</span>,<span class="number">0</span>],[<span class="number">0.25</span>,<span class="number">1</span>,<span class="number">0.25</span>],[<span class="number">0</span>,<span class="number">0.25</span>,<span class="number">0</span>]])</span><br><span class="line">    b_weight = np.array([[<span class="number">0.25</span>,<span class="number">0.5</span>,<span class="number">0.25</span>],[<span class="number">0.5</span>,<span class="number">1</span>,<span class="number">0.5</span>],[<span class="number">0.25</span>,<span class="number">0.5</span>,<span class="number">0.25</span>]])</span><br><span class="line"></span><br><span class="line">    new_r = convolve(r, r_weight, mode=<span class="string">&#x27;mirror&#x27;</span>)</span><br><span class="line">    new_g = convolve(g, g_weight, mode=<span class="string">&#x27;mirror&#x27;</span>)</span><br><span class="line">    new_b = convolve(b, b_weight, mode=<span class="string">&#x27;mirror&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#print(new_g)</span></span><br><span class="line">    <span class="comment">#print(new_r)</span></span><br><span class="line">    <span class="comment">#print(new_b)</span></span><br><span class="line"></span><br><span class="line">    H, W = new_r.shape</span><br><span class="line">    new_img = np.zeros((H, W, <span class="number">3</span>))</span><br><span class="line">    new_img[:,:,<span class="number">0</span>] = new_r[:,:]</span><br><span class="line">    new_img[:,:,<span class="number">1</span>] = new_g[:,:]</span><br><span class="line">    new_img[:,:,<span class="number">2</span>] = new_b[:,:]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> new_img</span><br></pre></td></tr></table></figure><br>最后就是对以上函数的调用，如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">problem2</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Example code implementing the steps in Problem 2</span></span><br><span class="line"><span class="string">    Note: uses display_image() from Problem 1&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    data = loaddata(<span class="string">&quot;data/bayerdata.npy&quot;</span>)</span><br><span class="line">    r, g, b = separatechannels(data)</span><br><span class="line"></span><br><span class="line">    img = assembleimage(r, g, b)</span><br><span class="line">    display_image(img)</span><br><span class="line"></span><br><span class="line">    img_interpolated = interpolate(r, g, b)</span><br><span class="line">    display_image(img_interpolated)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment">#problem1()</span></span><br><span class="line">    problem2()</span><br><span class="line">    <span class="comment">#problem3()</span></span><br><span class="line">    <span class="comment">#problem4()   </span></span><br></pre></td></tr></table></figure><br>最后得到的图片如下：<br><img src="https://img-blog.csdnimg.cn/20201218115816873.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h1 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h1><p>本次作业及该篇学习记录遗留下的问题：</p>
<blockquote>
<ol>
<li>卷积的mode是“mirror”，原因未明；\</li>
<li>要对python里各个库的卷积函数进行总结；\</li>
<li>这次作业用到的卷积函数的output以及origin参数</li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>学习总结</category>
      </categories>
  </entry>
  <entry>
    <title>学习记录_Computer Vision1_作业1_(2) Projective Transformation</title>
    <url>/posts/1.html</url>
    <content><![CDATA[<h1 id="（2）Projective-Transformation"><a href="#（2）Projective-Transformation" class="headerlink" title="（2）Projective Transformation"></a>（2）Projective Transformation</h1><span id="more"></span>
<p>作业要求：<br><img src="https://img-blog.csdnimg.cn/2020121818034961.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>简单总结一下，就是说：给２Ｄ点核其他一些信息重构回３Ｄ图。然后再回构２Ｄ图。</p>
<h2 id="步骤1："><a href="#步骤1：" class="headerlink" title="步骤1："></a>步骤1：</h2><p>笛卡尔坐标与转齐次坐标的相互转换，代码如下：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def cart2hom(points):</span><br><span class="line">  &quot;&quot;&quot; Transforms from cartesian to homogeneous coordinates.</span><br><span class="line"></span><br><span class="line">  Args:</span><br><span class="line">    points: a np array of points in cartesian coordinates</span><br><span class="line"></span><br><span class="line">  Returns:</span><br><span class="line">    points_hom: a np array of points in homogeneous coordinates</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">  one = np.ones((1,points.shape[1]))</span><br><span class="line">  points_hom = np.r_[points, one]       #Add 1 to the last line</span><br><span class="line"></span><br><span class="line">  return points_hom</span><br></pre></td></tr></table></figure><br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def hom2cart(points):</span><br><span class="line">  &quot;&quot;&quot; Transforms from homogeneous to cartesian coordinates.</span><br><span class="line"></span><br><span class="line">  Args:</span><br><span class="line">    points: a np array of points in homogenous coordinates</span><br><span class="line"></span><br><span class="line">  Returns:</span><br><span class="line">    points_hom: a np array of points in cartesian coordinates</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">  points_cart = np.delete(points/points[-1], points.shape[0]-1, axis=0)    #Delete the last line</span><br><span class="line">  return points_cart</span><br></pre></td></tr></table></figure><br>这里补充一点关于齐次坐标的东西…\<br>（本来想自己总结的，但偶然看到一篇写得实在好，就在这里给个<a href="https://blog.csdn.net/zhuiqiuzhuoyue583/article/details/95228010">链接</a>了）\<br>（这里简单总结总结下，当作唤醒记忆版：常见的笛卡尔坐标没办法表示无穷远，所以在n维笛卡尔坐标中添加一维变成n+1维，以二维为例，原来的(X, Y)变成(x,y,w)，其中 X=x/w，Y=y/w，当w=0时即可表示笛卡尔中得无穷远处。引进齐次坐标可把坐标的旋转平移放缩等操作用一个矩阵表示。）</p>
<h2 id="步骤2："><a href="#步骤2：" class="headerlink" title="步骤2："></a>步骤2：</h2><p>3D坐标的平移变换。齐次坐标的平移可如下表示：<br><img src="https://img-blog.csdnimg.cn/20201219195822542.png#pic_center" alt="在这里插入图片描述"><br>左边矩阵为平移后的齐次坐标，vx, vy, vz为平移分量。\<br>代码如下：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def gettranslation(v):</span><br><span class="line">  &quot;&quot;&quot; Returns translation matrix T in homogeneous coordinates for translation by v.</span><br><span class="line"></span><br><span class="line">  Args:</span><br><span class="line">    v: 3d translation vector</span><br><span class="line"></span><br><span class="line">  Returns:</span><br><span class="line">    T: translation matrix in homogeneous coordinates</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  # [x,y,z] --&gt; [x,y,z,1]</span><br><span class="line">  T = np.array([[1,0,0,v[0]],[0,1,0,v[1]],[0,0,1,v[2]],[0,0,0,1]])</span><br><span class="line"></span><br><span class="line">  return T</span><br></pre></td></tr></table></figure></p>
<h2 id="步骤3："><a href="#步骤3：" class="headerlink" title="步骤3："></a>步骤3：</h2><p>获得x, y, z 的旋转矩阵，三个轴的旋转矩阵都在代码的注释里给出了，代码如下：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def getxrotation(d):</span><br><span class="line">  &quot;&quot;&quot; Returns rotation matrix Rx in homogeneous coordinates for a rotation of d degrees around the x axis.</span><br><span class="line"></span><br><span class="line">  Args:</span><br><span class="line">    d: degrees of the rotation</span><br><span class="line"></span><br><span class="line">  Returns:</span><br><span class="line">    Rx: rotation matrix</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  # R_x: [1      0      0   ]</span><br><span class="line">  #      [0   cos(d)  sin(d)]</span><br><span class="line">  #      [0  -sin(d)  cos(d)]</span><br><span class="line">  n_x = np.array([[0,0,0],[0,0,-1],[0,1,0]])</span><br><span class="line">  R_x = np.eye(3) + np.sin(d*np.pi/180)*n_x + (1 - np.cos(d*np.pi/180))*n_x.dot(n_x)</span><br><span class="line">  Rx = np.c_[R_x, [0,0,0]]</span><br><span class="line">  # matrix in homogeneous coordinates</span><br><span class="line">  Rx = np.r_[Rx, [[0,0,0,1]]]</span><br><span class="line">  return Rx</span><br></pre></td></tr></table></figure><br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def getyrotation(d):</span><br><span class="line">  &quot;&quot;&quot; Returns rotation matrix Ry in homogeneous coordinates for a rotation of d degrees around the y axis.</span><br><span class="line"></span><br><span class="line">  Args:</span><br><span class="line">    d: degrees of the rotation</span><br><span class="line"></span><br><span class="line">  Returns:</span><br><span class="line">    Ry: rotation matrix</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  # R_y: [ cos(d)  0  sin(d)]</span><br><span class="line">  #      [   0     1     0  ]</span><br><span class="line">  #      [-sin(d)  0  cos(d)]</span><br><span class="line">  n_y = np.array([[0,0,1],[0,0,0],[-1,0,0]])</span><br><span class="line">  R_y = np.eye(3) + np.sin(d*np.pi/180)*n_y + (1 - np.cos(d*np.pi/180))*n_y.dot(n_y)</span><br><span class="line">  Ry = np.c_[R_y, [0,0,0]]</span><br><span class="line">  # matrix in homogeneous coordinates</span><br><span class="line">  Ry = np.r_[Ry, [[0,0,0,1]]]</span><br><span class="line">  return Ry</span><br></pre></td></tr></table></figure><br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def getzrotation(d):</span><br><span class="line">  &quot;&quot;&quot; Returns rotation matrix Rz in homogeneous coordinates for a rotation of d degrees around the z axis.</span><br><span class="line"></span><br><span class="line">  Args:</span><br><span class="line">    d: degrees of the rotation</span><br><span class="line"></span><br><span class="line">  Returns:</span><br><span class="line">    Rz: rotation matrix</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  # R_z: [cos(d)  -sin(d)  0]</span><br><span class="line">  #      [sin(d)   cos(d)  0]</span><br><span class="line">  #      [  0       0      1]</span><br><span class="line">  n_z = np.array([[0,-1,0],[1,0,0],[0,0,0]])</span><br><span class="line">  R_z = np.eye(3) + np.sin(d*np.pi/180)*n_z + (1 - np.cos(d*np.pi/180))*n_z.dot(n_z)</span><br><span class="line">  Rz = np.c_[R_z, [0,0,0]]</span><br><span class="line">  # matrix in homogeneous coordinates</span><br><span class="line">  Rz = np.r_[Rz, [[0,0,0,1]]]</span><br><span class="line">  return Rz</span><br></pre></td></tr></table></figure><br>这里再简单说一下绕其它轴旋转的情况：\<br>绕其它轴旋转的话可以用轴角公式（Rodrigues’ rotation），通过给定旋转轴和角度可计算出旋转后的向量。这里依旧给个<a href="https://www.bilibili.com/video/BV1h7411c7zK?p=1">链接</a>，B站的一个视频。</p>
<h2 id="步骤4："><a href="#步骤4：" class="headerlink" title="步骤4："></a>步骤4：</h2><p>然后中心投影到ｘｙ平面上，主点为[8，10]，焦距为8，像素为正方形。 我们最终获得了图像平面中的２D点。\<br>代码如下：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def getcentralprojection(principal, focal):</span><br><span class="line">  &quot;&quot;&quot; Returns the (3 x 4) matrix L that projects homogeneous camera coordinates on homogeneous</span><br><span class="line">  image coordinates depending on the principal point and focal length.</span><br><span class="line">  </span><br><span class="line">  Args:</span><br><span class="line">    principal: the principal point, 2d vector</span><br><span class="line">    focal: focal length</span><br><span class="line"></span><br><span class="line">  Returns:</span><br><span class="line">    L: central projection matrix</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  # central: [f_x  0  u_0  0]</span><br><span class="line">  #          [ 0  f_y v_0  0]</span><br><span class="line">  #          [ 0   0   1   0]</span><br><span class="line">  square_L = np.array([[focal,0,principal[0]],[0,focal,principal[1]],[0,0,1]])</span><br><span class="line">  I = np.eye(3)</span><br><span class="line">  I_0 = np.c_[I, [0,0,0]]   #[I|0]</span><br><span class="line">  L = square_L.dot(I_0)</span><br><span class="line">  return L</span><br></pre></td></tr></table></figure><br>简单说明下：\<br>f_x和f_y是焦距，在这里都是8，u_0和v_0是中心点坐标，中心点坐标一般是取图像中点，比如，如果图像宽和高分别是W和H的话，中心点为（W/2，H/2）.</p>
<h2 id="步骤5："><a href="#步骤5：" class="headerlink" title="步骤5："></a>步骤5：</h2><p>将上面的平移，旋转，中心投影矩阵写在一起，如下：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def getfullprojection(T, Rx, Ry, Rz, L):</span><br><span class="line">  &quot;&quot;&quot; Returns full projection matrix P and full extrinsic transformation matrix M.</span><br><span class="line"></span><br><span class="line">  Args:</span><br><span class="line">    T: translation matrix</span><br><span class="line">    Rx: rotation matrix for rotation around the x-axis</span><br><span class="line">    Ry: rotation matrix for rotation around the y-axis</span><br><span class="line">    Rz: rotation matrix for rotation around the z-axis</span><br><span class="line">    L: central projection matrix</span><br><span class="line"></span><br><span class="line">  Returns:</span><br><span class="line">    P: projection matrix</span><br><span class="line">    M: matrix that summarizes extrinsic transformations</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  # The order is: y-axis first, then x-axis, and finally z-axis</span><br><span class="line">  # R = R_xR_yR_z</span><br><span class="line">  # extrinsic transformations:</span><br><span class="line">  # M = [R T]</span><br><span class="line">  #     [0 1]</span><br><span class="line">  # matrix in homogeneous coordinates:</span><br><span class="line">  M = Rz.dot(Rx.dot(Ry.dot(T)))			  # [4,4]</span><br><span class="line">  # P = L.M</span><br><span class="line">  P = L.dot(M)            # [3,4]</span><br><span class="line">  return P, M</span><br></pre></td></tr></table></figure><br><strong>这里要注意计算矩阵M的顺序！！！！！！！</strong><br>这里贴一张图方便理解：<br><img src="https://img-blog.csdnimg.cn/20201223221703759.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>x^I^ 是图像坐标，x^C^ 是摄像机坐标，x^W^是世界3D坐标，K·[I | 0]是代码里的投影矩阵L。</p>
<h2 id="步骤6："><a href="#步骤6：" class="headerlink" title="步骤6："></a>步骤6：</h2><p>借助上面得出的转换矩阵将世界３D坐标转化为图像２D坐标，代码如下：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def projectpoints(P, X):</span><br><span class="line">  &quot;&quot;&quot; Apply full projection matrix P to 3D points X in cartesian coordinates.</span><br><span class="line"></span><br><span class="line">  Args:</span><br><span class="line">    P: projection matrix</span><br><span class="line">    X: 3d points in cartesian coordinates</span><br><span class="line"></span><br><span class="line">  Returns:</span><br><span class="line">    x: 2d points in cartesian coordinates</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  X = cart2hom(X)</span><br><span class="line">  x = P.dot(X)        #[3,2904] &amp; [x,y,1]</span><br><span class="line">  x = hom2cart(x)     #[2,2904] &amp; [x,y]</span><br><span class="line">  return x</span><br></pre></td></tr></table></figure></p>
<h2 id="步骤7："><a href="#步骤7：" class="headerlink" title="步骤7："></a>步骤7：</h2><p>一些常规的加载操作。\<br>加载2D点：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">def loadpoints():</span><br><span class="line">  &quot;&quot;&quot; Load 2D points from obj2d.npy.</span><br><span class="line"></span><br><span class="line">  Returns:</span><br><span class="line">    x: np array of points loaded from obj2d.npy</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  x = np.load(&#x27;data/obj2d.npy&#x27;)       #[2,2904]&amp;[x,y]</span><br><span class="line">  return x</span><br></pre></td></tr></table></figure><br>加载z坐标点：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def loadz():</span><br><span class="line">  &quot;&quot;&quot; Load z-coordinates from zs.npy.</span><br><span class="line"></span><br><span class="line">  Returns:</span><br><span class="line">    z: np array containing the z-coordinates</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">  z = np.load(&#x27;data/zs.npy&#x27;)         #[1,2904]&amp;[z]</span><br><span class="line">  return z</span><br></pre></td></tr></table></figure></p>
<h2 id="步骤8："><a href="#步骤8：" class="headerlink" title="步骤8："></a>步骤8：</h2><p>通过投影矩阵以及z方向的坐标，将上面的2D点显示为3D点，代码如下：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def invertprojection(L, P2d, z):</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  Invert just the projection L of cartesian image coordinates P2d with z-coordinates z.</span><br><span class="line"></span><br><span class="line">  Args:</span><br><span class="line">    L: central projection matrix</span><br><span class="line">    P2d: 2d image coordinates of the projected points</span><br><span class="line">    z: z-components of the homogeneous image coordinates</span><br><span class="line"></span><br><span class="line">  Returns:</span><br><span class="line">    P3d: 3d cartesian camera coordinates of the points</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  # square_L = np.delete(L, 3, axis=1)   #(3,3)</span><br><span class="line">  # L^-1</span><br><span class="line">  pinv_L = np.linalg.pinv(L)</span><br><span class="line">  # [x,y](2,2904) --&gt; [x,y,1](3,2904)</span><br><span class="line">  hom_point_xy = cart2hom(P2d)  </span><br><span class="line">  for i in range(P2d.shape[1]):</span><br><span class="line">    hom_point_xy[:,i] = hom_point_xy[:,i]*z[:,i]</span><br><span class="line">  hom_point_xy = pinv_L.dot(hom_point_xy) </span><br><span class="line"></span><br><span class="line">  # [x,y,z]</span><br><span class="line">  P3d = np.delete(hom_point_xy, 3, axis = 0)        </span><br><span class="line">  return P3d</span><br></pre></td></tr></table></figure><br>效果如下：<br><img src="https://img-blog.csdnimg.cn/20201224033516517.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>而原来2D点的图则如下：<br><img src="https://img-blog.csdnimg.cn/2020122403354922.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h2 id="步骤9："><a href="#步骤9：" class="headerlink" title="步骤9："></a>步骤9：</h2><p>获得外部转化后的3D点的笛卡尔坐标，以便用步骤6的代码获得3D点对应的２D点（步骤写得有点乱，不过不改了。。。。），代码如下：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def inverttransformation(M, P3d):</span><br><span class="line">  &quot;&quot;&quot; Invert just the model transformation in homogeneous coordinates</span><br><span class="line">  for the 3D points P3d in cartesian coordinates.</span><br><span class="line"></span><br><span class="line">  Args:</span><br><span class="line">    M: matrix summarizing the extrinsic transformations</span><br><span class="line">    P3d: 3d points in cartesian coordinates</span><br><span class="line"></span><br><span class="line">  Returns:</span><br><span class="line">    X: 3d points after the extrinsic transformations have been reverted</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  hom_P3d = cart2hom(P3d)      #(4,2904)(x,y,z,1)</span><br><span class="line">  inv_M = np.linalg.inv(M)    #Inverse matrix of M</span><br><span class="line">  P3d = inv_M.dot(hom_P3d)                   </span><br><span class="line">  </span><br><span class="line">  return P3d       #(4,2904)</span><br></pre></td></tr></table></figure></p>
<h2 id="步骤10："><a href="#步骤10：" class="headerlink" title="步骤10："></a>步骤10：</h2><p>对以上函数的引用：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def problem3():</span><br><span class="line">    &quot;&quot;&quot;Example code implementing the steps in Problem 3&quot;&quot;&quot;</span><br><span class="line">    t = np.array([-27.1, -2.9, -3.2])</span><br><span class="line">    principal_point = np.array([8, -10])</span><br><span class="line">    focal_length = 8</span><br><span class="line"></span><br><span class="line">    # model transformations</span><br><span class="line">    T = gettranslation(t)</span><br><span class="line">    Ry = getyrotation(135)</span><br><span class="line">    Rx = getxrotation(-30)</span><br><span class="line">    Rz = getzrotation(90)</span><br><span class="line">    print(T)</span><br><span class="line">    print(Ry)</span><br><span class="line">    print(Rx)</span><br><span class="line">    print(Rz)</span><br><span class="line"></span><br><span class="line">    K = getcentralprojection(principal_point, focal_length)</span><br><span class="line"></span><br><span class="line">    P,M = getfullprojection(T, Rx, Ry, Rz, K)</span><br><span class="line">    print(P)</span><br><span class="line">    print(M)</span><br><span class="line"></span><br><span class="line">    points = loadpoints()</span><br><span class="line">    #displaypoints2d(points)</span><br><span class="line"></span><br><span class="line">    z = loadz()</span><br><span class="line">    Xt = invertprojection(K, points, z)</span><br><span class="line"></span><br><span class="line">    Xh = inverttransformation(M, Xt)</span><br><span class="line"></span><br><span class="line">    worldpoints = hom2cart(Xh)</span><br><span class="line">    displaypoints3d(worldpoints)</span><br><span class="line"></span><br><span class="line">    points2 = projectpoints(P, worldpoints)</span><br><span class="line">    displaypoints2d(points2)</span><br><span class="line"></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></p>
<h1 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h1><p>最大收获是弄懂了齐次坐标，然后就是一些杂七杂八的变换矩阵</p>
<h1 id="剩下待完善的："><a href="#剩下待完善的：" class="headerlink" title="剩下待完善的："></a>剩下待完善的：</h1><ol>
<li>关于摄像机矩阵，包括镜头畸变，本科毕设的时候有涉及到，但现在忘得差不多了，纠结下有没有必要单独总结一篇。。。</li>
</ol>
]]></content>
      <categories>
        <category>学习总结</category>
      </categories>
  </entry>
  <entry>
    <title>李群和李代数</title>
    <url>/posts/4.html</url>
    <content><![CDATA[<h2 id="1-群的定义"><a href="#1-群的定义" class="headerlink" title="1. 群的定义"></a>1. 群的定义</h2><p>群(Group)是<strong>一种集合</strong>加上<strong>一种运算</strong>的代数结构，以A表示集合，“·”表示运算，则群一般写作 G(A, ·)。群要求满足以下四个条件：<br><span id="more"></span><br><img src="https://img-blog.csdnimg.cn/20210429051226112.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>举个例子，旋转矩阵和乘法可以构成旋转矩阵群，因为其满足：</p>
<pre><code>1). 旋转矩阵相乘后仍是旋转矩阵；
2). 矩阵乘法满足结合律；
3). 幺元为单位矩阵，也属旋转矩阵；
4). 旋转矩阵的逆也为旋转矩阵，且相乘为单位矩阵。
</code></pre><p>常见的群有：<br><img src="https://img-blog.csdnimg.cn/20210501063426658.png#pic_center" alt="在这里插入图片描述"></p>
<h2 id="2-李群"><a href="#2-李群" class="headerlink" title="2. 李群"></a>2. 李群</h2><p>李群是指拥有(连续)光滑性质的群，比如上面提到的旋转矩阵群，因其能在空间中连续旋转，变换矩阵群也是李群（就是在旋转过程中加上平移）。</p>
<h2 id="3-李代数的引入"><a href="#3-李代数的引入" class="headerlink" title="3. 李代数的引入"></a>3. 李代数的引入</h2><h3 id="直观的导出"><a href="#直观的导出" class="headerlink" title="直观的导出"></a>直观的导出</h3><p>在视觉SLAM中，我们需要做的就是不断计算相机的位姿和构建地图（其中的相机位姿表现在其变换矩阵T），但由于干扰的存在，我们无法准确获得所需要的信息，所以我们转而求其最小误差。\<br>现假设我们有N个三维点p和对应的观测值z，那么我们的目标就是寻找一个最佳的位姿T，使得整体误差最小化，也就是求：<br><img src="https://img-blog.csdnimg.cn/20210501044418122.png#pic_center" alt="在这里插入图片描述"><br>要求解上面的方程，即是要求目标函数J对T的导数，但由于T所在的变换矩阵群（下面用SO(3)空间表示）对加法不封闭，无法直接求取，所以我们需要引入一个新的量，通过对该量的计算间接获得对变换矩阵T的求导，这个引入的量就是李代数。</p>
<h3 id="数学层面的导出"><a href="#数学层面的导出" class="headerlink" title="数学层面的导出"></a>数学层面的导出</h3><p><strong>（之后的部分《视觉SLAM 十四讲》里写得已经很精炼很清楚了，这里大概重述一遍以加深印象，也方便以后的查询）</strong></p>
<p>先给一个结论：<strong>李代数对应李群的正切空间，描述了李群的局部导数。</strong></p>
<p>这里用旋转矩阵群SO(3)为例：\<br>我们知道旋转矩阵满足以下关系：<script type="math/tex">RR^{T}=I</script><br>然后引入时间变量$t$变为时间的函数$R(t)$，即是有：<script type="math/tex">R(t)R(t)^{T}=I</script><br>对两边求导得到：<script type="math/tex">\dot{R}(t)R(t)^{T}+\dot{R}(t)^{T}R(t)=0</script><br>整理得：<script type="math/tex">\dot{R}(t)R(t)^{T}=-(\dot{R}(t)R(t)^{T})^{T}</script><br>可见，$\dot{R}(t)R(t)^{T}$是一个反对称矩阵。用符号$^{\land}$表示反对称矩阵，设：<script type="math/tex">a=\begin{bmatrix} a_{1} \\ a_{2} \\ a_{3} \end{bmatrix}</script><br>则$a^{\land}$为：<script type="math/tex">a^{\land}=A=\begin{bmatrix} 0 & -a_{3} & a_{2} \\ a_{3} & 0 & -a_{1} \\ -a_{2} & a_{1} & 0 \end{bmatrix}</script><br>同理：<script type="math/tex">A^{\vee}=a</script><br>我们设一个三维向量$\phi(t)\in\Re^{3}$与反对称矩阵$\dot{R}(t)R(t)^{T}$对应，即：<script type="math/tex">\dot{R}(t)R(t)^{T}=\phi(t)^{\land}</script><br>等式两边右乘$R(t)$，由于$R$为正交阵，得：<script type="math/tex">\dot{R}(t)=\phi(t)^{\land}R(t)=\begin{bmatrix}0 & -\phi_{3} & \phi_{2}\\\phi_{3}&0&-\phi_{1}\\-\phi_{2}&\phi_{1}&0\end{bmatrix}R(t)</script><br>由此可见，每对$R$求一次导数，只需左乘一个$\phi^{\land}(t)$即可。\<br>当$t=0$时，设此时$R(0)=I$，把$R(t)$在$t=0$附近进行一阶泰勒展开得：<script type="math/tex">R(t)\approx R(t_{0})+\dot{R}(t_{0})(t-t_{0})=I+\phi(t_{0})^{\land}(t)</script><br>可见，$\phi$反映了$R$的导数性质，所以称它为SO(3)原点附近的正切空间。同时在$t_{0}$附近设$\phi$保持为常数$\phi(t_{0})=\phi_{0}$，故有：<script type="math/tex">\dot{R}(t)=\phi(t_{0})^{\land}R(t)=\phi_{0}^{\land}R(t)</script><br>这是一个关于$R$的微分方程，且初始值$R(0)=I$，解得：<script type="math/tex">R(t)=exp(\phi_{0}^{\land}t)</script><br>表明，旋转矩阵可由$exp(\phi_{0}^{\land}t)$计算出来，给定某时刻的$R$我们就能求得一个$\phi$，它描述$R$在局部的导数关系，<strong>这个$\phi$正是对应到SO(3)上的李代数so(3)</strong>。</p>
<h2 id="4-李代数的定义"><a href="#4-李代数的定义" class="headerlink" title="4. 李代数的定义"></a>4. 李代数的定义</h2><p>每个李群都有其对应的李代数，一般李代数的定义如下：<br><img src="https://img-blog.csdnimg.cn/20210501060351439.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>二元运算[,]称为李括号，表达两元素的差异。举个例子，三维向量$\Re^{3}$的叉积就是一种李括号，因此$g=(\Re^{3},\Re,\times)$构成了一个李代数。</p>
<h2 id="5-李代数so-3"><a href="#5-李代数so-3" class="headerlink" title="5. 李代数so(3)"></a>5. 李代数so(3)</h2><p>由上面的结果可得，三维向量$\phi$为李群SO(3)所对应的，定义在$\Re^{3}$上的李代数，每个$\phi$对应一个反对称矩阵：<script type="math/tex">\Phi=\phi(t)^{\land}R(t)=\begin{bmatrix}0 & -\phi_{3} & \phi_{2}\\\phi_{3}&0&-\phi_{1}\\-\phi_{2}&\phi_{1}&0\end{bmatrix}\in \Re^{3\times3}</script><br>两个向量$\phi_{1},\phi_{2}$的李括号为：<script type="math/tex">[\phi_{1},\phi_{2}]=(\Phi_{1}\Phi_{2}-\Phi_{2}\Phi_{1})^{\vee}</script><br>它与SO(3)的关系由指数映射指定：<script type="math/tex">R=exp(\phi^{\land})</script></p>
<h2 id="6-李代数se-3"><a href="#6-李代数se-3" class="headerlink" title="6. 李代数se(3)"></a>6. 李代数se(3)</h2><p>SE(3)对应的李代数se(3)与so(3)类似，但其定义在$\Re^{6}$空间中，如下：<br><img src="https://img-blog.csdnimg.cn/20210501232727577.png#pic_center" alt="在这里插入图片描述"><br>我们把每个se(3)的元素记作$\xi$，表示一个六维向量，前三维表平移，但不同于变换矩阵的平移，记作$\rho$，后三维表旋转，记作$\phi$，其实就是上面提到的so(3)元素。se(3)表示如下（<strong>注：这里的$\land$不再表示反对称矩阵，而是表示六维向量向四维矩阵的转换</strong>）：</p>
<script type="math/tex; mode=display">\xi^{\land}=\begin{bmatrix}\phi^{\land}&\rho\\0^{T}&0\end{bmatrix}</script><p>(注：$\land$和$\vee$这里表示“从向量到矩阵”和“从矩阵到向量”，se(3)可理解成：由平移加上一个so(3)元素构成的向量)\<br>其李括号为：<script type="math/tex">[\xi_{1},\xi_{2}]=(\xi^{\land}_{1}\xi^{\land}_{2}-\xi^{\land}_{2}\xi^{\land}_{1})^{\vee}</script></p>
<h2 id="7-指数与对数映射"><a href="#7-指数与对数映射" class="headerlink" title="7. 指数与对数映射"></a>7. 指数与对数映射</h2><p>矩阵的指数如$exp(\phi^{\land})$在李群李代数中称为指数映射。</p>
<h3 id="关于so-3-的映射"><a href="#关于so-3-的映射" class="headerlink" title="关于so(3)的映射"></a>关于so(3)的映射</h3><p>那么如何计算$exp(\phi^{\land})$呢？首先，任何矩阵$A$的指数都可以表示成一个泰勒展开，但只有收敛的情况下才会有结果，结果如下：<script type="math/tex">exp(A)=\sum_{n=0}^{\infty}\frac{1}{n!}A^{n}</script><br>即是对so(3)的元素有：<script type="math/tex">exp(\phi^{\land})=\sum_{n=0}^{\infty}\frac{1}{n!}(\phi^{\land})^{n}</script><br>但要计算这个太复杂，因为有无穷次幂。所以一般情况下我们用下面的方法：$\phi$是一个三维向量，我们定义其模长和方向，分别记为$\theta$和$a$，写作$\phi=\theta a$，这里$a$的模长为1.即: $||a||=1$:</p>
<script type="math/tex; mode=display">a = [a_{1}, a_{2}, a_{3}]^{T}$$ $$a^{2}_{1}+a^{2}_{2}+a^{2}_{3}=1</script><p>对于$a^{\land}$有以下性质：<br><img src="https://img-blog.csdnimg.cn/20210502002859946.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>图里的两个式子提供了处理高阶$a^{\land}$的方法，所以其指数映射可写成一个神奇的形式：（<em>最好自己写一遍</em>）<br><img src="https://img-blog.csdnimg.cn/20210503231438538.png#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20210503231603885.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>这个式子与罗德里格斯公式如出一辙。</p>
<p>这里简单回顾下罗德里格斯公式（旋转向量到旋转矩阵的转化）：</p>
<script type="math/tex; mode=display">R=cos\theta I+(1-cos\theta)nn^{T}+sin\theta n^{\land}$$ 两边取迹（即是求矩阵对角线元素之和）：$$tr(R)=cos\theta tr(I)+(1-cos\theta)tr(nn^{T})+sin\theta tr(n^{\land})\\=3cos\theta+(1-cos\theta)=1+2cos\theta</script><p>所以角度为：</p>
<script type="math/tex; mode=display">\theta=arc cos\frac{tr(R)-1}{2}</script><p>这表明，so(3)实际就是由旋转向量组成的空间，而指数映射即是罗德里格斯公式。\<br>反之，通过对数映射也可以将SO(3)的元素对应到so(3)中：<br><img src="https://img-blog.csdnimg.cn/202105032347198.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>对这个式子的求解也没必要用泰勒展开，而是可以通过迹的性质分别求解转角和转轴（上面回顾罗德里格斯公式的时候已经一起列出来了）。\<br>最后需要提一下：每个SO(3)中的元素都可以找到一个so(3)元素与之对应，但一个so(3)元素却可能有多个SO(3)中的元素，直观一点理解的话：比如旋转360度会对应一个周期。</p>
<h3 id="关于se-3-的映射（待补充。。。）"><a href="#关于se-3-的映射（待补充。。。）" class="headerlink" title="关于se(3)的映射（待补充。。。）"></a>关于se(3)的映射（待补充。。。）</h3><p>与so(3)的过程类似，所以这里主要给结论（<strong>这个过程没具体算，之后补上。</strong>）。\<br>se(3)的指数映射形式如下：<br><img src="https://img-blog.csdnimg.cn/20210504000629746.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>同样令$\phi=\theta a$，$a$为单位向量，则：<br><img src="https://img-blog.csdnimg.cn/20210504001031241.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>从结果来看，$\xi$的指数映射左上角的$R$就是上面的SO(3)，与se(3)中的旋转部分$\phi$对应，而右上角的$J$由上面图里的推导给出，即：<br><img src="https://img-blog.csdnimg.cn/20210504001730154.png#pic_center" alt="在这里插入图片描述"></p>
<h3 id="映射表"><a href="#映射表" class="headerlink" title="映射表*"></a>映射表*</h3><p>这里给出SO(3), SE(3), so(3), se(3)的对应映射表<br><img src="https://img-blog.csdnimg.cn/20210507171027871.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p><strong><em>本文主要参考《视觉SLAM十四讲》</em></strong></p>
]]></content>
      <categories>
        <category>视觉SLAM相关</category>
      </categories>
  </entry>
  <entry>
    <title>李代数和扰动模型</title>
    <url>/posts/5.html</url>
    <content><![CDATA[<p>前一篇大概介绍了【李群李代数】的相关性质，这里主要介绍李代数在SLAM中的作用。<br><span id="more"></span></p>
<h2 id="BCH公式及其近似形式"><a href="#BCH公式及其近似形式" class="headerlink" title="BCH公式及其近似形式"></a>BCH公式及其近似形式</h2><p>李代数在SLAM中的作用主要是进行优化，优化过程就不可避免地需要涉及到求导的过程。\<br>与标量不同，在矩阵中指数的运算法则并不适用，即是说在矩阵中：<script type="math/tex">ln(exp(A)exp(B))\ne A+B</script><br>取而代之的时BCH公式：<script type="math/tex">ln(exp(A)exp(B))=A+B+\frac{1}{2}[A,B]+\frac{1}{12}[A,[A,B]]-\frac{1}{12}[B,[A,B]]+...</script><br>其中的”[,]”为李括号，也就是说，两个矩阵指数之积会由这两个矩阵的和与一系列由李括号组成的余项组成。对于SO(3)，当$\phi_{1}$或$\phi_{2}$为小量时，小量二次以上的项都可以被忽略，即是有以下关系：<br><img src="https://img-blog.csdnimg.cn/2021050722481379.png#pic_center" alt="在这里插入图片描述"><br>以第一个近似为例，当对一个旋转向量$R_{2}$（对应的李代数为$\phi_{2}$）左乘一个微小旋转矩阵$R_{1}$，可以近似看作在原有李代数$\phi_{2}$的基础上加上了$J_{l}(\phi_{2})^{-1}\phi_{1}$（第二个近似同理，但是是右乘）。\<br>李代数在BCH近似下分为左乘和右乘两种，注意区分。\<br>这里的$J$就是在上一篇关于李群李代数里提到的式子：<br><img src="https://img-blog.csdnimg.cn/20210507230638737.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>右乘的话只需要改变自变量的符号即可：<script type="math/tex">J_{r}(\phi)=J_{l}(-\phi)</script><br>现在对于一个旋转$R$(对应李代数为$\phi$)，给之左乘一个微小旋转$\triangle R$(对应于$\triangle \phi$)，我们就有了如下关系：<br><img src="https://img-blog.csdnimg.cn/20210507231646273.png#pic_center" alt="在这里插入图片描述"><br>反之，当我们在李代数上做加法运算时则有：<br><img src="https://img-blog.csdnimg.cn/20210507231729943.png#pic_center" alt="在这里插入图片描述"><br>对于SE(3)，也有类似的关系：<br><img src="https://img-blog.csdnimg.cn/20210507231854489.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>这里的$\jmath$是一个$6\times6$的矩阵。</p>
<h2 id="SO-3-上的李代数的求导"><a href="#SO-3-上的李代数的求导" class="headerlink" title="SO(3)上的李代数的求导"></a>SO(3)上的李代数的求导</h2><p>在SLAM中要估计一个相机的位置和位姿，我们是通过SO(3)上的旋转矩阵或者SE(3)上的变换矩阵描述的。举个例子，在某时刻相机位姿为$T$，其观察到世界坐标位于$p$的一点，产生一个观测数据$z$，那么我们就有如下关系：<script type="math/tex">z=Tp+\omega</script>其中的$\omega$为随机噪音。\<br>由于噪音的存在我们无法获得精准的$z-p$对应关系，所以现在我们就要对这个噪音产生的误差进行计算，即：<script type="math/tex">e=z-Tp</script><br>假设一共有$N$个这样的路标点和观察，即是有$N$个上式，我们现在要预计该相机的位姿，就是要求出一组$T$，使整体误差最小化：<script type="math/tex">\min_T J(T)=\sum_{i=1}^{N}||z_{i}-Tp_{i}||_{2}^{2}</script><br>要求解这个问题，就要求$J$对$T$的导数，然而因为群SO(3)和SE(3)没有加法，所以我们一般转而处理它们的李代数（因为李代数是向量的集合，对加法封闭），因此思路一般分为两种：</p>
<blockquote>
<ol>
<li>用李代数表示位姿，然后对李代数进行求导；\</li>
<li>对李群左乘或者右乘微小扰动，然后对这个扰动进行求导，称为左扰动和右扰动模型</li>
</ol>
</blockquote>
<p>第一种对应李代数求导模型，第二种对应扰动模型。\<br>以下分别进行说明。</p>
<h3 id="李代数求导"><a href="#李代数求导" class="headerlink" title="李代数求导"></a>李代数求导</h3><p><em>（其实这部分可不看，因为最后会发现扰动模型更适合）</em>\<br>现在处理SO(3)的情况。假设对空间一点$p$进行旋转，得到$Rp$，我们要计算旋转后的点坐标相对于旋转的角度，即是：<script type="math/tex">\frac{\partial(Rp)}{\partial R}</script><br>由于SO(3)没有加法，所以我们转而求其对应的李代数，即是：<script type="math/tex">\frac{\partial(exp(\phi^{\land})p)}{\partial \phi}</script><br>根据导数的定义计算：<br><img src="https://img-blog.csdnimg.cn/20210508033658749.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>第二行的近似为BCH近似，第三行为在0附近的泰勒展开舍去高阶项后的近似，第四到第五行将反对称符号看作叉积，交换后变号。于是我们可以得出以下式子：<br><img src="https://img-blog.csdnimg.cn/20210508033958370.png#pic_center" alt="在这里插入图片描述"><br>这里简单解释下反对称矩阵和叉积的关系，也就是上面第四到第五行的变换：</p>
<script type="math/tex; mode=display">a=[a_{1},a_{2},a_{3}]$$ $$b=[b_{1},b_{2},b_{3}]</script><script type="math/tex; mode=display">a\times b=\begin{bmatrix} i& j& k\\ a_{1}&a_{2}& a_{3}\\ b_{1}& b_{2}& b_{3}\end{bmatrix} \\= \begin{bmatrix} a_{2}b_{3}-a_{3}b_{2} \\ -(a_{1}b_{3}-a_{3}b_{1}) \\ a_{1}b_{2}-a_{2}b_{1}\end{bmatrix} \\ =\begin{bmatrix} 0 & -a_{3} & a_{2} \\ a_{3} & 0 & -a_{1} \\ -a_{2} & a_{1} & 0 \end{bmatrix} \begin{bmatrix} b_{1} \\ b_{2} \\ b_{3} \end{bmatrix}</script><p>所以，$a$ 与$b$的叉乘即是等于$a$的反对称矩阵乘以$b$，同时叉乘满足如下反交换律：<script type="math/tex">a\times b=-b\times a</script><br>回到原题，在上图中，由于结果式子含有形式比较复杂的$J_{l}$，不容易计算，所以我们一般不采用上面的方法，而用下面的扰动模型的方法。</p>
<h3 id="扰动模型（左乘）"><a href="#扰动模型（左乘）" class="headerlink" title="扰动模型（左乘）"></a>扰动模型（左乘）</h3><p>扰动模型的求导方法是对$R$进行一次扰动$\triangle R$，这个微小扰动可以左乘，也可以右乘，最后结果会有一点微小差异，这里以左乘为例。设左扰动$\triangle R$对应的李代数为$\varphi$，然后对$\varphi$进行求导：<br><img src="https://img-blog.csdnimg.cn/20210508041300661.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>相比直接对李代数求导，这里省去了一个雅可比$J_{l}$的计算，更为简便。</p>
<h2 id="SE-3-上的李代数求导"><a href="#SE-3-上的李代数求导" class="headerlink" title="SE(3)上的李代数求导"></a>SE(3)上的李代数求导</h2><p>假设空间一点$p$经过一次变换$T$(对应的李代数为$\xi$)，得到$Tp$(这里的$p$注意用齐次坐标)。现给$T$左乘一个扰动$\triangle T=exp(\delta \xi^{\land})$，设扰动项的李代数为$\delta \xi=[\delta \rho,\delta \phi]^{T}$，可得：<br><img src="https://img-blog.csdnimg.cn/20210508042117565.png#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20210508042125983.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>把最后的结果定义成一个算符$\odot$，它把一个齐次空间坐标变换成一个$4\times 6$的矩阵。\<br>最后这里简单解释下矩阵求导的顺序，设$a,b,x,y$都是列向量，其求导顺序为：<br><img src="https://img-blog.csdnimg.cn/2021050804251624.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><strong><em>注：本文主要参考自《视觉SLAM十四讲》</em></strong></p>
]]></content>
      <categories>
        <category>视觉SLAM相关</category>
      </categories>
  </entry>
  <entry>
    <title>概率密度估计（Probability Density Estimation）--Part 1：参数估计</title>
    <url>/posts/6.html</url>
    <content><![CDATA[<h1 id="概率密度的引入"><a href="#概率密度的引入" class="headerlink" title="概率密度的引入"></a>概率密度的引入</h1><span id="more"></span>
<p>当我们有如下的点分布<br><img src="https://img-blog.csdnimg.cn/20210623072951840.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_2,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>为了能区分它们，我们需要知道这些点的概率分布。常见的有贝叶斯最优分类(Bayes optimal classification)，这是基于如下的概率分布：<script type="math/tex">p(x|C_k)p(C_k)</script><br>其中的先验$p(C_k)$很容易算，就是统计，或者说数数，现在的问题就成了计算如下的条件概率密度：$p(x|C_k)$。这主要有三种情况：</p>
<pre><code>1. Parametric model
2. Non-parametric model
3. Mixture models
</code></pre><p>（简单解释一下，有参估计是指我们知道样本数据符合某种概率密度模型，通过给出的数据求出所需要的参数，比如高斯分布的均值和方差，有参估计的Robust比较好；无参估计是指我们不知道这些数据点符合哪些模型，所以无法求出其参数，这种情况在现实生活中更加常见）</p>
<h1 id="参数模型（Parametric-model）"><a href="#参数模型（Parametric-model）" class="headerlink" title="参数模型（Parametric model）"></a>参数模型（Parametric model）</h1><p>概率密度模型的符号表示为：<script type="math/tex">x \sim p(x|\theta)</script><br>以高斯模型为例：<script type="math/tex">p(x|\mu,\sigma)=\frac{1}{\sqrt{2\pi \sigma^2}}\exp \left\{ -\frac{(x-\mu)^2}{2\sigma^2}\right\}</script><br>我们只要知道它的均值和方差就能完整地描述这个模型，即是：<script type="math/tex">\theta = (\mu, \sigma)$$$$x\sim p(x|\mu,\sigma)</script>现在我们需要根据模型和数据集求出$\theta$。\<br>其似然函数表示为：($X$为数据集数据)</p>
<p><script type="math/tex">L(\theta)=p(X|\theta)</script>对于有参模型，这里介绍最大似然方法（Maximum Likelihood Method）</p>
<h2 id="最大似然方法（Maximum-Likelihood-Method）"><a href="#最大似然方法（Maximum-Likelihood-Method）" class="headerlink" title="最大似然方法（Maximum Likelihood Method）"></a>最大似然方法（Maximum Likelihood Method）</h2><p>现假设我们有数据集$X=\left\{x_1,x_2,…,x_N\right\}$\<br>对于这些数据集我们假设数据是i.i.d(independent and identically<br>distributed)的，即是：</p>
<blockquote>
<ol>
<li>$P(x_1\le\alpha, x_2\le\beta)=P(x_1\le\alpha)P(x_2\le\beta)$\</li>
<li>$P(x_1\le\alpha)=P(x_2\le\alpha)$<br>通过该假设，似然计算可写为：<br><img src="https://img-blog.csdnimg.cn/20210624014545305.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>两边取对数为：<br><img src="https://img-blog.csdnimg.cn/20210624014705555.png#pic_center" alt="在这里插入图片描述"><br>然后就是求导之类的事了。\<br>现在有个问题，如果$N=1$，即是说$X=\left\{x_1\right\}$，这时候产生的高斯模型会如下图所示：<br><img src="https://img-blog.csdnimg.cn/2021062402093825.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>看上去就像个$\delta$函数。\<br>这时候我们可以给平均数加上先验<br><img src="https://img-blog.csdnimg.cn/20210624021134780.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li>
</ol>
</blockquote>
<h2 id="贝叶斯估计（Bayesian-Estimation）"><a href="#贝叶斯估计（Bayesian-Estimation）" class="headerlink" title="贝叶斯估计（Bayesian Estimation）"></a>贝叶斯估计（Bayesian Estimation）</h2><p>与极大似然估计不同，贝叶斯估计假设待估计参数不是固定而是随机的。\<br>现在我们要根据给出的数据集$X$，得到$x$的密度函数$p(x)$。将其写成条件概率形式：<script type="math/tex">p(x|X)</script>用边缘概率和贝叶斯公式得出下图关系：<br><img src="https://img-blog.csdnimg.cn/20210625183231837.png#pic_center" alt="在这里插入图片描述"><br>由于$p(x)$能根据$\theta$完全确定，即是说$\theta$是sufficient statistic，所以<script type="math/tex">p(x|\theta, X) = p(x|\theta)</script>进而上面两个式子可写成：<br><img src="https://img-blog.csdnimg.cn/20210625184001131.png#pic_center" alt="在这里插入图片描述"><br>（简单解释一下这个式子，$p(\theta |X)$表示估计参数对数据集$X$的依赖程度，也就是说$\theta$在一定程度上可以代指$X$，比如当$p(\theta|X)$在大多数地方都很小，但唯独在$\hat{\theta}$处很大，我们就能近似表示：$p(x|X) \approx p(x|\hat{\theta})$，该点也称为Bayes point）\<br>继续计算如下：<br><img src="https://img-blog.csdnimg.cn/20210625184553971.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>这里稍微对图里的参数进行解释：\<br>$p(\theta)$是估计参数的先验分布，它与给的数据集无关，而是我们的一种经验性判断。对于$p(\theta|X)$，是一个后验分布，所以一般取期望值，而且<br><img src="https://img-blog.csdnimg.cn/20210625191701870.png#pic_center" alt="在这里插入图片描述"><br>举个例子：\<br>我们现在有高斯分布的数据集，其方差已知，我们要估计其均值，有如下关系：<br><img src="https://img-blog.csdnimg.cn/20210625191859887.png#pic_center" alt="在这里插入图片描述"><br>对于先验我们的判断是：<br><img src="https://img-blog.csdnimg.cn/20210625191940406.png#pic_center" alt="在这里插入图片描述"><br>根据采样均值和贝叶斯估计就能对均值进行估计：<br><img src="https://img-blog.csdnimg.cn/20210625192158823.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>最后这里补充个概念：</p>
<blockquote>
<p>在贝叶斯统计中，如果后验分布与先验分布属于同类，则先验分布与后验分布被称为共轭分布，而先验分布被称为似然函数的共轭先验（Conjugate prior）。比如，高斯分布家族在高斯似然函数下与其自身共轭 (自共轭)。(来自 wiki百科)</p>
</blockquote>
<p>比如上面那个均值的高斯先验，就是高斯模型的共轭，这里使用是因为：</p>
<blockquote>
<ol>
<li>The product of two Gaussians is a Gaussian.\</li>
<li>The marginal of a Gaussian is a Gaussian.</li>
</ol>
</blockquote>
<h2 id="（附）参数估计的代码作业"><a href="#（附）参数估计的代码作业" class="headerlink" title="（附）参数估计的代码作业"></a>（附）参数估计的代码作业</h2><p><img src="https://img-blog.csdnimg.cn/20210625194745608.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>简单来说就是给两组二维数据，模型是高斯分布，进行参数估计拟合，第一题是纯粹的统计，跳过，第二题的话。。。还是直接附代码了吧（代码正确性未知，因为作业还没有公布结果）<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span>(<span class="params">path</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.loadtxt(path)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prior_probability</span>(<span class="params">arr_n, arr_sum</span>):</span></span><br><span class="line">    h_n, w_n = arr_n.shape</span><br><span class="line">    h_sum, w_sum = arr_sum.shape</span><br><span class="line">    p_n = (h_n * w_n) / (h_sum * w_sum)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> p_n</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">MLE_mean</span>(<span class="params">arr</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.mean(arr, axis = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_unbias_var</span>(<span class="params">arr</span>):</span></span><br><span class="line">    num = arr.shape[<span class="number">0</span>]</span><br><span class="line">    arr_mean = MLE_mean(arr)</span><br><span class="line">    dist = arr - arr_mean</span><br><span class="line">    sum_vor_cov = np.zeros((<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num):</span><br><span class="line">        ls = []</span><br><span class="line">        ls.append(dist[i])</span><br><span class="line">        dist_arr = np.array(ls)</span><br><span class="line">        sum_vor_cov += np.dot(dist_arr.T, dist_arr)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> sum_vor_cov / num, sum_vor_cov / (num - <span class="number">1</span>)</span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">two_dim_Gauss</span>(<span class="params">x, mu, cov</span>):</span></span><br><span class="line">    <span class="comment">#x = x.reshape(2, -1)</span></span><br><span class="line">    p_x = np.exp((-<span class="number">0.5</span>*(x - mu) @ np.linalg.inv(cov) @ (x - mu).T)) \</span><br><span class="line">          / np.sqrt((<span class="number">2</span> * np.pi) ** <span class="number">2</span> * np.linalg.det(cov))</span><br><span class="line">    <span class="comment">#print(p_x.shape)</span></span><br><span class="line">    <span class="keyword">return</span> p_x</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">posterior</span>(<span class="params">mu1, mu2, cov1, cov2, p_c1, p_c2</span>):</span></span><br><span class="line">    num = <span class="number">300</span></span><br><span class="line">    x = np.linspace(-<span class="number">10</span>, <span class="number">10</span>, num)</span><br><span class="line">    y = np.linspace(-<span class="number">10</span>, <span class="number">10</span>, num)</span><br><span class="line"></span><br><span class="line">    X,Y = np.meshgrid(x, y)</span><br><span class="line">    C1 = []</span><br><span class="line">    C2 = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x)):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(y)):</span><br><span class="line">            z1 = two_dim_Gauss(np.array([[x[i], y[j]]]), mu1, cov1)</span><br><span class="line">            z2 = two_dim_Gauss(np.array([[x[i], y[j]]]), mu2, cov2)</span><br><span class="line">            temp1 = z1 * p_c1</span><br><span class="line">            temp2 = z2 * p_c2</span><br><span class="line">            <span class="keyword">if</span> temp1 &gt; temp2:</span><br><span class="line">                C1.append(np.array([[x[i], y[j]]]))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                C2.append(np.array([[x[i], y[j]]]))</span><br><span class="line">    C1 = np.array(C1).reshape(-<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">    C2 = np.array(C2).reshape(-<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    plt.scatter(C1[:,<span class="number">0</span>], C1[:,<span class="number">1</span>])</span><br><span class="line">    plt.scatter(C2[:,<span class="number">0</span>], C2[:,<span class="number">1</span>])</span><br><span class="line">    plt.xlabel(<span class="string">&quot;X&quot;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&quot;Y&quot;</span>)</span><br><span class="line">    plt.legend([<span class="string">&#x27;Class1&#x27;</span>, <span class="string">&#x27;Class2&#x27;</span>])</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_Gauss_point</span>(<span class="params">arr, mu, cov</span>):</span></span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.scatter(arr[:,<span class="number">0</span>], arr[:,<span class="number">1</span>])</span><br><span class="line">    x_min, x_max = plt.gca().get_xlim()</span><br><span class="line">    y_min, y_max = plt.gca().get_ylim()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    num = <span class="number">50</span></span><br><span class="line">    x = np.linspace(x_min, x_max, num)</span><br><span class="line">    y = np.linspace(y_min, y_max, num)</span><br><span class="line">    X, Y = np.meshgrid(x, y)</span><br><span class="line">    </span><br><span class="line">    Z = np.zeros_like(X)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x)):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(y)):</span><br><span class="line">            z = two_dim_Gauss(np.array([[x[i], y[j]]]), mu, cov)</span><br><span class="line">            Z[j][i] = z</span><br><span class="line">    plt.contour(X,Y,Z)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;X&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;Y&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">path1 = <span class="string">&quot;./dataSets/densEst1.txt&quot;</span></span><br><span class="line">path2 = <span class="string">&quot;./dataSets/densEst2.txt&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    arr1 = load_data(path1)</span><br><span class="line">    arr2 = load_data(path2)</span><br><span class="line"></span><br><span class="line">    arr_sum = np.vstack((arr1, arr2))</span><br><span class="line"></span><br><span class="line">    p_C1 = prior_probability(arr1, arr_sum)</span><br><span class="line">    p_C2 = prior_probability(arr2, arr_sum)</span><br><span class="line"></span><br><span class="line">    mu2 = MLE_mean(arr2)</span><br><span class="line">    bias_s2, sigma2 = bias_unbias_var(arr2)</span><br><span class="line"></span><br><span class="line">    mu1 = MLE_mean(arr1)</span><br><span class="line">    bias_s1, sigma1 = bias_unbias_var(arr1)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;The mean for C1 is: &quot;</span>, mu1)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;The mean for C2 is: &quot;</span>, mu2)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;The sigma with bias for C1 is&quot;</span>, bias_s1)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;The sigma without bias for C1 is&quot;</span>, sigma1)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;The sigma with bias for C2 is&quot;</span>, bias_s2)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;The sigma without bias for C2 is&quot;</span>, sigma2)</span><br><span class="line">    </span><br><span class="line">    plot_Gauss_point(arr1, mu1, sigma1)</span><br><span class="line">    plt.title(<span class="string">&quot;densEst1&quot;</span>)</span><br><span class="line">    plot_Gauss_point(arr2, mu2, sigma2)</span><br><span class="line">    plt.title(<span class="string">&quot;densEst2&quot;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    posterior(mu1, mu2, sigma1, sigma2, p_C1, p_C2)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><br>结果如下：<br><img src="https://img-blog.csdnimg.cn/20210625195929319.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20210625195925925.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20210625195929478.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
]]></content>
      <categories>
        <category>学习总结</category>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>概率密度估计（Probability Density Estimation）--Part 2：无参估计</title>
    <url>/posts/7.html</url>
    <content><![CDATA[<h1 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h1><p>接上一篇的【有参估计】，这篇介绍无参估计，也就是说在这里我们事先不知道数据的模型，而要求数据进行划分，这也是实际中比较常见的情况。<br><span id="more"></span></p>
<p>这主要介绍三种无参估计方法，分别是：</p>
<pre><code>1. 直方图（Histograms）
2. 核密度估计（Kernel Density Estimation，KDE）
3. K最邻近法（K-nearest Neighbors，KNN）
</code></pre><h1 id="直方图（Histograms）"><a href="#直方图（Histograms）" class="headerlink" title="直方图（Histograms）"></a>直方图（Histograms）</h1><p>直方图是最简单的一种方法，通过对数据进行统计分类，画出直方图即可，这里需要注意的是直方图中$bin$值的选取。\<br>对于$bin$值的不同选取可能会有下面三种情况：<br><img src="https://img-blog.csdnimg.cn/20210627054925776.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>直方图的特点是：</p>
<blockquote>
<ol>
<li>使用非常普遍，没有数据的限制，任何概率密度都可以被任意地近似；\</li>
<li>这个方法太粗暴。。。</li>
</ol>
</blockquote>
<p>直方图的缺陷有：</p>
<blockquote>
<ol>
<li>高维空间中，$bin$的数目以指数级增长；\</li>
<li>$bin$值得选取不容易。</li>
</ol>
</blockquote>
<p>直方图的相关计算：<br>（也不局限于直方图，这些计算方法是很通用的，后面也会用到。）\<br>我们现在有数据点$x$，其是从概率密度$p(x)$中取样的。\<br>$x$落在区域$R$的概率是：<script type="math/tex">P(x\in R)=\int_Rp(x)dx</script>如果$R$非常小，体积为$V$，则$p(x)$几乎是常数：<script type="math/tex">P(x\in R)=\int_Rp(x)dx\approx p(x)V</script>如果$R$很大，则：<script type="math/tex">P(x\in R) = \frac{K}{N}\rightarrow p(x)\approx \frac{K}{NV}</script>其中$N$是数据总数，$K$是落在区域$R$内的数据数。</p>
<p>对于Kernel density estimation (KDE)，是固定$V$并确定$K$，比如说：确定固定在超立方体中的数据点$K$的数量，如下图示：<br><img src="https://img-blog.csdnimg.cn/20210627061013461.png#pic_center" alt="在这里插入图片描述"><br>对于K-nearest neighbors (kNN)，是固定$K$并确定$V$，比如说：增加球的尺寸直到$K$个点都落在球内，如下图：<br><img src="https://img-blog.csdnimg.cn/20210627061147105.png#pic_center" alt="在这里插入图片描述"></p>
<h1 id="KDE"><a href="#KDE" class="headerlink" title="KDE"></a>KDE</h1><h2 id="Parzen-Window"><a href="#Parzen-Window" class="headerlink" title="Parzen Window"></a>Parzen Window</h2><p>对于一个$d$维，边长为$h$的超立方体，我们有如下关系：<br><img src="https://img-blog.csdnimg.cn/20210628225842881.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h2 id="Gaussian-Kernel"><a href="#Gaussian-Kernel" class="headerlink" title="Gaussian Kernel"></a>Gaussian Kernel</h2><p>对于高斯核，我们则可以写成：<br><img src="https://img-blog.csdnimg.cn/2021062822595979.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h2 id="General-Formulation-–-Arbitrary-Kernel"><a href="#General-Formulation-–-Arbitrary-Kernel" class="headerlink" title="General Formulation – Arbitrary Kernel"></a>General Formulation – Arbitrary Kernel</h2><p>对于更一般的形式，则有：<br><img src="https://img-blog.csdnimg.cn/20210628230351141.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h2 id="各种内核的总结"><a href="#各种内核的总结" class="headerlink" title="各种内核的总结"></a>各种内核的总结</h2><h3 id="高斯核（Gaussian-Kernel）"><a href="#高斯核（Gaussian-Kernel）" class="headerlink" title="高斯核（Gaussian Kernel）"></a>高斯核（Gaussian Kernel）</h3><script type="math/tex; mode=display">k(u)=\frac{1}{\sqrt{2\pi}}exp\left\{-\frac{1}{2}u^2\right\}</script><p>缺点是：</p>
<pre><code>1. kernel has infinite support
2. Requires a lot of computation
</code></pre><h3 id="Parzen-window"><a href="#Parzen-window" class="headerlink" title="Parzen window"></a>Parzen window</h3><p><img src="https://img-blog.csdnimg.cn/20210701060654119.png#pic_center" alt="在这里插入图片描述"><br>缺点是：</p>
<pre><code>1. Not very smooth results
</code></pre><h3 id="Epanechnikov-kernel"><a href="#Epanechnikov-kernel" class="headerlink" title="Epanechnikov kernel"></a>Epanechnikov kernel</h3><p><img src="https://img-blog.csdnimg.cn/20210701060839776.png#pic_center" alt="在这里插入图片描述"><br>缺点是：</p>
<pre><code>1. Smoother, but finite support
</code></pre><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>核方法的缺点是：我们必须合适地选择核的带宽$h$。\<br>如果$h$太大则会过于平滑，太小则不够平滑。\<br>一个高斯核的例子：<br><img src="https://img-blog.csdnimg.cn/20210701061312403.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h1 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h1><p>KNN分类大体意思是：假设我们的数据集有$N$个点，有$N_j$个点属于类$C_j$，并且有$\sum_jN_j=N$。现在要对点$x$进行分类，我们先画一个球，中心在$x$处，并且包含了（任意类的）$K$个点。假设球体积为$V$，且包含有$K_j$个点是属于$C_j$的，则我们有如下关系：<br><img src="https://img-blog.csdnimg.cn/20210701062118992.png#pic_center" alt="在这里插入图片描述"></p>
<blockquote>
<p>注意：与灰度图的$bins$和KDE的$h$类似，$K$值太大则会过于平滑，太小则不够平滑。</p>
</blockquote>
<p>一个例子：<br><img src="https://img-blog.csdnimg.cn/20210701062621685.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h1 id="（附）作业相关代码"><a href="#（附）作业相关代码" class="headerlink" title="（附）作业相关代码"></a>（附）作业相关代码</h1><p><img src="https://img-blog.csdnimg.cn/20210701062801504.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>简单解释下作业：\<br>有两个数据集，分别为训练集和测试集，第1题要求画出不同$bins$下的直方图；第2题则是用高斯核，用不同的$\sigma$计算训练集的对数似然，比较差异性；第3题要求用不同的$K$值写$KNN$代码。\<br>代码如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.special <span class="keyword">import</span> logsumexp</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span>(<span class="params">path</span>):</span></span><br><span class="line">    data = np.loadtxt(path)</span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_histogram</span>(<span class="params">bins, arr</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(bins) % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">        fir = <span class="built_in">len</span>(bins) / <span class="number">2</span> * <span class="number">100</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        fir = (<span class="built_in">len</span>(bins) // <span class="number">2</span> + <span class="number">1</span>) * <span class="number">100</span></span><br><span class="line">    </span><br><span class="line">    pos = fir + <span class="number">20</span> + <span class="number">1</span></span><br><span class="line">    interval = np.<span class="built_in">max</span>(arr) - np.<span class="built_in">min</span>(arr)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(bins)):</span><br><span class="line">        sub_bin = <span class="built_in">int</span>(interval // bins[i]) + <span class="number">1</span></span><br><span class="line">        plt.subplot(pos+i)</span><br><span class="line">        plt.hist(arr, bins = sub_bin, facecolor=<span class="string">&quot;blue&quot;</span>, edgecolor=<span class="string">&quot;black&quot;</span>, alpha=<span class="number">0.7</span>)</span><br><span class="line">        plt.title(<span class="string">&quot;Bin = %.3f&quot;</span> % bins[i])</span><br><span class="line">        plt.xlabel(<span class="string">&#x27;data&#x27;</span>)</span><br><span class="line">        plt.ylabel(<span class="string">&#x27;times&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kernel_density_estimate</span>(<span class="params">x_arr, x_lst, sigma</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_H_Gauss</span>(<span class="params">h, u, d</span>):</span></span><br><span class="line">        H_u = np.exp(-(np.linalg.norm(u) ** <span class="number">2</span>) / (<span class="number">2</span> * h * h)) \</span><br><span class="line">              / (np.sqrt(<span class="number">2</span> * np.pi * h * h) ** d)</span><br><span class="line">        <span class="keyword">return</span> H_u</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_K</span>(<span class="params">N, x, x_arr, h, d</span>):</span></span><br><span class="line">        k = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">            u = x - x_arr[i]</span><br><span class="line">            k += _H_Gauss(h, u, d)</span><br><span class="line">        <span class="keyword">return</span> k</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">p</span>(<span class="params">N, x, x_arr, h, d, V</span>):</span></span><br><span class="line">        p_x = _K(N, x, x_arr, h, d) / (N * V)</span><br><span class="line">        <span class="keyword">return</span> p_x</span><br><span class="line">    </span><br><span class="line">    V = <span class="number">1</span></span><br><span class="line">    N = <span class="built_in">len</span>(x_arr.flatten())</span><br><span class="line">    p_x_lst = []</span><br><span class="line">    d = <span class="built_in">len</span>(x_arr.flatten())/(x_arr.shape[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> x_lst:</span><br><span class="line">        temp_p = p(N, x, x_arr, sigma, d, V)</span><br><span class="line">        p_x_lst.append(temp_p)</span><br><span class="line">    <span class="keyword">return</span> p_x_lst</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_logsumexp</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.log(np.<span class="built_in">sum</span>(np.exp(x)))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ked_log_likelihood</span>(<span class="params">x_arr, x_lst, sigma</span>):</span></span><br><span class="line">    N = x_arr.shape[<span class="number">0</span>]</span><br><span class="line">    ll_lst = []</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> x_lst:</span><br><span class="line">        single_val = _logsumexp(-np.linalg.norm(x_arr.reshape(-<span class="number">1</span>,<span class="number">1</span>) - x, axis=<span class="number">1</span>) ** <span class="number">2</span> / (<span class="number">2</span> * sigma * sigma) \</span><br><span class="line">                               - np.log(np.sqrt(<span class="number">2</span> * np.pi * sigma * sigma) * N))</span><br><span class="line">        ll_lst.append(single_val)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> np.<span class="built_in">sum</span>(ll_lst)</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kNN</span>(<span class="params">data, x_lst, k</span>):</span></span><br><span class="line">    knn_lst = []</span><br><span class="line">    N = <span class="built_in">len</span>(data.flatten())</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> x_lst:</span><br><span class="line">        u = np.linalg.norm(x - data.reshape(-<span class="number">1</span>,<span class="number">1</span>), axis = <span class="number">1</span>)</span><br><span class="line">        r = np.sort(u)[k-<span class="number">1</span>]</span><br><span class="line">        v = <span class="number">2</span> * r</span><br><span class="line">        temp_p = k / (N * v)</span><br><span class="line">        knn_lst.append(temp_p)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> knn_lst, np.<span class="built_in">sum</span>(np.log(knn_lst))</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Comparison_NPM</span>(<span class="params">data, test_data, k_lst, sigma_lst</span>):</span></span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> sigma_lst:</span><br><span class="line">        temp_kde_test = ked_log_likelihood(data, test_data, s)</span><br><span class="line">        temp_kde_train = ked_log_likelihood(data, data, s)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;train_data: log_likelihood of KDE with sigma = %.2f: %.12f&#x27;</span> % (s, temp_kde_train) )</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;test_data : log_likelihood of KDE with sigma = %.2f: %.12f&#x27;</span> % (s, temp_kde_test) )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> k_lst:</span><br><span class="line">        _, temp_knn_test = kNN(data, test_data, k)</span><br><span class="line">        _, temp_knn_train = kNN(data, data, k)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;train_data: log_likelihood of KDE with k = %d: %.12f&#x27;</span> % (k, temp_knn_train) )</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;test_data : log_likelihood of KDE with k = %d: %.12f&#x27;</span> % (k, temp_knn_test) )        </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    path = <span class="string">&quot;./dataSets/nonParamTrain.txt&quot;</span></span><br><span class="line">    path_test = <span class="string">&#x27;./dataSets/nonParamTest.txt&#x27;</span></span><br><span class="line">    data = load_data(path)</span><br><span class="line">    data_test = load_data(path_test)</span><br><span class="line">    bins_w = [<span class="number">0.02</span>, <span class="number">0.5</span>, <span class="number">2.0</span>]</span><br><span class="line">    plot_histogram(bins_w, data)</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    num = <span class="built_in">len</span>(data.flatten())</span><br><span class="line"></span><br><span class="line">    p_x = []</span><br><span class="line">    p_test = []</span><br><span class="line">    sigma = [<span class="number">0.03</span>, <span class="number">0.2</span>, <span class="number">0.8</span>]</span><br><span class="line">    x_lst = np.linspace(-<span class="number">4</span>, <span class="number">8</span>, data.shape[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> h <span class="keyword">in</span> sigma:</span><br><span class="line">        temp_p = kernel_density_estimate(data, x_lst, h)</span><br><span class="line">        p_x.append(temp_p)</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> p_x:</span><br><span class="line">        plt.plot(x_lst, x)</span><br><span class="line">    plt.xlabel(<span class="string">&quot;X&quot;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&quot;kde&quot;</span>)</span><br><span class="line">    plt.legend([<span class="string">&#x27;sigma=0.03&#x27;</span>,<span class="string">&#x27;sigma=0.2&#x27;</span>,<span class="string">&#x27;sigma=0.8&#x27;</span>])</span><br><span class="line">    plt.title(<span class="string">&#x27;KDE with different sigma&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    k = [<span class="number">2</span>, <span class="number">8</span>, <span class="number">35</span>]</span><br><span class="line">    knn_lst = []</span><br><span class="line">    <span class="keyword">for</span> sub_k <span class="keyword">in</span> k:</span><br><span class="line">        temp_knn, _ = kNN(data, x_lst, sub_k)</span><br><span class="line">        knn_lst.append(temp_knn)</span><br><span class="line">    <span class="keyword">for</span> knn <span class="keyword">in</span> knn_lst:</span><br><span class="line">        plt.plot(x_lst, knn)</span><br><span class="line">    plt.xlabel(<span class="string">&quot;X&quot;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&quot;knn&quot;</span>)</span><br><span class="line">    plt.axis([-<span class="number">4</span>, <span class="number">8</span>, <span class="number">0</span>, <span class="number">1.5</span>])</span><br><span class="line">    plt.legend([<span class="string">&#x27;k=2&#x27;</span>,<span class="string">&#x27;k=8&#x27;</span>,<span class="string">&#x27;k=35&#x27;</span>])</span><br><span class="line">    plt.title(<span class="string">&#x27;KNN with different k&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    Comparison_NPM(data, data_test, k, sigma)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p>结果显示如下：<br><img src="https://img-blog.csdnimg.cn/20210701063451438.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/2021070106345256.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20210701063452883.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
]]></content>
      <categories>
        <category>学习总结</category>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>概率密度估计（Probability Density Estimation）--Part 3：混合模型</title>
    <url>/posts/8.html</url>
    <content><![CDATA[<h1 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h1><p>在结束了有参估计，无参估计后，现在记录混合模型（Mixture models）。这里附一张有参和无参的对比图（本来应该附在Part 2的，不想回去改了。。）：<br><span id="more"></span><br><img src="https://img-blog.csdnimg.cn/20210703061913869.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<blockquote>
<p>字面意思，混合模型就是有参模型和无参模型的混合。</p>
</blockquote>
<p>举个例子，高斯模型的混合（Mixture of Gaussians，MoG）。\<br>现有三个高斯模型如下：<br><img src="https://img-blog.csdnimg.cn/20210703062219101.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>我们可以将其视为：<br><img src="https://img-blog.csdnimg.cn/20210703062258416.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>其概率密度可以近似表示为：<script type="math/tex">p(x)=\sum^M_{j=1}p(x|j)p(j)</script>各变量有如下关系：<br><img src="https://img-blog.csdnimg.cn/20210703062522599.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>大概解释一下：$p(x|j)$表示第$j$个高斯模型的概率密度，$p(j)=\pi_j$表示第$j$个高斯模型出现的概率，是一种先验概率。\<br>这里记住两点：<br><img src="https://img-blog.csdnimg.cn/20210703062812442.png#pic_center" alt="在这里插入图片描述"><br>即是说，混合模型概率密度积分为1，然后对于混合模型的每个子模型，我们都要求出对应的三个值（均值，方差和模型先验），这是对于MoG而言的。</p>
<h1 id="求解方法"><a href="#求解方法" class="headerlink" title="求解方法"></a>求解方法</h1><h2 id="MLE法"><a href="#MLE法" class="headerlink" title="MLE法"></a>MLE法</h2><p>对于简单的（单个）高斯模型，我们可以用MLE近似求解，但在高斯混合中无法使用该方法，因为我们在求参数时并不知道这个参数属于高斯混合的哪个子分布。如下：<br><img src="https://img-blog.csdnimg.cn/20210707183134950.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>这个$p(j|x)$我们是无法观测到的，毕竟如果这个都知道，那高斯混合就只是多个高斯模型的简单叠加而已。\<br>也就是说单个高斯模型的参数会取决于所有高斯模型的参数。所以MLE不可用。</p>
<h2 id="Clustering"><a href="#Clustering" class="headerlink" title="Clustering"></a>Clustering</h2><p>分为两种：</p>
<blockquote>
<ol>
<li>Clustering with soft assignments\</li>
<li>Clustering with hard assignments</li>
</ol>
</blockquote>
<p>简单说的话，hard assignments是根据Label划分的，也就是说每个数据点只属于一个label，比如说K-mean；soft assignment是根据概率区分的，每个数据点可能属于多个label，但概率不同。如下图：<br><img src="https://img-blog.csdnimg.cn/20210707184402519.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/2021070718442371.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="所有点都可能是label1或2"><br>这里只介绍soft assignment，毕竟两个几乎是一样的。这就要用到这章的重点了，$EM$算法。</p>
<h3 id="EM-算法"><a href="#EM-算法" class="headerlink" title="$EM$算法"></a>$EM$算法</h3><h4 id="大概的说明"><a href="#大概的说明" class="headerlink" title="大概的说明"></a>大概的说明</h4><p>$EM$算法是一种迭代算法，本质上它也是一种最大似然估计的方法，其特点是，当我们的数据不完整时，比如说只有观测数据，缺乏隐含数据时，可以用EM算法进行迭代推导。\<br>该算法包括$E$步骤和$M$步骤，这里不具体介绍一般性的$EM$算法，主要说明其在高斯混合中如何运用。其步骤大体可以概括为：</p>
<blockquote>
<ol>
<li>随机初始化各子分布的期望$\mu_1,\mu_2…\mu_m$。</li>
<li><strong>E-step</strong>：计算每个子分布的后验<script type="math/tex">p(j|x_n)</script></li>
<li><strong>M-step</strong>计算所有数据点的加权平均值<br><img src="https://img-blog.csdnimg.cn/202107090127095.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>其效果如下图所示：<br><img src="https://img-blog.csdnimg.cn/20210709012920879.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li>
</ol>
</blockquote>
<h4 id="较为详细的说明"><a href="#较为详细的说明" class="headerlink" title="较为详细的说明"></a>较为详细的说明</h4><p>现在说一下更加详细的步骤。\<br>假设现在有两组数据，即是观测数据和隐藏数据：</p>
<blockquote>
<ol>
<li>Incomplete (observed) data: $X={(X_1, X_2, … ,X_n)}$</li>
<li>Hidden (unobserved) data: $Y=(Y_1, Y_2,…,Y_n)$</li>
</ol>
</blockquote>
<p>组合后形参完整数据：</p>
<blockquote>
<ol>
<li>Complete data: $Z=(X,Y)$</li>
</ol>
</blockquote>
<p>联合密度为<script type="math/tex">p(Z)=p(X,Y)=p(Y|X)p(X)</script>即是<script type="math/tex">p(Z|\theta)=p(X,Y|\theta)=p(Y|X,\theta)p(X|\theta)</script>在高斯混合中：\<br>$p(X|\theta)$是混合模型的似然\<br>$p(Y|X,\theta)$是混合模型中子分布的估计</p>
<p>对于不完整的数据（观测数据），其似然为：<script type="math/tex">L(\theta|X)=p(X|\theta)=\prod_{n=1}^{N}p(X_n|\theta)</script>对于完整数据（Z），其似然为：<br><img src="https://img-blog.csdnimg.cn/20210712040548473.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>在这里我们虽然不知道$Y$，但如果我们知道当前的参数猜测$\theta^{i-1}$，我们就能用它来预测$Y$。\<br>在这里我们计算完整数据的对数似然的期望，如下：<br><img src="https://img-blog.csdnimg.cn/20210712042100870.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>其中$X$和$\theta^{i-1}$是已知的。更进一步展开如下：<br><img src="https://img-blog.csdnimg.cn/20210712051332414.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>这个等式是根据均值和积分的关系写出来的。即是如下关系：\</p>
<script type="math/tex; mode=display">E[x]=\int_Xxf(x)dx$$其中$f(x)$是概率密度（_这部分不太确定，有错的话麻烦大家指出_）。\
我们需要最大化这个$Q$函数。\
接下来是$EM$算法：\
**E-step(expectation)**: 计算$p(y|X,\theta^{i-1})$以便计算$Q(\theta,\theta^{i-1})$;\
**M-step(maximization)**: 最大化$Q$函数求出$\theta$ $$\hat{\theta}=arg max_\theta Q(\theta,\theta^{i-1})</script><p>这是一种迭代运算，我们要确保每次迭代中，第$i$次的结果至少和第$i-1$的一样好，即是：<script type="math/tex">Q(\theta^i,\theta^{i-1})\geq Q(\theta^{i-1},\theta^{i-1})</script><br>若该期望值对于$\theta$来说是最大的，则可以认为(==这部分未理解==)：<script type="math/tex">L(\theta^i|X)\geq L(\theta^{i-1}|X)</script><br>也就是说，在每次迭代中观测数据的对数似然都会不断增大（或者至少保持不变），最终达到局部最大值。\<br>所以，在实际运用中，初始化对于$EM$算法很重要，一个不好的初始化可能会使结果停在一个不好的局部最优值中。</p>
<h4 id="高斯混合中的-EM-算法（EM-for-Gaussian-Mixtures）"><a href="#高斯混合中的-EM-算法（EM-for-Gaussian-Mixtures）" class="headerlink" title="高斯混合中的$EM$算法（EM for Gaussian Mixtures）"></a>高斯混合中的$EM$算法（EM for Gaussian Mixtures）</h4><p>步骤：</p>
<ol>
<li>初始化参数$\mu_1,\sigma_1, \pi_1…$\</li>
<li><p>循环，直到满足终止条件：</p>
<ol>
<li>E-step: 计算每个数据点对于每个子分布的后验分布:<br><img src="https://img-blog.csdnimg.cn/20210712054310641.png#pic_center" alt="在这里插入图片描述"><br>这里的$\alpha$可以理解成每个数据点属于各个子分布的权重或者说概率。\</li>
<li>M-step: 使用E步骤的权重进行更新数据：    <img src="https://img-blog.csdnimg.cn/2021071205454123.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述">至此，高斯混合的$EM$算法就结束了，然后还有最后一个问题，这部分不太理解，但把结论贴上来吧，以后再探究：</li>
</ol>
</li>
</ol>
<p><img src="https://img-blog.csdnimg.cn/2021071205595953.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h1 id="（附）作业相关代码"><a href="#（附）作业相关代码" class="headerlink" title="（附）作业相关代码"></a>（附）作业相关代码</h1><p><img src="https://img-blog.csdnimg.cn/2021071502051214.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>也就是给出数据点，然后用EM算法进行高斯拟合：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> multivariate_normal</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span>(<span class="params">path</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.loadtxt(path)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_params</span>(<span class="params">k, d = <span class="number">2</span></span>):</span></span><br><span class="line">    pi = np.ones(k) * <span class="number">1</span>/k</span><br><span class="line">    mu = [np.random.rand(<span class="number">2</span>, <span class="number">1</span>) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k)]</span><br><span class="line">    cov = [np.eye(<span class="number">2</span>) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> pi, mu, cov</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">EM</span>(<span class="params">iter_times, data, k</span>):</span></span><br><span class="line">    pi, mu, cov = init_params(k)</span><br><span class="line">    N = data.shape[<span class="number">0</span>]</span><br><span class="line">    alpha = np.zeros((N, k))</span><br><span class="line">    likelihood_list = []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_alpha</span>(<span class="params">alpha, N, k</span>):</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">                p_ij = pi[j] * multivariate_normal.pdf(data[i], mu[j].flatten(), cov[j])</span><br><span class="line">                alpha[i][j] = p_ij</span><br><span class="line">                likelihood[i] += p_ij</span><br><span class="line">            alpha[i] = alpha[i] / np.<span class="built_in">sum</span>(alpha[i])</span><br><span class="line">        <span class="keyword">return</span> alpha</span><br><span class="line"></span><br><span class="line">    sigma = np.empty((N, <span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">        d = data[i, :].reshape(<span class="number">2</span>, -<span class="number">1</span>)</span><br><span class="line">        sigma[i] = np.dot(d, d.T)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iter_times):</span><br><span class="line">        likelihood = np.zeros(N)</span><br><span class="line">        <span class="comment"># E-step: update alpha</span></span><br><span class="line">        alpha = update_alpha(alpha, N, k)</span><br><span class="line">        likelihood_list.append(np.<span class="built_in">sum</span>(np.log(likelihood)))</span><br><span class="line">        <span class="comment"># M-step: update mu, cov, pi</span></span><br><span class="line">        N_j = np.<span class="built_in">sum</span>(alpha, axis = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">            <span class="comment"># update mu</span></span><br><span class="line">            alpha_x = <span class="number">0</span></span><br><span class="line">            alpha_x_mu = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">                alpha_x += (alpha[n][j] * data[n])</span><br><span class="line">            alpha_x = alpha_x.reshape(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">            mu[j] = alpha_x / N_j[j]</span><br><span class="line">            <span class="comment"># update pi</span></span><br><span class="line">            pi[j] = N_j[j] / N</span><br><span class="line">            <span class="comment"># update cov</span></span><br><span class="line">            <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">                alpha_x_mu += alpha[n][j]*(data[n] - mu[j].T)*(data[n] - mu[j].T).T</span><br><span class="line">            cov[j] = alpha_x_mu / N_j[j]</span><br><span class="line"></span><br><span class="line">    plot_contour(iter_times, data, mu, cov, k)</span><br><span class="line">    <span class="keyword">if</span> iter_times == <span class="number">30</span>:</span><br><span class="line">        plt.figure()</span><br><span class="line">        plt.plot(np.arange(<span class="number">1</span>, <span class="number">31</span>), np.array(likelihood_list))</span><br><span class="line">        plt.title(<span class="string">&#x27;log-likelihood for every iteration&#x27;</span>)</span><br><span class="line">        plt.xlabel(<span class="string">&#x27;iteration&#x27;</span>)</span><br><span class="line">        plt.ylabel(<span class="string">&#x27;log-likelihood&#x27;</span>)</span><br><span class="line">        plt.grid()</span><br><span class="line">        plt.show()</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_contour</span>(<span class="params">iter_num, data, mu, cov, k</span>):</span></span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.title(<span class="string">&quot;iter_num = %d&quot;</span> % iter_num)</span><br><span class="line">    plt.scatter(data[:,<span class="number">0</span>], data[:,<span class="number">1</span>])</span><br><span class="line">    x_min, x_max = plt.gca().get_xlim()</span><br><span class="line">    y_min, y_max = plt.gca().get_ylim()</span><br><span class="line"></span><br><span class="line">    num = <span class="number">50</span></span><br><span class="line">    x = np.linspace(x_min, x_max, num)</span><br><span class="line">    y = np.linspace(y_min, y_max, num)</span><br><span class="line">    X, Y = np.meshgrid(x, y)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> sub_k <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">        Z = np.zeros_like(X)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x)):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(y)):</span><br><span class="line">                z = multivariate_normal.pdf(np.array([[x[i]], [y[j]]]).flatten(), mu[sub_k].flatten(), cov[sub_k])</span><br><span class="line">                Z[j, i] = z</span><br><span class="line">        plt.contour(X, Y, Z)</span><br><span class="line">        plt.show()</span><br><span class="line"></span><br><span class="line">path = <span class="string">&quot;./dataSets/gmm.txt&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    k = <span class="number">4</span></span><br><span class="line">    iter_lst = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">30</span>]</span><br><span class="line">    pi, mu, cov = init_params(k)</span><br><span class="line">    data = load_data(path)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> iter_lst:</span><br><span class="line">        EM(i, data, k)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure></p>
<p>效果如下图所示：<br>迭代1次：<br><img src="https://img-blog.csdnimg.cn/20210715020905924.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>迭代3次：<br><img src="https://img-blog.csdnimg.cn/20210715020944160.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>迭代5次：<br><img src="https://img-blog.csdnimg.cn/20210715021008112.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>迭代10次：<br><img src="https://img-blog.csdnimg.cn/20210715021034201.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>迭代30次：<br><img src="https://img-blog.csdnimg.cn/20210715021055371.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>每次迭代的对数似然：<br><img src="https://img-blog.csdnimg.cn/20210715021143437.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
]]></content>
      <categories>
        <category>学习总结</category>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>线性回归（Linear Regression）</title>
    <url>/posts/9.html</url>
    <content><![CDATA[<h1 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h1><p>回归问题和分类问题以前老是弄混，最直接的记法就是，分类问题是针对离散空间的，就比如说给定一张图片，根据其特征判断这张图片是猫还是狗，这就是分类问题；回归问题的话是针对连续空间的，比如预测距离，概率等。<br><span id="more"></span></p>
<h1 id="最小二乘法线性回归（Least-Squares-Linear-Regression）"><a href="#最小二乘法线性回归（Least-Squares-Linear-Regression）" class="headerlink" title="最小二乘法线性回归（Least Squares Linear Regression）"></a>最小二乘法线性回归（Least Squares Linear Regression）</h1><h2 id="一次项回归"><a href="#一次项回归" class="headerlink" title="一次项回归"></a>一次项回归</h2><p>给定一组训练数据以及相关的函数值$(x_i, y_i)$（这里的$x_i$是一组向量）如下：<script type="math/tex">X=\left\{x_1 \in R^d,...,x_n\right\}$$$$Y=\left\{y_1 \in R,...,y_n\right\}</script>线性回归的方程如下：<script type="math/tex">x_i^Tw+w_0=y_i\   \forall i =1,...n</script>这个等式与最小二乘法分类的等式一样，唯一的区别就是值是连续的。\<br>$w_0$在这里是一个偏置，即是常见的$bias$，一般我们会把它整合到权重里，既方便也好算。\<br>最小二乘法线性回归的一般性步骤如下：</p>
<p><strong>Step 1</strong>：Define：</p>
<script type="math/tex; mode=display">\hat{x}_i=\begin{bmatrix} x_i\\ 1\end{bmatrix}$$$$\hat{w}=\begin{bmatrix} w\\ w_0\end{bmatrix}</script><p><strong>Step 2</strong>: Rewrite:</p>
<script type="math/tex; mode=display">\hat{x}_i^T\hat{w}=y_i\ \forall i=1,...,n</script><p><strong>Step 3</strong>: Matrix-vector notation</p>
<script type="math/tex; mode=display">\hat{X}^T\hat{w}=y$$这里$\hat{X}=[\hat{x}_1,...,\hat{x}_n]$，每个$\hat{x}_i$都是一个向量，且$y=[y_1,...,y_n]^T$。\
**Step 4**: Fine the least squares solution
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210716080914567.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center)
这里就直接解出权重$w$了，在这一系列的计算里，计算量最大的就是那个$R^{D\times D}$的逆矩阵，直接计算逆矩阵的话，其复杂度是$O(D^3)$，所以当$D$很大时计算成本就很高了，所以一般我们会用下面两种方法：

> 1. Gradient descent\
> 2. Work with fewer dimensions

这两种方法应该都挺熟悉了，所以这里不做介绍。

## 多项式回归（Polynomial Regression）
这是建立在最小二乘回归上的，等式形式跟上面的类似：$$y(x)=w^T\phi(x)=\sum_{i=0}^{M}w_i\phi_i(x)</script><p>并且$\phi_0(x)=1$，$\phi_i(.)$叫基础方程（basis functions），对于$w$来说这依旧是个线性方程。\<br>此处的$\phi(x)$代表多项式，比如说：<script type="math/tex">\phi(x)=(1,x,x^2,x^3)^T</script><br>举个例子，对于不同degree的多项式，其拟合效果如下：<br><img src="https://img-blog.csdnimg.cn/20210716084632249.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>对于不同的多项式会出现欠拟合或者过拟合的现象。</p>
<h1 id="回归的最大似然法（Maximum-Likelihood-Approach-to-Regression）"><a href="#回归的最大似然法（Maximum-Likelihood-Approach-to-Regression）" class="headerlink" title="回归的最大似然法（Maximum Likelihood Approach to Regression）"></a>回归的最大似然法（Maximum Likelihood Approach to Regression）</h1><h2 id="概率回归（Probabilistic-Regression）"><a href="#概率回归（Probabilistic-Regression）" class="headerlink" title="概率回归（Probabilistic Regression）"></a>概率回归（Probabilistic Regression）</h2><p>对于概率回归有以下两个假设：</p>
<p><strong>Assumption 1</strong>：我们的目标函数值是通过向方程添加噪声产生的，即是：<script type="math/tex">y = f(x,w)+\epsilon</script><br>$y$：目标函数值；    $x$：输入值\<br>$f$： 回归方程；    $w$：权重或者说参数；$\epsilon$：噪声\<br><strong>Assumption 2</strong>：这个噪声是服从高斯分布的随机变量，即：<script type="math/tex">\epsilon \sim N(0,\beta^{-1})$$$$p(y|x,w,\beta)=N(y|f(x,w),\beta^{-1})</script>其中$f(x,w)$是均值，$\beta^{-1}$是方差（$\beta$是precision），注意，现在$y$是一个随机变量，其基本概率分布为$p(y|x,w,\beta)$。</p>
<p><strong>具体说明</strong>\<br>现在给定一组数据点$X=[x_1,…,x_n]\in \mathcal{R}^{d\times n}$以及相关的函数值$Y=[y_1,…,y_n]^T$\<br>其条件似然为（设数据是i.i.d.的）：<script type="math/tex">p(y|X,w,\beta)=\prod_{i=1}^n N(y_i|f(x_i,w),\beta^{-1})</script>带入线性模型：<script type="math/tex">=\prod_{i=1}^n N(y_i|w^T\phi(x_i),\beta^{-1})</script>其中$w^T\phi(x_i)$是广义的线性回归方程，然后现在我们就要计算关于$\beta$和$w$的最大化似然了。\<br>用对数似然如下：<br><img src="https://img-blog.csdnimg.cn/20210716094220653.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>对于$w$的梯度为：<br><img src="https://img-blog.csdnimg.cn/20210716094315769.png#pic_center" alt="在这里插入图片描述"><br>我们做如下定义：<br><img src="https://img-blog.csdnimg.cn/20210716094531974.png#pic_center" alt="在这里插入图片描述"><br>求解上面的式子可以得到：<br><img src="https://img-blog.csdnimg.cn/20210716202619384.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>由此可见，用最大似然求解的权重值和前面最小二乘法回归的结果一样，我们也可得出这么一个结论：</p>
<blockquote>
<p><strong>最小二乘法等同于假设目标是高斯分布。</strong></p>
</blockquote>
<p>所以要注意，运用最小二乘法是有假设前提的。\<br>然而，与最小二乘法相比，最大似然的运用更广法，因为除了权重$w$，我们还可以用这个方法估计$\beta$：<br><img src="https://img-blog.csdnimg.cn/20210716203235914.png#pic_center" alt="在这里插入图片描述"><br>而且可以衡量我们的估计的不确定性（用loss function）。</p>
<h2 id="回归中的损失函数（Loss-Functions-in-Regression）"><a href="#回归中的损失函数（Loss-Functions-in-Regression）" class="headerlink" title="回归中的损失函数（Loss Functions in Regression）"></a>回归中的损失函数（Loss Functions in Regression）</h2><p>如果我们现在有一个新的数据$x_t$，在最小二乘回归中，其对应的值为：$y_t=\hat{x}_t^T\hat{w}$\<br>但在最大似然回归中，我们则是要计算相关概率值：$p(y|x,w,\beta)$。我们如何准确地估计$y_t$呢？这就要引入损失函数（loss function）了。</p>
<script type="math/tex; mode=display">L: \mathcal{R}\times \mathcal{R} \rightarrow \mathcal{R}^+$$$$(y_t,f(x_t))\rightarrow L(y_t,f(x_t))</script><p>然后我们最小化<strong>预期</strong>损失：<script type="math/tex">E_{x,y\sim p(x,y)}[L]=\int \int L(y,f(x))p(x,y)dxdy</script><br>举个例子，比如说损失函数是squared loss的话，可进行如下计算：<br><img src="https://img-blog.csdnimg.cn/20210716204651957.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20210716205115573.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>由此可见，对于squared loss，其最佳回归方程为后验$p(y|x)$的均值$E[y|x]$，这也称为均值预测。\<br>所以对于我们的广泛线性回归方程，可写为：</p>
<script type="math/tex; mode=display">f(x)=\int yN(y|w^T\phi(x),\beta^{-1})dy=w^T\phi(x)$$这又可以跟前面 产生联系了。

# 贝叶斯线性回归（Bayesian Linear Regression）
现在回到最原来的问题，我们想要避免过拟合和不稳定，然而用最大似然法依旧可能造成过拟合现象（比如说只有一个数据点的情况）。为了解决这个问题，现在就要引入贝叶斯线性回归。\
我们在参数$w$上放置一个先验，以控制不稳定性：$$p(w|X,y)= p(y|X,w)p(w)$$其中：

$p(w)$为参数先验；

$p(y|X,w)$为给定数据和参数下目标的似然（跟前面定义的一样）；

$p(w|X,y)$为参数后验。

>**注：这里得到的不再是参数的单一值，而是参数的概率分布**

最简单的想法就是假设参数$w$的先验符合高斯分布：$$w\sim p(w|\alpha)=N(w|0,\alpha^{-1}I)$$这里通过放置一个”软“限制，也就是$\alpha$，来避免不稳定性。另外，这里设均值为0只是为了方便后面计算，均值可为其它数。\
然后就有如下关系：
![在这里插入图片描述](https://img-blog.csdnimg.cn/2021071621150373.png#pic_center)
求解上面的参数概率可有多种方法，这里介绍两种：

## 最大后验（MAP）
取对数后验，然后最大化，如下：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210716212004115.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center)
然后对$w$求导：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210716212611934.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center)
（注：上面这个图里的式子有错，右边$\beta$应该在括号里，但我实在不想手打这些式子，只是计算过程而已。。。）
这就得出参数的等式了：
$$w_{map}=(\Phi \Phi^T+\frac{\alpha}{\beta})^{-1}\Phi y</script><p>与前面的结论不同，这里多了个$\frac{\alpha}{\beta}$，先验的作用是可以正则化这个伪逆，这个也叫岭回归（ridge regression）。</p>
<blockquote>
<p>既然说到了这个概念，就顺道提一下另一个跟它很像的$LASSO$ 回归，Lasso回归跟岭回归非常相似，它们的差别在于使用了不同的正则化项（LASSO是$l1$正则化，岭回归是$l2$）。最终都实现了约束参数从而防止过拟合的效果。另外，Lasso能够将一些作用比较小的特征的参数训练为0，从而获得稀疏解。也就是说用这种方法，在训练模型的过程中实现了降维(特征筛选)的目的。</p>
</blockquote>
<p>下面这张图可以直观显示岭回归和最小二乘回归的差别：<br><img src="https://img-blog.csdnimg.cn/20210716213954811.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h3 id="MAP与正则化的最小二乘法的比较"><a href="#MAP与正则化的最小二乘法的比较" class="headerlink" title="MAP与正则化的最小二乘法的比较"></a>MAP与正则化的最小二乘法的比较</h3><p>现在我们在前面的最小二乘法的结论等式上加一个正则化项：<img src="https://img-blog.csdnimg.cn/20210716214450502.png#pic_center" alt="在这里插入图片描述"><br>求解$w$我们得到一个新的估计：<script type="math/tex">\hat{w}=(\hat{X}\hat{X}^T+\lambda I)^{-1}\hat{X}y</script><br>此处 $\lambda = \alpha / \beta$。\<br>也就是说，如果我们在最小二乘回归上添加一个正则化项$\lambda$，则意味着我们假设目标有一个符合高斯分布的噪音，而且参数也是符合高斯分布的。\<br>以前面的9次多项式回归为例，加上不同的正则化项$\lambda$后，其效果为：<br><img src="https://img-blog.csdnimg.cn/20210716215303870.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>由此可见，$\lambda=\alpha / \beta$控制模型的复杂程度，并决定过拟合的程度。</p>
<h2 id="完全贝叶斯回归（Full-Bayesian-Regression）"><a href="#完全贝叶斯回归（Full-Bayesian-Regression）" class="headerlink" title="完全贝叶斯回归（Full Bayesian Regression）"></a>完全贝叶斯回归（Full Bayesian Regression）</h2><p>换个思路，其实我们也不是非得求出参数$w$，我们只需要通过训练数据来预测对应的函数值即可。也就是说可用边缘概率进行计算：<script type="math/tex">$p(y_t|x_t,X,y)=\int p(y_t,w|x_t,X,y)dw</script><br>用贝叶斯公式可将上面等式写为：<br><img src="https://img-blog.csdnimg.cn/20210716222918384.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>若先验之类的都是高斯的话，则这个预测分布也将服从高斯分布，即：<br><img src="https://img-blog.csdnimg.cn/20210716231602167.png#pic_center" alt="在这里插入图片描述"><br>（注：上图的等式有一个错误，中间那个等式最右边的$\Phi$没有转置。）</p>
<h1 id="（附）作业相关代码"><a href="#（附）作业相关代码" class="headerlink" title="（附）作业相关代码"></a>（附）作业相关代码</h1><p><img src="https://img-blog.csdnimg.cn/20210716231922529.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20210716232000630.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20210716232025659.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>大概就是按照这篇博文全部实现一遍：（这次的代码大的方面应该是没问题的，但一些细节上可能会有问题，改得有点心累，先这样吧）\<br>(a):<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_features</span>(<span class="params">X_train, X_test, y_train, y_test, _lambda = <span class="number">0.01</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    param:</span></span><br><span class="line"><span class="string">        X_train(ndarray): shape = (N, 1)</span></span><br><span class="line"><span class="string">        y_train(ndarray): shape = (N, 1)</span></span><br><span class="line"><span class="string">        _lambda(float)</span></span><br><span class="line"><span class="string">    return:</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_dim_bias</span>(<span class="params">X</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        [x1, x2, x3, ... , xn] -&gt; [[x1,1], [x2,1], ... ,[xn,1]]</span></span><br><span class="line"><span class="string">        param:</span></span><br><span class="line"><span class="string">            X(ndarray): train_data (N_train, 1)</span></span><br><span class="line"><span class="string">        return:</span></span><br><span class="line"><span class="string">            new_X(ndarray): train_data (2, N_train)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        N_x = X.shape[<span class="number">0</span>]</span><br><span class="line">        bias_dim = np.ones((N_x, <span class="number">1</span>))</span><br><span class="line">        new_X = np.c_[X, bias_dim].T</span><br><span class="line">        <span class="keyword">return</span> new_X</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss_fn</span>(<span class="params">X, y, N</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        calculate RMSE</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        rmse = np.sqrt(np.<span class="built_in">sum</span>((X.T @ w - y) ** <span class="number">2</span>) / N)</span><br><span class="line">        <span class="keyword">return</span> rmse</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">    X_train = add_dim_bias(X_train)</span><br><span class="line">    X_test = add_dim_bias(X_test)</span><br><span class="line">    N_train = X_train.shape[<span class="number">1</span>]</span><br><span class="line">    N_test = X_test.shape[<span class="number">1</span>]</span><br><span class="line">    w = np.linalg.inv(X_train @ X_train.T + _<span class="keyword">lambda</span> * np.eye(X_train.shape[<span class="number">0</span>])) @ X_train @ y_train <span class="comment"># w(2, 1)</span></span><br><span class="line">    rmse_train = loss_fn(X_train, y_train, N_train)</span><br><span class="line">    rmse_test = loss_fn(X_test, y_test, N_test)</span><br><span class="line"></span><br><span class="line">    plt.figure()</span><br><span class="line">    x = np.linspace(X_train[<span class="number">0</span>,:].<span class="built_in">min</span>(), X_train[<span class="number">0</span>,:].<span class="built_in">max</span>(), <span class="number">100</span>)</span><br><span class="line">    x = add_dim_bias(x)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(X_train.shape[<span class="number">1</span>]):</span><br><span class="line">        plt.scatter(X_train[<span class="number">0</span>,i], y_train[i, <span class="number">0</span>], marker = <span class="string">&quot;o&quot;</span>, color = <span class="string">&quot;black&quot;</span>)</span><br><span class="line">    plt.plot(x[[<span class="number">0</span>]].flatten(), (w.T @ x).flatten(), color = <span class="string">&quot;blue&quot;</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;linear_features&quot;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> rmse_train, rmse_test</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main_a</span>():</span></span><br><span class="line">    rmse_train, rmse_test = linear_features(X_train, X_test, y_train, y_test, _<span class="keyword">lambda</span> = <span class="number">0.01</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;root mean squared error of the training data: &quot;</span>, rmse_train)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;root mean squared error of the test data: &quot;</span>,rmse_test)</span><br></pre></td></tr></table></figure><br>(b):<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">polynomial_features</span>(<span class="params">X_train, X_test, y_train, y_test, _lambda = <span class="number">0.01</span>, degrees = [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    param:</span></span><br><span class="line"><span class="string">        X_train(ndarray): shape(N, 1)</span></span><br><span class="line"><span class="string">        y_train(ndarray): shape(N, 1)</span></span><br><span class="line"><span class="string">        _lambda(floar): ridge coefficient</span></span><br><span class="line"><span class="string">        degrees(list): list of degrees</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_degree</span>(<span class="params">X, degree</span>):</span></span><br><span class="line">        N = X.shape[<span class="number">0</span>]</span><br><span class="line">        new_X = np.ones((N, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, degree+<span class="number">1</span>):</span><br><span class="line">            temp = X ** i</span><br><span class="line">            new_X = np.c_[new_X, temp]</span><br><span class="line">        new_X = new_X.T </span><br><span class="line">        <span class="keyword">return</span> new_X</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss_fn</span>(<span class="params">w, X, y</span>):</span></span><br><span class="line">        N = <span class="built_in">len</span>(y)</span><br><span class="line">        rmse = np.sqrt(np.<span class="built_in">sum</span>((X.T @ w - y) ** <span class="number">2</span>) / N)</span><br><span class="line">        <span class="keyword">return</span> rmse</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    w_lst = []</span><br><span class="line">    rmse_train = []</span><br><span class="line">    rmse_test = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(degrees)):</span><br><span class="line">        tmp_X = add_degree(X_train, degrees[i])</span><br><span class="line">        tmp_w = np.linalg.inv(tmp_X @ tmp_X.T + _<span class="keyword">lambda</span> * np.eye(tmp_X.shape[<span class="number">0</span>])) @ tmp_X @ y_train</span><br><span class="line">        w_lst.append(tmp_w)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(degrees)):</span><br><span class="line">        tmp_X_train = add_degree(X_train, degrees[i])</span><br><span class="line">        tmp_X_test = add_degree(X_test, degrees[i])</span><br><span class="line">        tmp_rmse_train = loss_fn(w_lst[i], tmp_X_train, y_train)</span><br><span class="line">        tmp_rmse_test = loss_fn(w_lst[i], tmp_X_test, y_test)</span><br><span class="line">        rmse_train.append(tmp_rmse_train)</span><br><span class="line">        rmse_test.append(tmp_rmse_test)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(degrees)):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;polynomials of degrees=&#123;&#125;, root mean squared error of the training data is: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(degrees[i], rmse_train[i]))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(degrees)):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;polynomials of degrees=&#123;&#125;, root mean squared error of the test data is: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(degrees[i], rmse_test[i]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(w_lst)):</span><br><span class="line">        x = np.linspace(X_train[:,<span class="number">0</span>].<span class="built_in">min</span>(), X_train[:,<span class="number">0</span>].<span class="built_in">max</span>(), <span class="number">100</span>)</span><br><span class="line">        plt.figure()</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(X_train.shape[<span class="number">0</span>]):</span><br><span class="line">            plt.scatter(X_train[j,<span class="number">0</span>], y_train[j, <span class="number">0</span>], marker = <span class="string">&quot;o&quot;</span>, color = <span class="string">&quot;black&quot;</span>)</span><br><span class="line">        x = add_degree(x, degrees[i])</span><br><span class="line">        plt.plot(x[<span class="number">1</span>].flatten(), (w_lst[i].T @ x).flatten(), color = <span class="string">&quot;blue&quot;</span>)</span><br><span class="line">        plt.title(<span class="string">&quot;polynomial_features with degree = &#123;&#125;&quot;</span>.<span class="built_in">format</span>(degrees[i]))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main_b</span>():</span></span><br><span class="line">    polynomial_features(X_train, X_test, y_train, y_test)</span><br></pre></td></tr></table></figure><br>(c):<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bayesian_linear_regression</span>(<span class="params">X_train, y_train, X_test, y_test, mu = <span class="number">0</span>, sigma = <span class="number">0.1</span>, _lambda = <span class="number">0.01</span>, std = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span>):</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_dim_bias</span>(<span class="params">X</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        [x1, x2, x3, ... , xn] -&gt; [[x1,1], [x2,1], ... ,[xn,1]]</span></span><br><span class="line"><span class="string">        param:</span></span><br><span class="line"><span class="string">            X(ndarray): train_data (N_train, 1)</span></span><br><span class="line"><span class="string">        return:</span></span><br><span class="line"><span class="string">            new_X(ndarray): train_data (2, N_train)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        N_x = X.shape[<span class="number">0</span>]</span><br><span class="line">        bias_dim = np.ones((N_x, <span class="number">1</span>))</span><br><span class="line">        new_X = np.c_[X, bias_dim].T</span><br><span class="line">        <span class="keyword">return</span> new_X</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">prediction_mu_sigma</span>(<span class="params">X</span>):</span></span><br><span class="line">        <span class="comment">#X_bias = add_dim_bias(X)#(2,50)</span></span><br><span class="line">        mu_or_pred = X.T @ w<span class="comment">#(50,1)</span></span><br><span class="line">        sigma_2 = <span class="number">1</span> / beta + X.T @ np.linalg.inv(alpha * np.eye(X.shape[<span class="number">0</span>]) + beta *</span><br><span class="line">                                                          X @ X.T) @ X</span><br><span class="line">        <span class="comment"># Here take attention</span></span><br><span class="line">        sigma_2 += sigma * np.eye(sigma_2.shape[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">return</span> mu_or_pred, np.sqrt(sigma_2.diagonal())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss_fn</span>(<span class="params">X, y</span>):</span></span><br><span class="line">        N = <span class="built_in">len</span>(y)</span><br><span class="line">        pred, _ = prediction_mu_sigma(X)</span><br><span class="line">        rmse = np.sqrt(np.<span class="built_in">sum</span>((pred - y) ** <span class="number">2</span>) / N)</span><br><span class="line">        <span class="keyword">return</span> rmse</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">log_likelihood_fn</span>(<span class="params">X, y</span>):</span></span><br><span class="line">        n = <span class="built_in">len</span>(y)</span><br><span class="line">        log_likelihood = n / <span class="number">2</span> * (np.log(beta) - np.log(<span class="number">2</span> * np.pi)) - beta / <span class="number">2</span> * (np.linalg.norm(y - w.T @ X) ** <span class="number">2</span>)</span><br><span class="line">        average_ll = log_likelihood / n</span><br><span class="line">        <span class="keyword">return</span> average_ll</span><br><span class="line">        </span><br><span class="line">    </span><br><span class="line">    beta = <span class="number">1</span> / (sigma ** <span class="number">2</span>)</span><br><span class="line">    alpha = _<span class="keyword">lambda</span> * beta</span><br><span class="line">    X_train = add_dim_bias(X_train)<span class="comment">#(2,50)   y_train (50,1)</span></span><br><span class="line">    X_test = add_dim_bias(X_test)<span class="comment">#(2,100)   y_test  (100,1)</span></span><br><span class="line"></span><br><span class="line">    w = np.linalg.inv(X_train @ X_train.T + _<span class="keyword">lambda</span> * np.eye(X_train.shape[<span class="number">0</span>])) @ X_train @ y_train <span class="comment"># w(2, 1)</span></span><br><span class="line"></span><br><span class="line">    rmse_train = loss_fn(X_train, y_train)</span><br><span class="line">    rmse_test = loss_fn(X_test, y_test)</span><br><span class="line"></span><br><span class="line">    log_ll_train = log_likelihood_fn(X_train, y_train)</span><br><span class="line">    log_ll_test = log_likelihood_fn(X_test, y_test)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;the RMSE of the train data is: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(rmse_train))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;the RMSE of the test data is: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(rmse_test))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;the average log-likelihood of the train data is: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(log_ll_train))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;the average log-likelihood of the test data is: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(log_ll_test))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    pred, x_std = prediction_mu_sigma(X_train)</span><br><span class="line">    </span><br><span class="line">    plt.figure()</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(X_train.shape[<span class="number">1</span>]):</span><br><span class="line">        plt.scatter(X_train[<span class="number">0</span>,j], y_train[j, <span class="number">0</span>], marker = <span class="string">&quot;o&quot;</span>, color = <span class="string">&quot;black&quot;</span>)</span><br><span class="line">    plt.plot(X_train[<span class="number">0</span>], pred.flatten(), color = <span class="string">&quot;blue&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(std)):</span><br><span class="line">        plt.fill_between(X_train[<span class="number">0</span>], pred.flatten()+std[i]*x_std, pred.flatten()-std[i]*x_std,</span><br><span class="line">                          color = <span class="string">&quot;blue&quot;</span>, alpha = <span class="number">0.2</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;bayesian_linear_regression&quot;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main_c</span>():</span></span><br><span class="line">    bayesian_linear_regression(X_train, y_train, X_test, y_test)</span><br></pre></td></tr></table></figure><br>(d):<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">squared_exponential_features</span>(<span class="params">X_train, y_train, X_test, y_test, _lambda = <span class="number">0.01</span>, k = <span class="number">20</span>, sigma = <span class="number">0.1</span>, beta = <span class="number">10</span>, std = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">new_data</span>(<span class="params">X</span>):</span></span><br><span class="line">        n = <span class="built_in">len</span>(X)</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        new_x = np.zeros((n, k))</span></span><br><span class="line"><span class="string">        for i in range(n):</span></span><br><span class="line"><span class="string">            for j in range(k):</span></span><br><span class="line"><span class="string">                alpha_j = j * 0.1 - 1</span></span><br><span class="line"><span class="string">                tmp = np.exp(-0.5 * beta * (X[i] - alpha_j) ** 2)</span></span><br><span class="line"><span class="string">                new_x[i][j] = tmp</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        X_poly = np.ones(X.shape)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, k+<span class="number">1</span>):</span><br><span class="line">                X_poly = np.hstack((X_poly, np.exp(-beta/<span class="number">2</span>*np.power(X-(j*<span class="number">0.1</span> -<span class="number">1</span>), <span class="number">2</span>))))</span><br><span class="line">        <span class="keyword">return</span> X_poly.T</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">prediction_mu_sigma</span>(<span class="params">X</span>):</span></span><br><span class="line">        mu_or_pred = X.T @ w</span><br><span class="line">        sigma_2 = <span class="number">1</span> / beta + X.T @ np.linalg.inv(alpha * np.eye(X.shape[<span class="number">0</span>]) + beta *</span><br><span class="line">                                                          X @ X.T) @ X</span><br><span class="line">        sigma_2 += sigma * np.eye(sigma_2.shape[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">return</span> mu_or_pred, np.sqrt(sigma_2.diagonal())</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss_fn</span>(<span class="params">X, y</span>):</span></span><br><span class="line">        N = <span class="built_in">len</span>(y_test)</span><br><span class="line">        pred, _ = prediction_mu_sigma(X)</span><br><span class="line">        rmse = np.sqrt(np.<span class="built_in">sum</span>((pred - y) ** <span class="number">2</span>) / N)</span><br><span class="line">        <span class="keyword">return</span> rmse</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">log_likelihood_fn</span>(<span class="params">X, y</span>):</span></span><br><span class="line">        _beta = <span class="number">1</span> / (sigma ** <span class="number">2</span>)</span><br><span class="line">        n = <span class="built_in">len</span>(y)</span><br><span class="line">        log_likelihood = n / <span class="number">2</span> * (np.log(_beta) - np.log(<span class="number">2</span> * np.pi)) - _beta / <span class="number">2</span> * (np.linalg.norm(y - w.T @ X) ** <span class="number">2</span>)</span><br><span class="line">        average_ll = log_likelihood / n</span><br><span class="line">        <span class="keyword">return</span> average_ll    </span><br><span class="line"></span><br><span class="line">    orignal_data = X_train.copy()</span><br><span class="line">    X_train = new_data(X_train)</span><br><span class="line">    X_test = new_data(X_test)</span><br><span class="line">    alpha = _<span class="keyword">lambda</span> * beta</span><br><span class="line">    w = np.linalg.pinv(X_train @ X_train.T + _<span class="keyword">lambda</span> * np.eye(X_train.shape[<span class="number">0</span>])) @ X_train @ y_train</span><br><span class="line">    rmse_train = loss_fn(X_train, y_train)</span><br><span class="line">    rmse_test = loss_fn(X_test, y_test)</span><br><span class="line"></span><br><span class="line">    log_ll_train = log_likelihood_fn(X_train, y_train)</span><br><span class="line">    log_ll_test = log_likelihood_fn(X_test, y_test)</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;the RMSE of the train data is: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(rmse_train))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;the RMSE of the test data is: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(rmse_test))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;the average log-likelihood of the train data is: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(log_ll_train))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;the average log-likelihood of the test data is: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(log_ll_test))</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    pred, x_std = prediction_mu_sigma(X_train)</span><br><span class="line">    sorted_idx = np.argsort(X_train[<span class="number">0</span>])</span><br><span class="line">    orignal_data = orignal_data[sorted_idx]</span><br><span class="line">    pred = pred.flatten()[sorted_idx]</span><br><span class="line">    y_train = y_train[sorted_idx]</span><br><span class="line">    plt.figure()</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(orignal_data.shape[<span class="number">0</span>]):</span><br><span class="line">        plt.scatter(orignal_data[j, <span class="number">0</span>], y_train[j, <span class="number">0</span>], marker = <span class="string">&quot;o&quot;</span>, color = <span class="string">&quot;black&quot;</span>)</span><br><span class="line">    plt.plot(orignal_data[:,<span class="number">0</span>], pred.flatten(), color = <span class="string">&quot;blue&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(std)):</span><br><span class="line">        plt.fill_between(orignal_data[<span class="number">0</span>], pred.flatten()+std[i]*x_std, pred.flatten()-std[i]*x_std,</span><br><span class="line">                          color = <span class="string">&quot;blue&quot;</span>, alpha = <span class="number">0.2</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;bayesian_linear_regression&quot;</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></p>
<p>效果图如下：<br><img src="https://img-blog.csdnimg.cn/20210716232731384.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20210716232742944.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20210716232759263.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
]]></content>
      <categories>
        <category>学习总结</category>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>分类问题（Classification）</title>
    <url>/posts/10.html</url>
    <content><![CDATA[<h1 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h1><p>开始前先回顾一下贝叶斯决策理论。<br><span id="more"></span><br>给定观测变量$x$，我们要找到类别$C_k$的后验概率，有如下关系：<br><img src="https://img-blog.csdnimg.cn/20210717001003505.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>决策的条件是：<br>    如果$p(C_1|x)&gt;p(C_2|x)$，则选择$C_1$，否则$C_2$。用上面的条件概率则可以写成：<br> <img src="https://img-blog.csdnimg.cn/20210717001233629.png#pic_center" alt="在这里插入图片描述"><br>遵从这一规则的分类器称为贝叶斯最优分类器（Bayes optimal classifier）。 \<br>以前我们要计算计算这个分类后验的话，需要根据贝叶斯公式，建立模型并估计条件密度$p(x|C_k)$和类别先验$p(C_k)$，通过最大化$p(C_k|x)$来使错误概率最小。\<br>但现在我们不再需要对密度进行建模，而是直接编码决策边界，然后最小化错误概率。</p>
<h1 id="判别函数（Discriminant-Functions）"><a href="#判别函数（Discriminant-Functions）" class="headerlink" title="判别函数（Discriminant Functions）"></a>判别函数（Discriminant Functions）</h1><h2 id="基本知识"><a href="#基本知识" class="headerlink" title="基本知识"></a>基本知识</h2><p>大概就是通过比较法对数据进行分类。现有数据$x$，以及$K$个类别，对应有$K$个判别函数：<script type="math/tex">y_1(x),...,y_k(x)</script>当<script type="math/tex">y_k(x)>y_j(x)</script>时，将数据$x$分类给$C_k$。<br>举个例子，根据贝叶斯分类器，判别函数可有如下几种形式：<script type="math/tex">y_k(x)=p(C_k|x)</script> <script type="math/tex">y_k(x)=p(x|C_k)p(C_k)</script> <script type="math/tex">y_k(x)=\log(x|C_k)+\log p(C_k)</script><br>再举个例子，当$K=2$，即是说只有两个类别时，判断为类别$1$的标准有以下几种形式：<script type="math/tex">y_1(x)>y_2(x)</script> <script type="math/tex">y(x)>0</script> 其中$y(x)$可以为以下两种形式：<script type="math/tex">y(x)=p(C_1|x)-p(C_2|x)</script> <script type="math/tex">y(x)=\log \frac{p(x|C_1)}{p(x|C_2)}+\log \frac{p(C_1)}{p(C_2)}</script></p>
<h2 id="线性判别函数（Linear-Discriminant-Functions）"><a href="#线性判别函数（Linear-Discriminant-Functions）" class="headerlink" title="线性判别函数（Linear Discriminant Functions）"></a>线性判别函数（Linear Discriminant Functions）</h2><h3 id="二分类"><a href="#二分类" class="headerlink" title="二分类"></a>二分类</h3><p>在线性判别器中，决策面是（超）平面，其线性决策方程为：</p>
<script type="math/tex; mode=display">y(x)=w^Tx+w_0$$ 此处$w$是法向量，$w_0$是偏置。
假设数据维度是2的话，其图示为：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210717033655649.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center)
上图那些写得复复杂杂的东西就是在计算距离。稍微写得清楚一点的话就是，假设现有一点$x_0$，其到分离（超）平面的距离为：$r=\frac{|w*x_0+b|}{||w||}$，其中$||w||$为2范数。

### 多分类
对于多分类，如果我们只是简单地运用二分类地决策规则，则可能会产生争议数据（区间），比如说下面两种情况：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210717034936687.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center)
运用判别函数则可以知道每个数据属于某一标签的”可信度“，毕竟”距离“有长有短容易比较。其实跟上面的二分类类似，只是这里比较的东西多一点。如下图：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210717035308951.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center)
如果判别函数是线性的，则决策区域是互相连接且是凸的。

# Fisher 判别分析（Fisher Discriminant Analysis）
Fisher线性判别分析是线性判别分析（LDA模型）的一种，它的目的大概就是：给定一个投影向量$w$，将数据$x$投影到$w$向量上，使得不同类别的$x$投影后的值$y$尽可能互相分开。

## 一个例子用作引出
在讲解这个判别分析前先来看个例子：
以二维数据为例，也就是上面提过的：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210717040640731.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center)
其数据形式以及分类标准如下：$$y(x)=+1 \leftrightarrow x\in C_1$$ $$y(x)=-1 \leftrightarrow x\in C_2$$ 训练输入：$X=\left\{x_1 \in \mathcal{R}^d,...,x_n\right\}$\
训练标签：$Y=\left\{y_1\in[-1,+1],...,y_n\right\}$\
**最小二乘法**\
我们先尝试用最小二乘法。\
先写出线性判别函数：$$x_i^Tw+w_0=y_i,\forall i=1,...,n$$每个训练数据点及标签都有一个线性方程。\
把偏置$w_0$整合进权重$w$里比较好看，训练数据点也要因此多一个维度，即是：$$\hat{x}_i=(x_i \quad1)^T\in\mathcal{R}^{d\times1}$$ $$\hat{w}=(w \quad w_0)^T\in\mathcal{R}^{d\times1}$$所以方程改成：$$\hat{x}_i^T\hat{w}=y_i,\quad \forall i=1,...,n$$表示成矩阵向量形式则如下：$$\hat{X}^T\hat{w}=y$$此处$\hat{X}=[\hat{x}_1,...,\hat{x}_n]\in \mathcal{R}^{d\times n},\quad y=[y_1,...,y_n]^T$\
这里有$n$个方程和$d+1$个未知数，属于超定方程（因为一般来说$n$总是大于$d+1$的）。\
用最小二乘法求解如下：
![在这里插入图片描述](https://img-blog.csdnimg.cn/2021071704255380.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center)
这部分和上一篇的线性回归里的是一样的。
用最小二乘法有个问题，就是这种方法对于异常值特别敏感，下面两幅图就很直观了：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210717042818947.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center)

## 正式开始
对数据进行投影，这有一幅形象的图：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210717043324300.png#pic_center)
这里大抵还是跟前面类似，不过换个名字比较好说明。\
投影（Projection）：$y=w^Tx$\
对照阈值进行分析：$w^Tx\ge -w_0$ 或$w^Tx+w_0\ge0$\
怎样的投影$w$才是一个好的投影呢？可以最大限度地扩大两类之间的 "距离"，以实现良好的分离。那怎么做？

### 尝试最大化两个类的均值（效果不好）
首先我们可以试试两个类的均值，让其距离最大：
![在这里插入图片描述](https://img-blog.csdnimg.cn/2021071704401127.png#pic_center)
将均值投影到一维线上，如下：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210717044207217.png#pic_center)
然后最大化两均值距离的平方：$$\max(m_1-m_2)^2$$即是：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210717044425121.png#pic_center)
这里据说有个明显的问题，这个距离会随着$w$的norm的增加而无限增加，为了限制这种情况，我们加个约束条件，就有了如下式子：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210717045331674.png#pic_center)
这是一个带约束条件的优化问题，第一想法肯定是用拉格朗日了，加入拉格朗日算子，各种计算，就有下面的过程：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210717045515526.png#pic_center)
解出：$$w=\frac{m_1-m_2}{||m_1-m_2||}</script><p>我们能得出下图的结果：<br><img src="https://img-blog.csdnimg.cn/20210717045650708.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>这又有个问题，由图可以看出，两个类有很大一部分重合了。下面这个图才是我们想要的：<br><img src="https://img-blog.csdnimg.cn/20210717045819935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>但要怎么做？<br>一波自问自答：在尽可能远得分离两均值得同时，也要让每个类得方差尽量小。</p>
<h3 id="同时考虑均值和方差（正式引入Fisher线性判别法）"><a href="#同时考虑均值和方差（正式引入Fisher线性判别法）" class="headerlink" title="同时考虑均值和方差（正式引入Fisher线性判别法）"></a>同时考虑均值和方差（正式引入Fisher线性判别法）</h3><p>定义每个类的方差：<br><img src="https://img-blog.csdnimg.cn/20210717050730975.png#pic_center" alt="在这里插入图片描述"><br>然后是Fisher标准：<br><img src="https://img-blog.csdnimg.cn/20210717050818222.png#pic_center" alt="在这里插入图片描述"><br>将分子展开：<br><img src="https://img-blog.csdnimg.cn/20210717050930288.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>这就引入了类别间的协方差。<br>然后再将分母展开：<br><img src="https://img-blog.csdnimg.cn/2021071705115256.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>综合以上两个展开，Fisher标准可以写成：<br><img src="https://img-blog.csdnimg.cn/20210717051310157.png#pic_center" alt="在这里插入图片描述"><br>对$w$进行微分并等于0则有：<script type="math/tex">(w^TS_Bw)S_Ww=(w^TS_Ww)S_Bw</script><br>在这里$(w^TS_Bw)$和$(w^TS_Ww)$都是标量，所以有：<script type="math/tex">S_Ww ||S_Bw</script>这里$||$表共线（collinearity）。<br>然后由上面的式子我们可以知道：<script type="math/tex">S_Bw=(m_1-m_2)(m_1-m_2)^Tw\rightarrow S_Bw||(m_1-m_2)</script>(还是因为标量的关系所以共线)。<br>同理可得：<script type="math/tex">S_Ww\quad ||\quad(m_1-m_2)</script> <script type="math/tex">w\quad || \quad S_W^{-1}(m_1-m_2)</script><br>所以就引出了Fisher 线性判别（Fisher’s Linear Discriminant）：<script type="math/tex">w\propto S_W^{-1}(m_1-m_2)</script><br>这个线性判别只提供了投影，我们还需要找到阈值，方法有不少，比如贝叶斯分类器和高斯分类条件。\<br>当类条件分布相等，且协方差是对角矩阵，则Fisher线性判别是贝叶斯最优的。\<br>值得注意的是，Fisher线性判别法是最小二乘法的一个特例。另外，这种方法对于噪声依旧很敏感</p>
<h1 id="感知器算法（Perceptron-Algorithm）"><a href="#感知器算法（Perceptron-Algorithm）" class="headerlink" title="感知器算法（Perceptron Algorithm）"></a>感知器算法（Perceptron Algorithm）</h1><p>感知器算法是一种可以直接得到线性判别函数的线性分类方法，是基于样本线性可分的要求下使用的。\<br>如果我们的类别线性可分，则我们肯定可以找到一个分离（超）平面。\<br><strong>感受器判别函数（Perceptron discriminant function）</strong>为：<script type="math/tex">y(x)=sign(w^Tx+b)</script>此处的$sign(x)=\left\{+1,x&gt;0;0,x=0;-1,x&lt;0\right\}$\<br>即是下图的函数：<br><img src="https://img-blog.csdnimg.cn/20210717065852632.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>感知器算法的基本步骤如下：<br><img src="https://img-blog.csdnimg.cn/20210717070135533.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br> 现在就出现了一个优化问题了，这里最直接的想法是计算误分类的点总数，但这种方法不好微分，就用另一种，计算点到分离（超）平面的距离，就引出了下面这个优化函数：<br><img src="https://img-blog.csdnimg.cn/20210717170805416.png#pic_center" alt="在这里插入图片描述"><br>在很多地方更常见的是计算最小值，但都一样，只是他们事先加了个负号，这里没加。稍微解释一下，对于误分类的点，比如点$x_i$，不管是把这个点误分到了哪一类，$y_i<em>(wx_i+b)$都是小于0的，毕竟如果大于0就意味着同号即是分类正确了。\<br>把下面那个式子写完全的话应该是这样子：$$\sum_{x\in X:\left\langle w,x \right\rangle &lt;0}\left\langle w,x \right\rangle =\sum_{…}\frac{1}{||w||}y_i</em>(wx_i+b)<script type="math/tex">用gradient的方法求解如下：</script>\frac{\partial J}{\partial w}=\sum_{…}x$$</p>
<blockquote>
<p>注意，经典的感知器只能做线性二分类。</p>
</blockquote>
<h1 id="逻辑回归（Logistic-Regression）"><a href="#逻辑回归（Logistic-Regression）" class="headerlink" title="逻辑回归（Logistic Regression）"></a>逻辑回归（Logistic Regression）</h1><p>（这部分接触过很多，就简单记了）</p>
<h2 id="Generative-vs-Discriminative"><a href="#Generative-vs-Discriminative" class="headerlink" title="Generative vs. Discriminative"></a>Generative vs. Discriminative</h2><p>对解决分类问题一般有两种方向，Generative 和 Discriminative，这里对两种方法做个大概的对比：</p>
<blockquote>
<p>Generative modelling:<br>我们先构造条件分布模型$p(x|C_2)$和$p(x|C_1)$<br>运用贝叶斯规则计算其后验分布<br>例子：Navie Bayes</p>
<p>Discriminative modelling:<br>我们直接构造类的后验比如说$p(C_1|x)$<br>我们只关心分类是否正确，而不关心是否符合类的条件。<br>例子：逻辑回归</p>
</blockquote>
<h2 id="概率判别模型（Probabilistic-Discriminative-Models）"><a href="#概率判别模型（Probabilistic-Discriminative-Models）" class="headerlink" title="概率判别模型（Probabilistic Discriminative Models）"></a>概率判别模型（Probabilistic Discriminative Models）</h2><h3 id="Sigmoid函数的引出"><a href="#Sigmoid函数的引出" class="headerlink" title="Sigmoid函数的引出"></a>Sigmoid函数的引出</h3><p>运用贝叶斯定理：<br><img src="https://img-blog.csdnimg.cn/20210717180508708.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>其图像如下，呈$S$型，将传入的实数压在$[0,1]$区间内。<br><img src="https://img-blog.csdnimg.cn/20210717180723404.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>假设里面这个$\alpha$是线性的话（当然也可以是其它），后验就可以写为：<script type="math/tex">p(C_1|x)=\sigma(w^Tx+w_0)</script>然后就是求权重和偏置了。<br>求这两个东西的方法也有不少，印象中梯度下降应该是比较常见的，因为太常见了，所以这里不记录，而是记录最大似然：<br><img src="https://img-blog.csdnimg.cn/20210717182341543.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h1 id="（附）作业相关代码"><a href="#（附）作业相关代码" class="headerlink" title="（附）作业相关代码"></a>（附）作业相关代码</h1><p><img src="https://img-blog.csdnimg.cn/2021071718244386.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_split_data</span>(<span class="params">path</span>):</span></span><br><span class="line">    data = np.loadtxt(path)</span><br><span class="line">    C_1 = data[:<span class="number">50</span>]</span><br><span class="line">    C_2 = data[<span class="number">50</span>:<span class="number">93</span>]</span><br><span class="line">    C_3 = data[<span class="number">93</span>:]</span><br><span class="line">    <span class="keyword">return</span> C_1, C_2, C_3, data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_discriminant_analysis</span>(<span class="params">c_1, c_2</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    params:</span></span><br><span class="line"><span class="string">        c_1(ndarray): (n, 2)</span></span><br><span class="line"><span class="string">        c_2(ndarray): (n, 2)</span></span><br><span class="line"><span class="string">    return:</span></span><br><span class="line"><span class="string">        w(ndarray): (2,1)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    n1, d1 = c_1.shape</span><br><span class="line">    n2, d2 = c_2.shape</span><br><span class="line">    m1 = np.mean(c_1, axis=<span class="number">0</span>)<span class="comment">#(1,2)</span></span><br><span class="line">    m2 = np.mean(c_2, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    sw_1 = np.zeros((d1,d1))</span><br><span class="line">    sw_2 = np.zeros((d2,d2))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n1):</span><br><span class="line">        temp_sw = (c_1[[i]] - m1).T @ (c_1[[i]] - m1)</span><br><span class="line">        sw_1 += temp_sw</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n2):</span><br><span class="line">        temp_sw = (c_2[[j]] - m2).T @ (c_2[[j]] - m2)</span><br><span class="line">        sw_2 += temp_sw</span><br><span class="line">    sw = sw_1 + sw_2</span><br><span class="line">    inv_sw = np.linalg.inv(sw)</span><br><span class="line">    w = inv_sw @ (m1 - m2).T<span class="comment">#(2,1)</span></span><br><span class="line">    <span class="keyword">return</span> w</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decide</span>(<span class="params">w, x, m1, m2, N1, N2</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    params:</span></span><br><span class="line"><span class="string">        w(ndarray): weight, (2,1)</span></span><br><span class="line"><span class="string">        x(ndarray): data, (1,2)</span></span><br><span class="line"><span class="string">        m1(ndarray): mean of class1, (1,2)</span></span><br><span class="line"><span class="string">        m2(ndarray): mean of class2, (1,2)</span></span><br><span class="line"><span class="string">        N1(int): nums of class1</span></span><br><span class="line"><span class="string">        N2(int): nums of class2</span></span><br><span class="line"><span class="string">    return:</span></span><br><span class="line"><span class="string">        True: c1</span></span><br><span class="line"><span class="string">        False: c2</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    y = w.T @ x.T</span><br><span class="line">    <span class="comment">#w0 = (N1 * (w.T @ m1.T) + N2 * (w.T @ m2.T)) / (N1 + N2)</span></span><br><span class="line">    w0 = (w.T @ m1.T + w.T @ m2.T) / <span class="number">2</span> </span><br><span class="line">    <span class="keyword">if</span> y &gt; w0:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    path = <span class="string">&quot;./dataSets/ldaData.txt&quot;</span></span><br><span class="line">    C_1, C_2, C_3, data = load_split_data(path)</span><br><span class="line"></span><br><span class="line">    w_12 = linear_discriminant_analysis(C_1, C_2)</span><br><span class="line">    w_23 = linear_discriminant_analysis(C_2, C_3)</span><br><span class="line">    w_13 = linear_discriminant_analysis(C_1, C_3)</span><br><span class="line"></span><br><span class="line">    c1 = []</span><br><span class="line">    c2 = []</span><br><span class="line">    c3 = []</span><br><span class="line"></span><br><span class="line">    N1 = <span class="built_in">len</span>(C_1)</span><br><span class="line">    N2 = <span class="built_in">len</span>(C_2)</span><br><span class="line">    N3 = <span class="built_in">len</span>(C_3)</span><br><span class="line"></span><br><span class="line">    m1 = np.mean(C_1, axis=<span class="number">0</span>)</span><br><span class="line">    m2 = np.mean(C_2, axis=<span class="number">0</span>)</span><br><span class="line">    m3 = np.mean(C_3, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    N, d = data.shape</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">        <span class="keyword">if</span> decide(w_12, data[i], m1, m2, N1, N2):</span><br><span class="line">            <span class="comment"># not c2</span></span><br><span class="line">            <span class="keyword">if</span> decide(w_13, data[i], m1, m3, N1, N3):</span><br><span class="line">                <span class="comment"># c1</span></span><br><span class="line">                c1.append(data[i])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># c3</span></span><br><span class="line">                c3.append(data[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># not c1</span></span><br><span class="line">            <span class="keyword">if</span> decide(w_23, data[i], m2, m3, N2, N3):</span><br><span class="line">                <span class="comment"># c2</span></span><br><span class="line">                c2.append(data[i])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># c3</span></span><br><span class="line">                c3.append(data[i])</span><br><span class="line"></span><br><span class="line">    <span class="comment">#plot orignal_data</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(C_1)):</span><br><span class="line">        plt.scatter(C_1[i, <span class="number">0</span>], C_1[i, <span class="number">1</span>], marker = <span class="string">&quot;v&quot;</span>, color = <span class="string">&quot;r&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(C_2)):</span><br><span class="line">        plt.scatter(C_2[i, <span class="number">0</span>], C_2[i, <span class="number">1</span>], marker = <span class="string">&quot;x&quot;</span>, color = <span class="string">&quot;y&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(C_3)):</span><br><span class="line">        plt.scatter(C_3[i, <span class="number">0</span>], C_3[i, <span class="number">1</span>], marker = <span class="string">&quot;o&quot;</span>, color = <span class="string">&quot;b&quot;</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;orignal_data&quot;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">#plot LDA</span></span><br><span class="line">    <span class="keyword">for</span> sub_data <span class="keyword">in</span> c1:</span><br><span class="line">        plt.scatter(sub_data[<span class="number">0</span>], sub_data[<span class="number">1</span>], marker = <span class="string">&quot;v&quot;</span>, color = <span class="string">&quot;r&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> sub_data <span class="keyword">in</span> c2:</span><br><span class="line">        plt.scatter(sub_data[<span class="number">0</span>], sub_data[<span class="number">1</span>], marker = <span class="string">&quot;x&quot;</span>, color = <span class="string">&quot;y&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> sub_data <span class="keyword">in</span> c3:</span><br><span class="line">        plt.scatter(sub_data[<span class="number">0</span>], sub_data[<span class="number">1</span>], marker = <span class="string">&quot;o&quot;</span>, color = <span class="string">&quot;b&quot;</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;LDA&quot;</span>)</span><br><span class="line">    plt.show()    </span><br><span class="line">            </span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure></p>
<p>效果如下：<br><img src="https://img-blog.csdnimg.cn/20210717182742175.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20210717182739618.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
]]></content>
      <categories>
        <category>学习总结</category>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>线性降维与统计学习理论（Linear Dimensionality Reduction &amp; Statistical Learning Theory）</title>
    <url>/posts/11.html</url>
    <content><![CDATA[<h1 id="线性降维（Linear-Dimensionality-Reduction）（以PCA为例）"><a href="#线性降维（Linear-Dimensionality-Reduction）（以PCA为例）" class="headerlink" title="线性降维（Linear Dimensionality Reduction）（以PCA为例）"></a>线性降维（Linear Dimensionality Reduction）（以PCA为例）</h1><h2 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h2><p>主成分分析（Principal Component Analysis，PCA）是最常见的线性降维方法。<br><span id="more"></span><br>拿之前的线性回归举例，对于最小二乘法的线性回归，其求解参数为：<script type="math/tex">\hat{w}=(\hat{X}\hat{X})^{-1}\hat{X}y</script>其中$\hat{X}\in \mathcal{R}^{d\times n}$，$y\in \mathcal{R}^{n\times 1}$。<br>若直接求解$d\times d$的逆矩阵，其复杂度为$O(d^3)$，所以我们就想找到一个新的维度$d^{new}$，使其远小于原维度，即$d^{new}&lt;&lt;d$，但又不能对结果造成很大的影响。<br>这就引出了PCA，我们要抓住数据的”本质“。</p>
<h2 id="具体讲解"><a href="#具体讲解" class="headerlink" title="具体讲解"></a>具体讲解</h2><p>现假设我们的原始数据$X=\left\{x_1,…,x_n\right\}$，以第$i$个数据为例，其维度为：<script type="math/tex">x^i\in \mathcal{R}^M</script><br>然后这第$i$个数据的低纬度表示法为：<script type="math/tex">a^i\in \mathcal{R}^D, \quad D<<M</script>现在要做的就是找到个映射满足：<script type="math/tex">x^i \rightarrow a^i</script><br>现在我们限制这样的映射为线性方程，即：<script type="math/tex">a^i=Bx^i, \quad B\in \mathcal{R}^{D\times M}</script><br>———————————————————————————————————————</p>
<p>在继续之前先补充一个知识点，即是说，任何向量我们都可以改写成下面这样：<script type="math/tex">\pmb{x}=\sum_{i=1}^{M}a_i\pmb{u}_i, \quad其中 \pmb{u}_i^T\pmb{u}_j=\delta_{ij}</script><br>当$i=j$时，$\delta_{ij}=1$，否则等于0，$u_i$与$u_j$正交。举个例子如下：<br><img src="https://img-blog.csdnimg.cn/20210717210412307.png#pic_center" alt="在这里插入图片描述"><br>其中标量参数$a$可作如下表述：<br><img src="https://img-blog.csdnimg.cn/20210717213720773.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>也就是说，这个$a_i$是一个投影结果。</p>
<p>————————————————————————————————————————</p>
<p>所以原始向量我们可以写成如下两部分相加的形式：<script type="math/tex">x^n=\sum_{i=1}^{D}a_i\pmb{u}_i+\sum_{j=D+1}^{M}b_j\pmb{u}_j\approx\tilde{x}^n</script>其中第一部分就是我们想要的降维后的数据，第二部分是一些$error$组成的数据。这里的$\tilde{x}^n$是我们降维之后的数据，也就是第一部分。<br>我们选择$D$的标准是最小化数据的均方差，即是：<br><img src="https://img-blog.csdnimg.cn/2021071722173678.png#pic_center" alt="在这里插入图片描述"><br>我们现在要最小化这个$error$，首先对$error$进行改写：<br><img src="https://img-blog.csdnimg.cn/20210717230441723.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>把上图的结论写下来：<script type="math/tex">E(u)=\sum_{n=1}^N||x^n||^2-(u^Tx^n)^2=\sum^N_{n=1}||x^n||^2-(a^n)^2</script><br>最小化这个函数就等同于最大化后面的那个部分，也可以称为最大化投影的方差，这也就是PCA的基本思想，可以想象成把数据投影到直线上（跟另一篇分类问题里提到的有些类似），然后让数据尽可能分开，也就是让方差尽量大。<br>所以现在我们有：<script type="math/tex">\max \frac{1}{N}\sum_{n=1}^{N}a_n^2</script>注意，此处我们是假设均值为0的情况，否则则用下面的式子：<script type="math/tex">\max \frac{1}{N}\sum_{n=1}^{N}(a_n-\mu)^2</script><br>但一般我们不会用上面这个，因为计算太麻烦，所以会事先让每个数据都减去均值，以保证零均值的情况，即是：<script type="math/tex">x^n-\overline{x}</script><br>现在就可以写出如下形式的方程了：<br><img src="https://img-blog.csdnimg.cn/2021071801252064.png#pic_center" alt="在这里插入图片描述"><br>根据这个图就能发现，把数据投影到$u_1$上能保留数据更多的信息。现在的问题就是如何找到这条轴，以及沿这条轴的数据的方差。<br>以下图为例：<br><img src="https://img-blog.csdnimg.cn/20210718012915445.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>这里$\lambda_i$是沿方向$u_i$的边际方差（marginal variance）。</p>
<p>现假设如下数据，<script type="math/tex">X=[x^1,...,x^n]\in \mathcal{R}^{M\times N}</script>是一个有$N$个$M$维向量的矩阵，即$x^i\in\mathcal{R}^M$。另$u\in\mathcal{R}^M$为一个输入空间的方向，则第$j$个向量$x^j$在这个向量$u$上的投影为：<script type="math/tex">a_j=u^Tx^j=\sum_{i=1}{M}X_{ij}u_i</script>目标是找到一个方向$u$，使所有输入向量的投影的方差达到最大。\<br>首先计算所有投影的均值：<script type="math/tex">\overline{a}=\frac{1}{N}\sum^N_{j=1}a_j=\frac{1}{N}\sum_{j=1}^{N}\sum_{i=1}^{M}X_{ij}u_i=\sum^M_{i=1}u_i\mu_i</script>其中$\mu_i=\frac{1}{N}\sum^{N}_{j=1}X_{ij}$<br>计算方差为：<br><img src="https://img-blog.csdnimg.cn/20210718045939294.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>上面的方差也可以用另一个推导，意思一样但比较简洁：<br><img src="https://img-blog.csdnimg.cn/20210718050457581.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p>其中$\pmb{C}$为协方差矩阵。在约束$||u||^2=1$下最大化这个方差，带约束的优化问题就得看拉格朗日了。如下：<br><img src="https://img-blog.csdnimg.cn/2021071805054020.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>然后就很神奇地出现了特征值-特征向量的等式，其中最大的特征值就是最大方差，其对应的特征向量就是最大方差的方向。\<br>根据上面的结论可得：$\pmb{CU}=\pmb{U}\Lambda$，其中$\Lambda=diag(\lambda_1,…,\lambda_m)$，$\pmb{U}=[\pmb{u_1,…,u_M}]$，分别为特征值构成的对角矩阵以及特征向量构成的矩阵</p>
<blockquote>
<p>注意，这里我们假设矩阵$\pmb{U}$是正交的，也就是范数为1，且这个对角矩阵的特征值已经按照从大到小排列了</p>
</blockquote>
<p>然后我们就能把协方差矩阵$\pmb{C}$进行分解：<script type="math/tex">\pmb{C}=\pmb{U}\Lambda\pmb{U}^{-1}$$$$\pmb{C}=\pmb{U}\Lambda\pmb{U}^{T}</script><br>如下图：<br><img src="https://img-blog.csdnimg.cn/20210718062453710.png#pic_center" alt="在这里插入图片描述"><br>根据协方差矩阵的定义我们还知道，矩阵$\pmb{C}$是一个正的实对称矩阵。\<br>对于极小（约等于0）的特征值以及其对应的特征向量，我们就直接舍去，因为他们对于矩阵的信息作用不大。借助提取的特征值我们就能重新构造新的数据集，如下：<br><img src="https://img-blog.csdnimg.cn/20210718063540655.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述">这种方法计算出来的满足 最小均方误差，即：<script type="math/tex">\min E(\pmb{u_1,...,u_D})=\sum_{n=1}^{N}||\pmb{x}^n-\tilde{\pmb{x}}^n||^2</script><br>现在这个降维过程就以及大致结束了，做个总结，其步骤大致是：</p>
<blockquote>
<ol>
<li>数据标准化（求解并减去均值）\</li>
<li>求解协方差矩阵并进行分解，提取前$D$个特征值对应的特征向量（特征值已从大到小排列）\</li>
<li>用如下方法重构数据：\<br><img src="https://img-blog.csdnimg.cn/20210718064537281.png#pic_center" alt="在这里插入图片描述"></li>
<li>有时也会将每个维度的方差归一.</li>
</ol>
</blockquote>
<p>现在还剩最后一个问题：怎么选择维度$D$？\<br>$D$越大的话近似效果越好，极端情况就是$D=M$，但这就没意义了。\<br>选择维度$D$一般有以下两种考量：</p>
<ol>
<li>看表现效果，当选取某维度达到我们需要的效果时，就选这个维度$D$；\</li>
<li>事先定一个值，比如0.9，然后按照下面公式选取所需维度：<br><img src="https://img-blog.csdnimg.cn/20210718065326292.png#pic_center" alt="在这里插入图片描述"></li>
</ol>
<h2 id="PCA的总结与应用"><a href="#PCA的总结与应用" class="headerlink" title="PCA的总结与应用"></a>PCA的总结与应用</h2><p>总结：</p>
<blockquote>
<p>PCA将数据映射到线性子空间内\<br>PCA最大化映射的方差\<br>PCA最小化重构误差</p>
</blockquote>
<p>应用：</p>
<blockquote>
<p>PCA允许我们将高维的输入空间转化为低维的特征空间，同时捕捉到数据的本质\<br>PCA为数据找到一个更自然的坐标系(more natural coordinate)\<br>PCA 常用于高维输入数据的预处理</p>
</blockquote>
<h1 id="统计学习理论（Statistical-Learning-Theory）"><a href="#统计学习理论（Statistical-Learning-Theory）" class="headerlink" title="统计学习理论（Statistical Learning Theory）"></a>统计学习理论（Statistical Learning Theory）</h1><p>统计学习理论的目的是：我们一开始并不知道正确的模型，而是需要从一组模型中选一个最优的。\<br>这里的最优指的是模型的泛化能力，即不能只在训练数据上误差最小。\<br>既然提到了这类优化，就得引入一个概念——风险（Risk）。<br>现在我们有一个损失函数：<script type="math/tex">L(y,f(x,w))</script>则其经验风险（Empirical risk）为：<script type="math/tex">R_{emp}(w)=\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i,w))</script>这里的$N$是数据点的数量。\<br>举个例子，对于二次方的损失函数，可表示为：<script type="math/tex">R_{emp}(w)=\frac{1}{N}\sum_{i=1}^{N}(y_i-f(x_i,w))^2</script></p>
<p>但在现实中，我们更关注真实风险（True Risk）：<script type="math/tex">R(w)=\int L(y,f(x,w))p(x,y)dxdy=E_{x,y\sim p(x,y)}[L(y,f(x,w))]</script>这里的$p(\pmb{x},y)$是$\pmb{x}$和$y$的联合概率密度。\<br>由这个式子可以看出，这个风险是所有数据集的预期误差，是泛化误差的均值。\<br>但这里有一个问题，虽然$p(x,y)$是确定的，但我们通常无法知道，也就是说我们不能直接计算这个真实风险。</p>
<p>关于经验误差和真实误差，下面这个图就能很好说明了：</p>
<p><img src="https://img-blog.csdnimg.cn/20210718174731487.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt=""><br>这三条边界都很好地把数据分开了，也就是说经验误差都是0，但用于测试时效果很可能会有很大差别。</p>
<h2 id="经验风险与真实风险的对比"><a href="#经验风险与真实风险的对比" class="headerlink" title="经验风险与真实风险的对比"></a>经验风险与真实风险的对比</h2><p><img src="https://img-blog.csdnimg.cn/20210718182451841.png#pic_center" alt=""></p>
<blockquote>
<p>真实风险：\<br>优点：具有衡量泛化的能力\<br>缺点：通常我们不知道$p(\pmb{x},y)$</p>
<p>经验风险：\<br>优点：不需要知道$p(\pmb{x},y)$\<br>缺点：无法直接衡量泛化能力\<br>（学习算法通常时最小化经验误差）</p>
</blockquote>
<p>当数据的分布非常集中，经验风险可以通过蒙特卡洛采样和求平均近似表示真实风险，当数据集无限大时，这种近似效果会很好。<br>现给出如下这个图：<br><img src="https://img-blog.csdnimg.cn/20210718220214545.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>有如下两个假设：</p>
<blockquote>
<ol>
<li>当训练数据越来越多时，经验风险会收敛于真实风险；\</li>
<li>收敛是均匀的。</li>
</ol>
</blockquote>
<p>也就是说有如下两个式子：<br><img src="https://img-blog.csdnimg.cn/20210718220521913.png#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20210718220529526.png#pic_center" alt="在这里插入图片描述"><br>其中的inf, sup分别表示下限和上限。<br>如果收敛是均匀的，则：<br><img src="https://img-blog.csdnimg.cn/20210718221008431.png#pic_center" alt="在这里插入图片描述"><br>（这里多说一句，如果训练数据很多的话，这个$p^<em>$是趋近于0的）<br>对于$(1-p^</em>)$的情况，则有：<br><img src="https://img-blog.csdnimg.cn/20210718221431296.png#pic_center" alt="在这里插入图片描述"><br>根据上面式子以及那副曲线图，可得：<br><img src="https://img-blog.csdnimg.cn/20210718221600918.png#pic_center" alt="在这里插入图片描述"><br>以上就是收敛特性的推导过程。这里有个重要的<strong>充要</strong>条件：均匀收敛。\<br><strong>经验风险最小化保证了当$N\rightarrow \infty$时真实风险也最小。</strong></p>
<h2 id="风险边界（Risk-Bound）"><a href="#风险边界（Risk-Bound）" class="headerlink" title="风险边界（Risk Bound）"></a>风险边界（Risk Bound）</h2><p>思想就是：根据经验风险确定真实风险的上限，也就是说：<script type="math/tex">R(w)\leq R_{emp}(w)+\epsilon(N,p^*,h)</script>其中：$N$是训练数据总数；$p^*$是遇到边界的概率；$h$是学习机的学习能力，称为VC维度（VC-dimension）。<br>学习能力（Learning Power）可用下面这幅图解释：<br><img src="https://img-blog.csdnimg.cn/20210718222837981.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>就是模型的复杂程度</p>
<h3 id="VC-dimension"><a href="#VC-dimension" class="headerlink" title="VC-dimension"></a>VC-dimension</h3><p>VC全程是Vapnik-Chervonenkis，大概定义就是：</p>
<blockquote>
<p>一个函数族的VC维度是指能够被该函数族正确分类的数据点的最大数量（无论数据点的标签配置如何）。<br>VC维度是对一个分类器的分类能力或者说 “学习能力 “的衡量。</p>
</blockquote>
<p>VC维度的确定：对于$\mathcal{R}^n$的线性分类器（超平面），VC维度为$n+1$。比如说，对于下图所示的线性分类器($\mathcal{R}^2$)，维度为3.<br><img src="https://img-blog.csdnimg.cn/20210718223543362.png#pic_center" alt="在这里插入图片描述"><br>（注：上面这种确定方法只能说是对大多数情况都有效的，但不是全部。）</p>
<p>(这篇还未写完，后面关于VC维度的其余概念，不太明白在干嘛，先放着，以后有想法了再补充 )</p>
]]></content>
      <categories>
        <category>学习总结</category>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>【学习随记】时间空间复杂度</title>
    <url>/posts/14.html</url>
    <content><![CDATA[<h1 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h1><p>常用的时间复杂度有7种：<br><span id="more"></span></p>
<blockquote>
<ol>
<li>常数时间复杂度；$O(1)$\</li>
<li>对数时间复杂度；$O(\log n)$\</li>
<li>线性时间复杂度；$O(n)$\</li>
<li>平方时间复杂度；$O(n^2)$\</li>
<li>立方时间复杂度；$O(n^3)$\</li>
<li>指数时间复杂度；$O(2^n)$\</li>
<li>阶乘时间复杂度；$O(n!)$</li>
</ol>
</blockquote>
<p><strong>注：从上到下时间复杂度越来越大</strong><br>常规的时间复杂度都容易分析，麻烦的是递归，遇到递归时一般需要把状态树画出来。比如说代码是：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">fib</span><span class="params">(n)</span></span>&#123;</span><br><span class="line">	<span class="keyword">if</span>(n&lt;<span class="number">2</span>)	<span class="keyword">return</span> n;</span><br><span class="line">	<span class="keyword">return</span> fib(n-<span class="number">1</span>) + fib(n-<span class="number">2</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>当输入为6时，状态树为：<br><img src="https://img-blog.csdnimg.cn/img_convert/d621cf89d64f72fc092eb1f2982c68bc.png#pic_center" alt="在这里插入图片描述"><br>结果是$O(2^n)$，分析就很麻烦。</p>
<p>对于递归，常见的情况有4种</p>
<blockquote>
<ol>
<li>二分查找\<br>&emsp; 在有序数列中查找目标数，每次查找都一分为二，最后时间复杂度是$O(\log n)$。\</li>
<li>二叉树遍历\<br>&emsp;虽然每次也都一分为二，但每边都已相同的时间复杂度继续下去，或者说，二叉树每个节点都且仅遍历一次，所以时间复杂度是$O(n)$。比如二叉树的前，中，后序遍历，都是线性复杂度。\</li>
<li>有序的二维矩阵查找\<br>&emsp; 时间复杂度是$O(n)$。\</li>
<li>归并排序\<br>&emsp; 时间复杂度是$O(n\log n)$</li>
</ol>
</blockquote>
<p>关于具体的数学推导的话参看链接: <a href="https://www.zhihu.com/question/21387264">数学推导</a></p>
<p>关于主定理的话，参看这个：<a href="https://zh.wikipedia.org/wiki/%E4%B8%BB%E5%AE%9A%E7%90%86">主定理</a></p>
<h1 id="空间复杂度"><a href="#空间复杂度" class="headerlink" title="空间复杂度"></a>空间复杂度</h1><p>这个就比时间复杂度善良很多，一般就考虑两种情况：</p>
<blockquote>
<ol>
<li>新开数组的长度\</li>
<li>递归的深度<br>若递归里新开数组，我们就考虑大的那个复杂度就行。</li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>学习随记</category>
      </categories>
  </entry>
  <entry>
    <title>非线性优化</title>
    <url>/posts/15.html</url>
    <content><![CDATA[<h1 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h1><p>一个经典的SLAM模型由一个运动方程和一个观测方程构成</p>
<span id="more"></span>
<p>如下式所示：<script type="math/tex">\pmb{x}_k=f(\pmb{x}_{k-1},\pmb{u}_k)+\pmb{w}_k\\\pmb{z}_{k,j}=h(\pmb{y}_j,\pmb{x}_k)+\pmb{v}_{k,j}</script>其中$\pmb{u}_k$是运动传感器的读数或者输入；$\pmb{w}_k$是该过程中加入的噪声；$f$是一般函数，用来描述这个过程的；$\pmb{x}$是相机的位姿，可用SE(3)来描述；$\pmb{y}$是路标点；$\pmb{z}$是观测数据；$\pmb{v}_{k,j}$是这次观测里的噪声。由于观测所用的传感器形式很多，这里的观测数据$\pmb{z}$和观测方程$h$也有很多形式。<br>现假设观测方程由针孔模型给定，在$\pmb{x}_k$处对路标$\pmb{y}_j$进行了一次观测，对应到图像上的像素位置为$\pmb{z}_{k,j}$，则观测方程可以表示为：<script type="math/tex">s\pmb{z}_{k,j}=\pmb{K}(\pmb{R}_k\pmb{y}_j+\pmb{t}_K)</script>其中$\pmb{K}$为相机内参，$s$为像素点距离，也是$(\pmb{R}_k\pmb{y}_j+\pmb{t}_k)$的第三个分量，若用变换矩阵$\pmb{T}_k$来描述位姿，则路标$\pmb{y}_j$要用齐次坐标。</p>
<p>—————————————————————————————————————</p>
<p>简单回顾下相机内参的相关知识\<br>相机内参数是与相机自身特性相关的参数，比如相机的焦距、像素大小等，这些在相机出厂后是固定的。由于镜头的安装精度，形状，传感器上的像素等因素，使镜头的光轴不再穿过图像的正中间。内参矩阵一般是如下形式：<script type="math/tex">\begin{pmatrix} f_x & 0 & c_x \\ 0 & f_y & c_y \\ 0&0&1 \end{pmatrix}</script>其中$f_x,f_y$表示$u,v$轴的缩放，$c_x,c_y$表示原点平移的距离。</p>
<p>—————————————————————————————————————</p>
<p>要知道数据在受到噪声影响后会发生什么变化，通常会假设两个噪声项$\pmb{w}_k,\pmb{v}_{k,j}$，使其满足零均值的高斯分布，即：<script type="math/tex">\pmb{w}_k\sim N(0,\pmb{R}_k)\quad \pmb{v}_k\sim N(0,\pmb{Q}_{k,j})</script>其中$\pmb{R}_k,\pmb{Q}_{k,j}$为协方差矩阵。<br>通过带噪声的数据$\pmb{z}$和$\pmb{u}$来推断位姿$\pmb{x}$和地图$\pmb{y}$（及其概率分布），构成一个状态估计问题，即：<script type="math/tex">P(\pmb{x},\pmb{y}|\pmb{z},\pmb{u})</script>用贝叶斯法则为：<script type="math/tex">P(\pmb{x},\pmb{y}|\pmb{z},\pmb{u})=\frac{P(\pmb{z},\pmb{u}|\pmb{x},\pmb{y})P(\pmb{x},\pmb{y})}{P(\pmb{z},\pmb{u})}\varpropto P(\pmb{z},\pmb{u}|\pmb{x},\pmb{y})P(\pmb{x},\pmb{y})</script>这就是一个求MAP的问题：<script type="math/tex">(\pmb{x},\pmb{y})^*_{MAP}=arg\max P(\pmb{x},\pmb{y}|\pmb{z},\pmb{u}) = arg\max P(\pmb{z},\pmb{u}|\pmb{x},\pmb{y})P(\pmb{x},\pmb{y})</script>如果我们不知道先验的话，就变成了求MLE的问题，即：<script type="math/tex">(\pmb{x},\pmb{y})^*_{MLE}=arg\max P(\pmb{z},\pmb{u}|\pmb{x},\pmb{y})</script>根据上面，我们假设了噪声项为零均值的高斯分布，所以观测数据的条件概率为：<script type="math/tex">P(\pmb{z}_{j,k}|\pmb{x}_k,\pmb{y}_j)=N(h(\pmb{y}_j,\pmb{x}_k),\pmb{Q}_{k,j})</script>(前提：观测是高斯分布的)<br>然后就是常见的展开取负对数，求最大变成求最小：<script type="math/tex">-\ln(P(\pmb{x}))=\frac{1}{2}\ln((2\pi)^N\det(\Sigma))+\frac{1}{2}(\pmb{x}-\pmb{\mu})^T\Sigma^{-1}(\pmb{x}-\pmb{\mu})</script>右边第一项与$\pmb{x}$无关，就不用管了，只关注第二项那个二次型。<br>将SLAM的观测模型带入得：</p>
<script type="math/tex; mode=display">\begin{aligned}
 (\pmb{x}_k,\pmb{y}_j)^* 
 &= arg\max N(h(\pmb{y}_j,\pmb{x}_k),\pmb{Q}_{k,j})\\ 
 &= arg\min ((\pmb{z}_{k,j}-h(\pmb{x}_k,\pmb{y}_j))^T\pmb{Q}^{-1}_{k,j}(\pmb{z}_{k,j}-h(\pmb{x}_k,\pmb{y}_j)))
  \end{aligned}</script><p>在处理<strong>批量</strong>数据时我们通常是假设各个时刻的输入$\pmb{u}$和观测$\pmb{z}$相互独立，即是有：</p>
<script type="math/tex; mode=display">P(z,u|x,y)=\prod_k P(u_k|x_{k-1},x_k)\prod_{k,j}P(z_{k,j}|x_k,y_j)$$然后我们定义相关误差，运动误差为：$$e_{u,k}=x_k-f(x_{k-1},u_k)$$观测误差为：$$e_{z,j,k}=z_{k,j}-h(x_k,y_j)$$用跟上面同样的方法也能写出$P(u_k|x_{k-1},x_{k})$的最小二乘表达式。现在我们要让这两个误差最小，使用误差平方和定义一个cost function，如下：$$\min J(x,y)=\sum_ke^T_{u,k}R^{-1}_ke_{u,k}+\sum_k\sum_je^T_{z,k,j}Q^{-1}_{k,j}e_{z,k,j}$$这就是我们要处理的最小二乘问题，其实就是两个MLE结合。这个明显是非线性的最小二乘问题，没法直接计算，现介绍以下几种迭代方法。

# 非线性最小二乘问题
现讨论最广泛的情况，即以$f(x)$作为二乘项，我们要解决以下这个问题：$$\min_{x}\frac{1}{2}||f(x)||^2 \quad x \in R^n $$当函数$f$很简单时，直接求导等于0，比较各个极值点即可；但当$f$很复杂，求导不容易计算时，可用迭代的方法。
迭代的基本思想是：

> 1. 给定初值$x_0$；\
> 2. 对于第$k$次迭代，寻找一个增量$\Delta x_k$，使得$||f(x_k+\Delta x_k)||_2^2$达到极小值；\
> 3. 若$\Delta x_k$足够小，则停止；\
> 4. 否则，令$x_{k+1}=x_k+\Delta x_k$，返回步骤2。

现在的问题是如何确定这个增量$\Delta x_k$。

## 一阶/ 二阶梯度法
这就是常见的Gradient Descent。
首先根据泰勒展开（对平方展开）可写为：$$||f(x+\Delta x)||^2_2 \approx ||f(x)||_2^2 + J(x)\Delta x+\frac{1}{2}\Delta x^TH\Delta x$$其中$J,H$分别为雅可比和海森矩阵。

### 一阶（最速下降法，Steepest Method）
即只保留一阶梯度：$$\min_{\Delta x}||f(x)||_2^2+J\Delta x$$增量方向为$\Delta x^*=-J^T(x)$，通常还要计算步长。

#### 二阶（牛顿法）
即同时保留一阶和二阶：$$\Delta x^8=arg\min ||f(x)||_2^2+J(x)\Delta x+\frac{1}{2}\Delta x^TH\Delta x$$令上式关于$\Delta x$的导数为0则有：$$H\Delta x=-J^T</script><h3 id="方法分析（优缺点）"><a href="#方法分析（优缺点）" class="headerlink" title="方法分析（优缺点）"></a>方法分析（优缺点）</h3><p>最速下降和牛顿虽然直观，但两者均有一些缺点：</p>
<ol>
<li>最速下降法由于过于贪婪会碰到zigzag问题，如下图：<br><img src="https://img-blog.csdnimg.cn/c01029543cbd45a6bcab62e4cdc0bf10.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>每次迭代均是往梯度下降最快的方向进行，对于简单的问题依旧需要很多次迭代才能达到最优点。</li>
<li>牛顿法由于是二阶的，可以沿着曲线直接到最优点，但却需要计算复杂的海森矩阵。</li>
</ol>
<p>所以这两种方法都不太实用，我们希望可以在不计算海森矩阵的前提下实现二阶梯度，就有了以下两种方法</p>
<h2 id="Gauss-Newton"><a href="#Gauss-Newton" class="headerlink" title="Gauss-Newton"></a>Gauss-Newton</h2><p>一阶近似$f(x)$为：<script type="math/tex">f(x+\Delta x) \approx f(x)+J(x)\Delta x</script>其平方误差为：$$<br>\begin{aligned}<br>\frac{1}{2}||f(x)+J(x)\Delta x||^2 &amp;=\frac{1}{2}(f(x)+J(x)\Delta x)^T(f(x)+J(x)\Delta x)\\&amp;=\frac{1}{2}(||f(x)||_2^2+2f(x)^TJ(x)\Delta x+\Delta x^TJ(x)^TJ(x)\Delta x)<br>\end{aligned}</p>
<script type="math/tex; mode=display">令其对$\Delta x$的导数为零得：</script><p>2J(x)^Tf(x)+2J(x)^TJ(x)\Delta x=0</p>
<script type="math/tex; mode=display">即：</script><p>J(x)^TJ(x)\Delta x = -J(x)^Tf(x)</p>
<script type="math/tex; mode=display">我们可以把这个式子写为：</script><p>H\Delta x=g</p>
<script type="math/tex; mode=display">与上面的二阶（牛顿）法做对比，可以发现这里实际是用$J(x)^TJ(x)$近似海森矩阵。
所以这个迭代过程可以写为：

>1. 给定初始值$x_0$；
>2. 对于第$k$次迭代，求出当前的雅可比矩阵$J(x_k)$和误差$f(x_k)$；
>3. 求解增量方程：$H\Delta x=g$，即$\Delta x_k=H^{-1}g$；
>4. 若$\Delta x_k$足够小，则停止，否则令$x_{k+1}=x_k+\Delta x_k$，并返回步骤2。

**总结**：
由于海森矩阵不容易算，所以这里用雅可比的平方近似海森矩阵。这种方法简单实用，但$\Delta x_k=H^{-1}g$中无法保证$H$可逆（二次近似不可靠）。
然后Levenberg_Marquadt方法在一定程度上改善了它。

## Levenberg_Marquadt
与G-N方法不同，这里引进了一个参考值，或者说区域，用于描述近似程度：</script><p>\rho=\frac{f(x+\Delta x)-f(x)}{J(x)\Delta x}</p>
<script type="math/tex; mode=display">这个式子可以理解为：**实际下降 / 近似下降**，若太大，则减小近似范围，若太小，则增加近似范围。
其迭代过程可以描述为：

>1. 给定初始值$x_0$以及初始优化半径$\mu$；
>2. 对于第$k$次迭代，求解：$$\min_{\Delta x_k}\frac{1}{2}||f(x_k)+J(x_k)\Delta x_k||^2, \quad s.t. ||D\Delta x_k||^2\le\mu$$这里$\mu$是信赖区域的半径，$D$的话可以理解为形状参数，当$D=1$时，表示取一个球，后来普遍令$D$取非负对角矩阵，表示椭球。
>3. 计算$\rho$；
>4. 若$\rho>\frac{3}{4}$，则$\mu=2\mu$；
>5. 若$\rho<\frac{1}{4}$，则$\mu=0.5\mu$；（这两个都是经验值）
>6. 如果$\rho$大于某个阈值，认为近似可行，令$x_{k+1}=x_k+\Delta x_k$
>7. 判断算法是否收敛，如果不收敛则返回步骤2，否则结束。

最后，在上面的步骤中还剩一个问题，就是第二步的带约束的优化问题，依旧用拉格朗日进行计算即可：</script><p>\min_{\Delta x_k}\frac{1}{2}||f(x_k)+J(x_k)\Delta x_k||^2+\frac{\lambda}{2}||D\Delta x||^2</p>
<script type="math/tex; mode=display">参照G-N展开，增量方程为：</script><p>(H+\lambda D^TD)\Delta x=g </p>
<p><script type="math/tex">在Levenberg方法中，取$D=I$，则：</script>(H+\lambda I)\Delta x=g$$这里的$H$依旧是$J(x)^TJ(x)$。<br>对比G-N方法，这里前面加上了一项，相当于在G-N方法的基础上增强了$H$的正定性。</p>
<h2 id="G-N-和-L-M的对比"><a href="#G-N-和-L-M的对比" class="headerlink" title="G-N 和 L-M的对比"></a>G-N 和 L-M的对比</h2><blockquote>
<ol>
<li>G-N方法属于线性搜索方法：先找到方向，再确定长度；\</li>
<li>L-M方法属于信赖区域方法（Trust Region），认为近似只在区域内可靠。\</li>
<li>LM相比于GN，能够保证增量方程的正定性，即认为近似只在一定范围内成立，如果近似不好则缩小范围。收敛能够更好一点。\</li>
<li>从增量方程上来看，可以看成一阶和二阶的混合，参数$\lambda$控制两边的权重。以<script type="math/tex">(H+\lambda I)\Delta x=g</script>为例，去掉$H$则变成一阶的，去掉$\lambda$项则变成二阶的。\</li>
<li>一般来说，LM的计算量会比GN大一些，但收敛性也更好。所以简单的情况用GN，复杂的用LM。</li>
</ol>
</blockquote>
<p><strong><em>注：本文参考自《视觉SLAM十四讲》</em></strong></p>
]]></content>
      <categories>
        <category>视觉SLAM相关</category>
      </categories>
  </entry>
  <entry>
    <title>深度学习中的正则化</title>
    <url>/posts/17.html</url>
    <content><![CDATA[<h1 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h1><p>与其它机器学习方法类似，DNN在训练过程中也会遇到过拟合的现象，尤其是当参数数量多于输入数据量时。为了防止过拟合现象，除了增加训练样本外，最常见的就是各种正则化方法，比如：数据增强、$L1$ 正则化、$L2$ 正则化、Dropout、DropConnect 和早停法（Early stopping）等。<br>下面对这些方法进行逐一介绍。<br><span id="more"></span></p>
<h1 id="正则化方法"><a href="#正则化方法" class="headerlink" title="正则化方法"></a>正则化方法</h1><h2 id="1-数据增强"><a href="#1-数据增强" class="headerlink" title="1. 数据增强"></a>1. 数据增强</h2><p>以图像处理为例，我们可以预先将图像做翻转拉伸，亮度调节，随机平移等操作，从而增大模型的训练数据集，增加模型的泛化能力。一般来说，现在在进行深度学习时数据增强已经成为一个必要的操作了。<br>但要注意的是，这种方法也不能用得太过分，一般来说用这种方法将数据集拓展到两倍已经很够了，如果再继续拓展，意义不大而且会浪费很多时间。<br>对于图像的数据增强方法可参见另一篇博文【图像的各种预处理方式】。</p>
<h2 id="2-L1-L2-正则"><a href="#2-L1-L2-正则" class="headerlink" title="2. $L1 / L2$正则"></a>2. $L1 / L2$正则</h2><p>L1 / L2正则原理相似，是在损失函数后面加个正则化项，以此对损失函数进行约束，感觉就是用拉格朗日解带约束的优化问题。现假设我们的损失函数为$L(y_i,\hat{y}_i)$。<br>这里先写三个概念：</p>
<blockquote>
<ol>
<li>$L0$正则：向量中非零元素的个数，记作：$||W||_0$；\</li>
<li>$L1$正则：向量中元素绝对值之和，记作：$||W||_1$；\</li>
<li>$L2$正则：也就是模，记作：$||W||_2$</li>
</ol>
</blockquote>
<p>对于单个数据，假设其特征项$X$为：$x_0,x_1,…,x_N$，对应的权重项$W$为：$w_0,w_1,…,w_N$，为了防止过拟合，即是要减少数据的特征项数$N$，这里选择通过控制权重项$W$来控制特征项数$N$，为什么不通过特征项$X$呢？因为我们不确定下一个输入数据的特征项有多少，没法控制。也就是说，现在我们要做的就是控制权重项$W$的项数，使其数目最少。</p>
<h3 id="2-1-L1-正则"><a href="#2-1-L1-正则" class="headerlink" title="2.1 $L1$正则"></a>2.1 $L1$正则</h3><p>最开始用的其实是$L0$正则，也就是说我们要同时让损失函数$L(y_i,\hat{y}_i)$以及$||W||_0$正则项最小，即是求它们之和最小。</p>
<p>补充一点点东西</p>
<blockquote>
<ol>
<li>这里我们也就是要实现参数矩阵$W$稀疏，现在实现稀疏基本都是用$L1$正则，不用$L0$正则；\</li>
<li>参数矩阵稀疏通常是为了特征选择和易于解释方面的考虑</li>
</ol>
</blockquote>
<p>但由于$L0$正则不好计算，所以我们转而求$L1$正则，通过上面两概念可以知道，这两者在这里的计算意义是类似的，也就是最优凸近似（具体推导看不下去，有兴趣的参看Emmanuel Candes的paper）。<br>加上$L1$正则项后新的损失函数如下：<script type="math/tex">L^{new}=L(y_i,\hat{y}_i)+\alpha ||W||_1</script>有些地方会写得更具体，如下：<script type="math/tex">J(w,b)=\frac{1}{m}\sum_{i=1}^mL(y_i,\hat{y}_i)+\frac{\lambda}{2m}||w||_1</script>反正意思都一样。这里的$\lambda$属于超参数，也就是我们要自己调的，越大正则化越明显。<br>由于损失函数变化了，梯度也会跟着变化，也就是会带来梯度更新方向的不同，之后的计算跟以前一样。</p>
<h3 id="2-2-L2-正则"><a href="#2-2-L2-正则" class="headerlink" title="2.2 $L2$正则"></a>2.2 $L2$正则</h3><p>$L2$是求模，也就是矩阵各元素求平方和再开方，采用$L2$正则的目的是让各参数趋近于$0$，也就是让他们最小化。（$L1$则是让它们等于$0$，也就是稀疏）<br>那为什么让参数最小化可以有效防止过拟合呢？通过$L2$正则可以构造出一个参数都比较小的模型，一方面，对于那些实在不重要的特征，其权重会非常接近于0，但没有等于，影响已经很小了；另一方面，当参数很小时，既使数据变化比较大，其对结果的影响也不会很大，即模型的抗干扰能力会比较强。<br>其形式和$L1$基本一样，只要把后面的项改成$L2$正则即可。</p>
<h3 id="2-3-L1-与-L2-的总结比较"><a href="#2-3-L1-与-L2-的总结比较" class="headerlink" title="2.3 $L1$与$L2$的总结比较"></a>2.3 $L1$与$L2$的总结比较</h3><p>$L1$,$L2$在一些地方也写作LASSO和岭回归（在之前的【线性回归】也有提到一些）。<br>在各种防止过拟合的正则化中，$L2$防过拟合效果都要优于$L1$正则，所以与$L1$相比，正则化一般都是用$L2$，但当$L1$正则中的系数$\alpha$比较大时，也会得到系数极小的最优解，这时的$L1$也具有防止过拟合的作用。<br>$L1$正则由于可以实现稀疏矩阵，所以可以用来当作特征的选择。<br>有一点或许是需要注意的，在进行$L2$正则化中，我们把权重变得很小，所以在进入激活函数的时候值是在$0$附近的，以Sigmoid函数为例，当值在$0$附近时，其激活函数可看作线性，也就是说整个深度神经网络进行线性化近似了（不确定能不能用这种说法），这即是优点也是缺点，优点是我们把神经网络简化了，这也是实现这种正则化方法的基础；但，由于我们计算的是这种近似线性的东西，当我们要做非常复杂的决策时，这种正则化方法是不适用的。</p>
<h2 id="3-Dropout"><a href="#3-Dropout" class="headerlink" title="3. Dropout"></a>3. Dropout</h2><p>这个就简单写了，因为基本思路不难。Dropout的基本思路是随机去掉神经元，也就是在进行一批数据训练时，我们随机去掉一些神经元，既可以是隐藏层的，也可以是输入层的，然后进行训练，更新参数；然后在下一批数据进来的时候我们要先恢复回最原来的网络结构，再随机去掉一些神经元，更新参数，继续。。。。<br>需要注意的是，Dropout方法每次更新的都是同一套参数，也就是不会产生新的参数（这点跟Bagging不同，Bagging是每次训练都有自己单独的一套）。<br>因为Dropout将原始数据分批迭代，用这种正则化方法前提是训练样本足够大，否则会产生欠拟合现象。</p>
<h2 id="4-DropConnect"><a href="#4-DropConnect" class="headerlink" title="4. DropConnect"></a>4. DropConnect</h2><p>DropConnect跟Dropout其实很像，Dropout是通过去掉一些神经元来防止过拟合的，而DropConnect则是通过随机选择权重的子集设为$0$，两种方法都能增强模型的泛化能力，都在模型中引入了稀疏性，但不同的是DropConnect是在权重中引入稀疏性而不是在层的输出向量中引入。</p>
<h2 id="5-早停法（Early-stopping）"><a href="#5-早停法（Early-stopping）" class="headerlink" title="5. 早停法（Early stopping）"></a>5. 早停法（Early stopping）</h2><p>训练过神经网络的都知道，当训练次数过多时，模型容易发生过拟合现象，那么我们只要在过拟合之前停止训练就可以了。这就是早停法，也是深度学习中非常常用的一种方法，因为简单又有效。用这种方法我们就要时刻检测验证集的损失，当其开始<strong>持续</strong>上升时，就该停止训练了。</p>
<p><strong><em>END</em></strong>\<br>先写到这，另外写到后面总觉得这篇有点问题，我好像把正则化和防止过拟合这两个概念混为一谈了？</p>
<h1 id="参考网站来源"><a href="#参考网站来源" class="headerlink" title="参考网站来源"></a>参考网站来源</h1><p>网站1：<a href="https://developer.aliyun.com/article/632944">https://developer.aliyun.com/article/632944</a></p>
<p>网站2：<a href="https://cloud.tencent.com/developer/article/1486732">https://cloud.tencent.com/developer/article/1486732</a></p>
<p>网站3：<a href="http://imgtec.eetrend.com/blog/2019/100045162.html">http://imgtec.eetrend.com/blog/2019/100045162.html</a></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>【ML-2020/21】CNN &amp; Self-Attention</title>
    <url>/posts/16.html</url>
    <content><![CDATA[<h1 id="写在前面的说明"><a href="#写在前面的说明" class="headerlink" title="写在前面的说明"></a>写在前面的说明</h1><p>这个系列【ML-2020/21】大部分是课上内容的简单复述，之前上过但因为笔记写得很乱就忘了很多，所以重来一遍。与其看我这篇，不如直接去看视频，讲得还更生动。视频系列链接$\rightarrow$<a href="https://www.youtube.com/watch?v=OP5HcXJg2Aw">这里</a>。</p>
<p>这里介绍两个常见的Network架构，分别为CNN 和 Self-Attention。<br><span id="more"></span></p>
<h1 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h1><p>CNN主要是用来处理图像的，对于Fully Connected Network，每个神经元都要观察整张图片，这明显不是高效率的做法，所以更常见的是让每个神经元处理某一特定的pattern,，比如说就像下图：<br><img src="https://img-blog.csdnimg.cn/d439d0b3fc954e7ab460385d90d50a23.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>我们就希望能简化这个网络，这里有两种解释。</p>
<h2 id="两种解释"><a href="#两种解释" class="headerlink" title="两种解释"></a>两种解释</h2><h3 id="1-Receptive-field"><a href="#1-Receptive-field" class="headerlink" title="1. Receptive field"></a>1. Receptive field</h3><p>就像下图那样：<br><img src="https://img-blog.csdnimg.cn/170e557b01da49bc93680928517a2df0.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p>每个神经元只负责其中一个区域的信息，并且，同个区域可以有多个不同的神经元负责。<br>另外，这个Receptive field可以有大有小，也可以让Receptive field只考虑某一特定的channel，比如rgb里的红色区域。<br>一般来说，最经典的size大小是$3\times 3$，然后对于同一个Receptive field，一般会有一组神经元（比如64或者128个）去处理它，而且一般来说这个Receptive field的排布是会重叠的，以保证覆盖整个图像，如下：<br><img src="https://img-blog.csdnimg.cn/1229bff146434d25b5e3639067b5ed76.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h4 id="parameter-sharing"><a href="#parameter-sharing" class="headerlink" title="parameter sharing"></a>parameter sharing</h4><p>对于下面这种情况：<br><img src="https://img-blog.csdnimg.cn/9c209adfd54248839f667259ba391907.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>鸟喙出现在图中的不同位置，由于每个pattern都有很多个神经元去处理，所以理所当然地，这两种情况都能把鸟喙给检测出来，但对于同种作用的检测器，我们明显不希望它们分属两个不同的东西，不然参数会很多，所以就希望，对于相同作用的检测器，它们的参数应该是相同的（即参数共享）。<br>至此，就一步步地将模型简化了，如下图所示：<br><img src="https://img-blog.csdnimg.cn/d006bd63940c41afbf60c143a3d3902e.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>这就是convolutional layer，而用了convolutional layer的就是CNN。另外，CNN的$bias$一般会比较大。</p>
<h3 id="2-Filter"><a href="#2-Filter" class="headerlink" title="2. Filter"></a>2. Filter</h3><p>对于convolutional layer的另一种解释是用Filter。如下图：<br><img src="https://img-blog.csdnimg.cn/bd774849c64f4b7d916fc491c7939ef4.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>以右上角这个filter为例，从图片左上角一直滑到右下角，对应相乘后相加。对于每个卷积层都有很多个Filters。<br>另外，虽然这个filter只是$3\times 3$的，但由于不断进行卷积，每一个卷积层都把它的考虑范围扩大了，如下图，下面那层的$3\times 3$等于是考虑了上面那层的$5\times 5$范围：<br><img src="https://img-blog.csdnimg.cn/fa0ca16f08df47cf89241d5df3be1e5b.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>也就是说，network越深考虑的范围越大。</p>
<blockquote>
<p>上面的parameter sharing实际就是这里的filter扫过各个区域过程。<br>上面的receptive filter实际就是这里的不同的filter。<br>所以两种解释其实是一模一样的。</p>
</blockquote>
<h2 id="Pooling"><a href="#Pooling" class="headerlink" title="Pooling"></a>Pooling</h2><p>pooling的思想源于降采样，即是对于一幅图，我们把它进行将采用，比如把一幅图的偶数像素都删除，并不妨碍我们识别这幅图。如下图：<br><img src="https://img-blog.csdnimg.cn/c61338aae46b4c5a88911345f1aa50ff.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>pooling是不需要学习的，就跟激活函数类似。</p>
<h3 id="Max-Pooling"><a href="#Max-Pooling" class="headerlink" title="Max Pooling"></a>Max Pooling</h3><p>除了Max pooling，还有mean Pooling之类的，都差不多。<br>基本概念就是选各个区域最大的那个数，比如下图：<br><img src="https://img-blog.csdnimg.cn/3cb5104423b940a78fec669d4a55b660.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>选完之后就变成下面这样了：<br><img src="https://img-blog.csdnimg.cn/1dcccadc91294bf5bc1e2a9e39708a55.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>这里区域大小也是自己定的，不一定要用$2\times 2$。通过这个操作就能把”图片“变小。这个池化层一般是和卷积层交替使用的。</p>
<blockquote>
<p>pooling 是为了减少运算量，但现在由于运算能力的发展，越来越多地方开始去除pooling层了，这也是一种趋势。</p>
</blockquote>
<p>至此就有了CNN的基本架构了：<br><img src="https://img-blog.csdnimg.cn/b5dea29b55ef46c6bc23242e6a4e6bcf.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h1 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h1><p>在CNN中，我们的输入都是一个向量，但如果遇到输入是一组向量的话该怎么办？比如说语言处理，输入的单词长度都不一样，Self-Attention 就是解决这种问题的一种办法。<br>在这种架构中对于输出一般有以下三种情况：</p>
<blockquote>
<ol>
<li>$N$个输入对应$N$个输出，比如说输入一句话，我们要分析每个单词对应的词性。</li>
<li>$N$个输入对应$1$个输出，比如输入一句话，分析其是褒义还是贬义。</li>
<li>$N$个输入对应$M$个输出，比如翻译。</li>
</ol>
</blockquote>
<p>这里先介绍第一种，即同维度输入输出。以输入一句话为例。<br>第一种最直接的想法是用Fully Connected，由于要考虑单词间的联系，就有了如下这种结构：<br><img src="https://img-blog.csdnimg.cn/7739d772a5e948c98fdd41d5c82a9a71.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>这确实能做，某些情况下效果也不坏，但如果我们要考虑整句话，而不是图中的红色框框呢？又一个直接的想法是：扩大window的大小以囊括整句话。<br>但，这里每句话的长度是不相等的，如果要用这种方法，那就要首先检测数据集中最长那句话有多长，但这样运算量会非常大，而且容易overfitting，而且在测试中也不一定适用。<br>另一种更好的方法就是Self-Attention，其基本结构如下：<br><img src="https://img-blog.csdnimg.cn/93e6bba2e2574e2c9c5cea382972a37c.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>也就是说，我们先把整句话用self-attention处理，对应产生新的向量，这时候的每个向量就已经是考虑整句话后的向量了。<br>这两种网络是可以交替使用的，比如下面这个：<br><img src="https://img-blog.csdnimg.cn/fa7650543232479f877f07d793263602.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h2 id="运作"><a href="#运作" class="headerlink" title="运作"></a>运作</h2><p>假设我们的输入（或者中间某一层的输出）为这四个向量$[a^1,a^2,a^3,a^4]$，现在我们想要做的就是每一个向量与其它向量的关联程度。<br>以两个向量为例，计算关联度有很多种方法，比如下面两种：<br><img src="https://img-blog.csdnimg.cn/cf7e419319e54afebcd5aaadc7724300.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><strong>以下都用左边那种</strong><br>对于四个向量，计算与$a^1$的关联度如下图：<br><img src="https://img-blog.csdnimg.cn/f79769229bcc44af8cd9135f94d11a67.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<blockquote>
<p>注意，这里也要计算与自身的关联度。</p>
</blockquote>
<p>这里的激活函数不一定要用Soft-max，也可以用其他的比如ReLu。<br>接下来计算新的向量，也就是根据关联性计算，如下：<br><img src="https://img-blog.csdnimg.cn/2275dfc0a96049f19baae048018e16bf.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>以上过程可以用矩阵乘法表示。</p>
<h3 id="矩阵乘法表示"><a href="#矩阵乘法表示" class="headerlink" title="矩阵乘法表示"></a>矩阵乘法表示</h3><p> （这部分很无聊可以直接跳过）<img src="https://img-blog.csdnimg.cn/288c02b8a420479f9e272d7d552e0f28.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/6f9c645c1c114706ba87a7d1b06bfeb5.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/7ead13e9d4be40ff989996100ac166d7.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/c4b2fb755df94b35b09b8753761128bb.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>这其中只有$W^q,W^k,W^v$是需要学习的。</p>
<h2 id="Multi-head-Self-attention"><a href="#Multi-head-Self-attention" class="headerlink" title="Multi-head-Self-attention"></a>Multi-head-Self-attention</h2><p>这里以两个head为例：<br><img src="https://img-blog.csdnimg.cn/d881b6f1cb0f4337a0e9ada6ba36db3d.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>对于两个head计算出的$b^{i,1},b^{i,2}$可通过新引入的矩阵进行结合：<br><img src="https://img-blog.csdnimg.cn/17615172e1b343028283242f4f910432.png#pic_center" alt="在这里插入图片描述"></p>
<h2 id="Position-Encoding"><a href="#Position-Encoding" class="headerlink" title="Position Encoding"></a>Position Encoding</h2><p>现在还剩一个问题，就是我们虽然考虑了整个句子，但我们没有考虑各个单词的相对位置。<br>这里需要引入一个位置参数$e^i$，每个向量都要加上特定的位置参数，如下：<br><img src="https://img-blog.csdnimg.cn/c5c7c0b35d9c471ba36a69f54dbb2ff4.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>最早的$e^i$（也就是在paper”Attention all you need“里）长这样：<br><img src="https://img-blog.csdnimg.cn/f961ce0ac8284eb1938aed959001fc4a.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>这个图的每一列代表一个$e^i$。位置参数不一定要用这种，可以自己研究。</p>
]]></content>
      <categories>
        <category>ML-2020/21</category>
      </categories>
  </entry>
  <entry>
    <title>【学习随记】各种空间的简单介绍</title>
    <url>/posts/12.html</url>
    <content><![CDATA[<p>因为经常遇到各种特定的空间，有些没接触过，有些又容易弄混，这里做个记录，这篇应该会不断更新，毕竟空间概念这么多。（注：这里只是最简单的介绍）<br><span id="more"></span><br>先解释相关概念。</p>
<h1 id="相关概念："><a href="#相关概念：" class="headerlink" title="相关概念："></a>相关概念：</h1><h2 id="完备性"><a href="#完备性" class="headerlink" title="完备性"></a>完备性</h2><p>简单说的话，就是对极限封闭。也就是说，如果对于空间$S$内的一点$s_i$，$\lim_{i\rightarrow\infty}s_i=s$，$s$也属于空间$S$的话，则称该空间具有完备性。</p>
<h2 id="内积"><a href="#内积" class="headerlink" title="内积"></a>内积</h2><p>说到内积，第一反应应该就是向量内积，即：<script type="math/tex">\langle a,b \rangle = |a|*|b|*\cos\theta</script>，但更广泛的话，内积应该满足以下三个条件($f,g$都是空间元素)：</p>
<blockquote>
<ol>
<li>对称性：$\langle f,g \rangle = \langle g,f \rangle$；</li>
<li>正定性：$\langle f,f \rangle \ge0$，当且仅当$f=0$时等号成立。</li>
<li>线性：$\langle r_1f_1+r_2f_2,g \rangle = r_1\langle f_1,g \rangle+r_2\langle f_2,g \rangle$</li>
</ol>
</blockquote>
<h1 id="空间"><a href="#空间" class="headerlink" title="空间"></a>空间</h1><h2 id="度量（距离）空间"><a href="#度量（距离）空间" class="headerlink" title="度量（距离）空间"></a>度量（距离）空间</h2><p>设$X$是非空集合，对于$X$中的任意两个元素$x,y$，按某一法则都对应唯一的实数$\rho(x,y)$，并满足下面三个条件（距离公理）：</p>
<blockquote>
<ol>
<li>非负性：$\rho(x,y)\ge0$，当且仅当$x=y$时，$\rho(x,y)=0$;</li>
<li>对称性：$\rho(x,y)=\rho(y,x)$;</li>
<li>三角不等式：对任意$x,y,z$，$\rho(x,y)\leq\rho(x,z)+\rho(z,y)$</li>
</ol>
</blockquote>
<p>则称$\rho(x,y)$为$x$与$y$的距离（或度量），并称$X$是以$\rho$为距离的距离空间，记作$(X,\rho)$。<br>这里的距离不局限于“点空间”内的距离，比如下面两个也满足距离：<script type="math/tex">\rho(x,y)=\max\limits_{1\leq k\leq n}|x_k-y_k|$$$$\rho(x,y)=\sum_{k=1}^{n}|x_k-y_k|</script> $L^p[a,b]$表示区间$[a,b]$绝对值$p$次幂$L$可积函数的全体，并把几乎处处相等的函数看成是同一个函数，对于$x,y\in L^p[a,b]$，规定<script type="math/tex">\rho(x,y)=[\int_a^b|x(t)-y(t)|^pdt]^{1/p},p\ge1</script>则$L^p[a,b]$构成一个距离空间，称为$p$次幂可积函数空间。</p>
<h2 id="线性空间"><a href="#线性空间" class="headerlink" title="线性空间"></a>线性空间</h2><p>直观理解的话就是拥有加法和数乘的非空集合。首先要求非空，其次满足加法运算的4个属性：</p>
<blockquote>
<ol>
<li>加法交换律</li>
<li>加法结合律</li>
<li>存在零元：$x+0=x$</li>
<li>存在逆元：$x+(-x)=0$</li>
</ol>
</blockquote>
<p>满足数乘的4个属性：</p>
<blockquote>
<ol>
<li>$1x=x$</li>
<li>$a(bx)=(ab)x$</li>
<li>$(a+b)x=ax+bx$</li>
<li>$a(x+y)=ax+ay$</li>
</ol>
</blockquote>
<h2 id="赋范空间"><a href="#赋范空间" class="headerlink" title="赋范空间"></a>赋范空间</h2><p>设$X$是实（或复）线性空间，如果对于$X$中的每个元素$x$，按照一定的法则对应于实数$||x||$，且满足：</p>
<blockquote>
<ol>
<li>$||x||\ge0$，当且仅当$x$等于零元（$x=0$）时$||x||=0$;</li>
<li>$||ax||=|a|||x||$，$a$是实（或复）数；</li>
<li>$||x+y||\leq||x||+||y||$</li>
</ol>
</blockquote>
<p>则称$X$是实（或复）赋范线性空间，$||x||$称为$x$的范数。</p>
<blockquote>
<p>注：赋范线性空间必然是距离空间。定义<script type="math/tex">\rho(x,y)=||x-y||</script></p>
</blockquote>
<p>与距离空间的不同在于：</p>
<ol>
<li>平移不变性：$d(x+a,y+a)=d(x,y)$，$x,y,a\in X$</li>
<li>齐次性：$d(ax,ay)=|a|d(x,y)$，$x,y\in X$，$a\in K$。（$K$是实（或复）数域）。</li>
</ol>
<h2 id="Banach-空间"><a href="#Banach-空间" class="headerlink" title="Banach 空间"></a>Banach 空间</h2><p>如果赋范线性空间是完备的，则称该赋范线性空间是Banach 空间。</p>
<h2 id="内积空间"><a href="#内积空间" class="headerlink" title="内积空间"></a>内积空间</h2><p>（注：这里的括号全都应该为尖括号）<br>设$X$是定义在实（或复）数域$K$上的线性空间，若对于$X$任意一对有序元素$x,y$, 恒对应数域$K$的值$(x, y)$，且满足：</p>
<blockquote>
<ol>
<li>$(ax,y)=a(x,y)$</li>
<li>$(x+y,z)=(x,z)+(y,z)$</li>
<li>$(x,y)=(y,x)$</li>
<li>$(x,x)\ge0$，当且仅当$x=0$时等号成立</li>
</ol>
</blockquote>
<p>则称$X$为内积空间，$(x,y)$称为$x,y$的内积。跟上面内积的概念可以结合理解。</p>
<h2 id="Hilbert-空间"><a href="#Hilbert-空间" class="headerlink" title="Hilbert 空间"></a>Hilbert 空间</h2><p>完备的内积空间称为Hilbert空间，且Hilbert空间必为Banach 空间。<br>或者换种说法：<br>Hilbert空间是：完备的，可能是无限维的，被赋予内积的线性空间。</p>
]]></content>
      <categories>
        <category>学习随记</category>
      </categories>
  </entry>
</search>
