<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>相机模型与标定</title>
    <url>/posts/26.html</url>
    <content><![CDATA[<h1 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h1><p>相机模型化的话，说白了就是一个函数，把一个三维场景转化为二维图像。这里主要是介绍基本相机模型，世界、相机、图像坐标系之间的转换以及相机的标定。<br><span id="more"></span></p>
<h1 id="相机模型"><a href="#相机模型" class="headerlink" title="相机模型"></a>相机模型</h1><p>这里用最简单也是最广泛的针孔相机模型，单目的，也就是下面这个：<img src="/images/272.png" alt=""><br>从3D到2D的转换中我们遗失了<strong>角度</strong>以及<strong>距离</strong>信息。\<br>假设点$P$的三维坐标是$[x,y,z]$，其对应图像坐标为$[x^,,y^,]$，有如下关系：<img src="/images/273.png" alt=""><br>（就是简单的相似三角形）\<br>也就是说:</p>
<script type="math/tex; mode=display">
(x,y,z) \rightarrow (f'\frac{x}{z},f'\frac{y}{z})</script><p>用齐次坐标表示的话为：<img src="/images/274.png" alt=""><br>在很多地方左边的那个等式会写成这样子：</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
    f'&0&0&0\\
    0&f'&0&0\\
    0&0&1&0
\end{pmatrix}
\begin{pmatrix}
    x\\
    y\\
    z\\
    1
\end{pmatrix}
=z
\begin{pmatrix}
    f'\frac{x}{z}\\
    f'\frac{y}{z}\\
    1
\end{pmatrix}</script><p>虽然看上去只是把系数提出来而已，但表达的意思却多了一个：对于同一个图像坐标，其三维坐标有无数种可能，只要在投影线上就行。这是引进齐次坐标的一个很大的特点。</p>
<h1 id="坐标系转变"><a href="#坐标系转变" class="headerlink" title="坐标系转变"></a>坐标系转变</h1><p>上面那种情况是最简单的，但一般来说，相机在世界坐标系中的位置是不定的，所以要在中间引入一个新的坐标系————相机坐标系，又叫归一化坐标系。\<br>所以这里要用到三个坐标系，分别为：世界坐标系，相机坐标系和图像坐标系，它们的转变过程如下：<img src="/images/275.png" alt=""><br>也就是：</p>
<blockquote>
<p>首先将世界坐标转为相机坐标\<br>然后将相机坐标转为图像坐标</p>
</blockquote>
<h2 id="世界坐标到相机坐标"><a href="#世界坐标到相机坐标" class="headerlink" title="世界坐标到相机坐标"></a>世界坐标到相机坐标</h2><p>(这个过程也就是归一化的过程)\<br>相机坐标与世界坐标是旋转加平移的关系，即是如下：</p>
<script type="math/tex; mode=display">
\widetilde{x}^C=R(\widetilde{x}^W-\widetilde{c})</script><p>其中：$\widetilde{c}$是相机中心在世界坐标系中的位置。\<br>写成齐次形式的话就是：<img src="/images/276.png" alt=""><br>这是一种线性变换的形式，这时候维度还没有遗失\<br>这个转换矩阵写简洁一点即是：</p>
<script type="math/tex; mode=display">
x^C=
\begin{bmatrix}
    R&t\\
    0&1
\end{bmatrix}
x^W</script><p>其中$t=-R\widetilde{c}$</p>
<h2 id="相机坐标到图像坐标"><a href="#相机坐标到图像坐标" class="headerlink" title="相机坐标到图像坐标"></a>相机坐标到图像坐标</h2><p>这里先给三个定义：</p>
<blockquote>
<ol>
<li>主轴(Principal axis)：从相机中心垂直于图像平面的线；\</li>
<li>归一化(相机)坐标系(Normalized (camera) coordinate system)：以相机中心为原点，z轴为主轴的坐标系\</li>
<li>主点(Principal point)：主轴与图像平面相交的点(归一化坐标系的原点）</li>
</ol>
</blockquote>
<p>即是下图这样：<img src="/images/277.png" alt=""></p>
<p>相机坐标系的原点在主点上，而图像坐标系的原点在角落(一般是左上角)，也就是说，两个坐标系存在着偏移量，以图像坐标系为参考的话，该偏移量为$(p_x,p_y)$。\<br>将最上面那个投影转换写下来，对于三维坐标点$(X,Y,Z)$，其图像坐标为$(f\frac{X}{Z},f\frac{Y}{Z})$，即：</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
    f&0&0&0\\
    0&f&0&0\\
    0&0&1&0
\end{pmatrix}
\begin{pmatrix}
    X\\
    Y\\
    Z\\
    1
\end{pmatrix}
=
\begin{pmatrix}
    fX\\
    fY\\
    Z
\end{pmatrix}</script><p>写成紧凑一些的形式为：</p>
<script type="math/tex; mode=display">
x^I=K\cdot [I|0]\cdot x^C=K\cdot \widetilde{x}^C</script><p>其中：</p>
<script type="math/tex; mode=display">
K=diag\left\{f,f,1\right\}</script><p>(注意，这里有个非齐次向齐次转换的计算)\<br>然后考虑这个中心偏移量，即对于三维坐标$(X,Y,Z)$，其图像坐标应为$f\frac{X}{Z}+p_x,f\frac{Y}{Z}+p_y$，写成矩阵计算形式如下：</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
    f&0&p_x&0\\
    0&f&p_y&0\\
    0&0&1&0
\end{pmatrix}
\begin{pmatrix}
    X\\
    Y\\
    Z\\
    1
\end{pmatrix}
=
\begin{pmatrix}
    fX+Zp_x\\
    fY+Zp_y\\
    Z
\end{pmatrix}</script><p>这里对应的的$K$为标定矩阵(calibration matrix)，即：</p>
<script type="math/tex; mode=display">
K=
\begin{pmatrix}
    f&0&p_x\\
    0&f&p_y\\
    0&0&1
\end{pmatrix}</script><p>也就是输入一个归一化的世界坐标，通过矩阵$K$可以得到其对应的图像坐标。\<br>但是这里还有一个问题，就是说对于世界坐标系，我们用的单位一般会是“米”，“厘米”，“英寸”之类的，但在图像坐标系中我们统一使用“像素”为单位，所以这还需要一个转换过程：</p>
<blockquote>
<p>假设在水平方向上以某一单位(m,mm,inch,…)为基准共有$m_x$个像素，在垂直方向上有$m_y$个像素。</p>
</blockquote>
<p>就有如下关系：</p>
<script type="math/tex; mode=display">
K=
\begin{pmatrix}
    m_x&0&0\\
    0&m_y&0\\
    0&0&1
\end{pmatrix}
\begin{pmatrix}
    f&0&p_x\\
    0&f&p_y\\
    0&0&1
\end{pmatrix}
=
\begin{pmatrix}
    \alpha_x&0&\beta_x\\
    0&\alpha_y&\beta_y\\
    0&0&1
\end{pmatrix}</script><p>以上这些就是相机的主要内参数，包括有：</p>
<blockquote>
<ol>
<li>主点坐标;\</li>
<li>焦距;\</li>
<li>像素放大系数;\</li>
<li>对于非矩形像素，还有偏斜系数(Skew)，这里不讨论。\</li>
</ol>
</blockquote>
<p>常见的还有畸变参数，这个是为了补充内参数的，下面会讨论。\<br>这些参数是相机在出厂时就设定好的，一般不会轻易改变。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>用一张图简单总结下：<img src="/images/278.png" alt=""><br>这里的矩阵$P$是投影矩阵，即是给一个齐次世界坐标，通过矩阵$P$，可以得到对应的齐次图像坐标，其中$K\in R^{3\times 3}$，$P\in R^{3\times 4}$。</p>
<h1 id="正投影模型"><a href="#正投影模型" class="headerlink" title="正投影模型"></a>正投影模型</h1><p>关于这个简单说一下就好，正投影，也就是平行投影，形式如下：<img src="/images/279.png" alt=""><br>其投影矩阵为：<img src="/images/280.png" alt=""></p>
<h1 id="相机标定"><a href="#相机标定" class="headerlink" title="相机标定"></a>相机标定</h1><p>相机标定就是根据某些方法确定出相机的内参数和畸变参数。</p>
<h2 id="不考虑畸变"><a href="#不考虑畸变" class="headerlink" title="不考虑畸变"></a>不考虑畸变</h2><p>先假设我们有三维坐标点$X_i$，以及对应的图像坐标点$x_i$，都是齐次的，现在想要估计投影矩阵$P$。\<br>由于共线性，有如下关系：</p>
<script type="math/tex; mode=display">
\lambda x_i=PX_i</script><p>即：</p>
<script type="math/tex; mode=display">
\lambda
\begin{pmatrix}
    x_i\\
    y_i\\
    1
\end{pmatrix}=
\begin{bmatrix}
    p_1^T\\
    p_2^T\\
    p_3^T
\end{bmatrix}X_i</script><p>这里$p_j^T$表示矩阵$P$的第$j$行。\<br>用叉积：</p>
<script type="math/tex; mode=display">
x_i \times PX_i=0</script><p>即：</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
    y_ip_3^TX_i-p_2^TX_i\\
    p_1^TX_i-x_ip_3^TX_i\\
    x_ip_2^TX_i-y_ip_1^TX_i
\end{bmatrix}=0</script><p>写成矩阵乘积形式为：</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
    0&-X_i^T&y_iX_i^T\\
    X_i^T&0&-x_iX_i^T\\
    -y_iX_i^T&x_iX_i^T&0
\end{bmatrix}
\begin{pmatrix}
    p_1\\
    p_2\\
    p_3
\end{pmatrix}=0</script><p>这个矩阵$P$虽然有12个参数，但由于尺度是随意的，所以实际只要考虑11个自由度就行，然后由于上面那个矩阵等式只提供了两个线性独立等式(因为秩为2)，所以为了求解矩阵$P$，我们最少需要6个 3D/2D 对应点。如果我们有多于6个数据对的话，那就更好了，可以用最小二乘法估计。\<br>假设我们现在有很多个数据对，根据上面的方法写成如下形式：<img src="/images/281.png" alt=""><br>简单记为：</p>
<script type="math/tex; mode=display">
Ap=0</script><p>由于当$p=0$时虽然符合方程，但明显不是我们想要的，要排除，所以加个约束：$||p||=1$，也就是如下齐次最小二乘形式：</p>
<script type="math/tex; mode=display">
Ap=0\quad s.t. \quad ||p||=1</script><p>先说解法步骤：</p>
<blockquote>
<ol>
<li>进行SVD分解：$A=USV^T$</li>
<li>假设奇异值是排好序了的，即$S=diag(s_1,…,s_{12}),s_{i+1}\leq s_i$</li>
<li>取最后一个右奇异值向量，此时$p=v_{12}$</li>
</ol>
</blockquote>
<p>为什么是这样？下面简单说明。</p>
<h3 id="齐次最小二乘：简介"><a href="#齐次最小二乘：简介" class="headerlink" title="齐次最小二乘：简介"></a>齐次最小二乘：简介</h3><p>现假设向量$p\in R^n$满足：</p>
<script type="math/tex; mode=display">
Ap=0</script><p>其中$A\in R^{m\times n}$且$0\in R^m$，进一步假设$m\geq n$且$rank(A)=n$。\<br>由于$A$是测量矩阵，就难免有噪音，所以我们会最小化下面这个：</p>
<script type="math/tex; mode=display">
||Ap||^2</script><p>为了避免$p=0$这个解，我们在这里加个约束$||p||=1$，也就是说现在我们的问题变为：\</p>
<blockquote>
<p>在约束$||p||=1$的条件下，寻找$p$，使得$||Ap||^2$最小。</p>
</blockquote>
<p>现在开始解这个问题。\<br>首先我们改写这个约束条件为：$1-p^Tp=0$，然后用拉格朗日乘子：</p>
<script type="math/tex; mode=display">
\frac{\partial}{\partial p}(p^TA^TA^Tp+\lambda (1-p^Tp))=0</script><p>即：</p>
<script type="math/tex; mode=display">
2A^TAp-2\lambda p=0 \quad \rightarrow \quad (A^TA)p=\lambda p</script><p>这里$p$是$(A^TA)$的特征向量，$\lambda$是特征值。\<br>这就引出了特征值和特征向量了，但现在还有个问题，我们要选哪个特征值/向量？\<br>回到我们原来的问题，我们要最小化如下的量：</p>
<script type="math/tex; mode=display">
||Ap||^2=p^TA^TAp</script><p>用约束$||p||=1$改写一下如下：</p>
<script type="math/tex; mode=display">
||Ap||^2=p^T\lambda p=\lambda</script><p>这就出来了，我们要选的$p$就是$A^TA$最小特征值对应的特征向量。</p>
<p>然后继续，毕竟上面我们的对象是$A^TA$，不是$A$，现在要引进SVD分解。</p>
<h3 id="齐次最小二乘：SVD"><a href="#齐次最小二乘：SVD" class="headerlink" title="齐次最小二乘：SVD"></a>齐次最小二乘：SVD</h3><p>SVD，即是奇异值分解，全称为singular value decomposition，表示成如下形式：</p>
<script type="math/tex; mode=display">
A=USV^T</script><p>其中$U\in R^{m\times n}$，$V\in R^{n\times n}$是正交的(即是逆等于转置，行列式绝对值为1的矩阵)，$S\in R^{n\times n}$是对角矩阵，且值沿对角线递减。\<br>现在我们有：</p>
<script type="math/tex; mode=display">
A^TA=(USV^T)^TUSV^T=VS^TU^TUSV^T=VS^2V^T</script><p>上面这个式子就是一个特征值分解(正交性)，所以我们可以看到：</p>
<blockquote>
<ol>
<li>$A^TA$的特征向量就是$A$的右奇异向量；</li>
<li>$A^TA$的特征值是$A$奇异值的平方。</li>
</ol>
</blockquote>
<p>所以，要找$A^TA$的最小特征值，就是要计算$A$的最右边的奇异向量。\<br>至此就回答完上面的问题了。\<br>关于SVD分解，详细的可以参看<a href="https://shartoo.github.io/2016/10/25/SVD-decomponent/">这篇</a>。</p>
<h3 id="通过-P-求-K"><a href="#通过-P-求-K" class="headerlink" title="通过$P$求$K$"></a>通过$P$求$K$</h3><p>至此我们就求出了投影矩阵$P$，它包含了相机的外参和内参，但相机的标定只要求内参，所以我们要继续往下做。\<br>这里矩阵$P$是一个$3\times 4$的矩阵，我们把它按如下方式分解：</p>
<script type="math/tex; mode=display">
P=[M|m]=[KR|-KR\widetilde{c}]</script><p>这里矩阵$M\in R^{3\times 3},m\in R^{3\times 1}$，然后将矩阵$M$进行QR分解成一个上三角矩阵$K$和一个正交的矩阵$R$，这里的$K$就是我们要的标定矩阵，$R$是旋转矩阵。最后，通过SVD找到c作为P的零空间。\<br>至此，这种不考虑畸变的标定就结束了。</p>
<h2 id="考虑畸变参数"><a href="#考虑畸变参数" class="headerlink" title="考虑畸变参数"></a>考虑畸变参数</h2><p>上面是特殊情况，一般在现实中我们是不能忽略畸变参数的，在进行标定前，先对畸变参数作简要说明。\</p>
<h3 id="畸变参数"><a href="#畸变参数" class="headerlink" title="畸变参数"></a>畸变参数</h3><p>根据畸变的方式不同畸变参数可分为两部分，径向畸变和切向畸变(还有其他的比如薄透镜畸变，这里不考虑)。</p>
<h4 id="径向畸变"><a href="#径向畸变" class="headerlink" title="径向畸变"></a>径向畸变</h4><p>径向畸变是由于透镜形状的制造工艺导致。且越向透镜边缘移动径向畸变越严重。\<br>比如说这是我们正常的图像：<img src="/images/282.png" alt=""><br>其中的像素点坐标我们用极坐标来表示，为$(r,\theta)$，径向畸变的话，就是$r$的缩放，其又分为两种，分别为桶形畸变和枕形畸变，如下：<img src="/images/283.png" alt=""><br>其中桶形畸变大都发生在使用广角镜头或使用变焦镜头的广角端时，枕形畸变则是使用长焦镜头或使用变焦镜头的长焦端时发生。\<br>用数学形式表示的话，我们先假定归一化平面上有一点$p$，坐标为$[x,y]^T$，对应的极坐标形式为$[r,\theta]^T$，其中$r$表示与坐标系原点的距离，$\theta$表示与水平轴的夹角。径向畸变的话就是$r$发生了变化，畸变后的坐标可表示为：</p>
<script type="math/tex; mode=display">
x_{distorted}=x(1+k_1r^2+k_2r^4+k_3r^6)
$$</script><p>y_{distorted}=y(1+k_1r^2+k_2r^4+k_3r^6)</p>
<script type="math/tex; mode=display">
这里的$k_1,k_2,k_3$即是径向畸变参数。\
这里的多项式不一定非得是这几个，可以多写几项或少些几项，看自己需求吧，不过大都用这几个表示。\

#### 切向畸变
切向畸变是由于透镜本身与相机传感器平面（成像平面）或图像平面不平行而产生的，其畸变来源示意图如下：![](/images/284.png)
同样可用数学表示如下：</script><p>x_{distorted}=x+2p_1xy+p_2(r^2+2x^2)<br>y_{distorted}=y+p_1(r^2+2y^2)+2P_2xy</p>
<script type="math/tex; mode=display">
这里的$p_1,p_2$就是切向畸变参数。

#### 求解畸变参数
首先，根据上面两类畸变可知，一个畸变后的坐标应该是这样的：</script><p>x_{distorted}=x(1+k_1r^2+k_2r^4+k_3r^6)+2p_1xy+p_2(r^2+2x^2)</p>
<script type="math/tex; mode=display">$$
y_{distorted}=y(1+k_1r^2+k_2r^4+k_3r^6)+p_1(r^2+2y^2)+2P_2xy</script><p>这里涉及到5个畸变参数，分别为：$k_1,k_2,k_3,p_1,p_2$。\<br>畸变的话不能直接求解，但可以用优化的思想，目标函数选用最小化重投影误差，也就是将空间坐标点按照估计的投影方程投影到图像上，得到像素估计值，使该值与实际观测值之间的误差最小。\<br>其中变量初始值设为：畸变参数为0，其余参数用上面无畸变标定的方法的结果。\<br>注意，这里的优化变量不只是畸变参数，还有其余内参。\<br>至于其中的优化细节，一大堆公式还没看，留着以后吧。\<br>其实直接用OpenCV或者Matlab就可以弄出来了，尤其是Matlab，真-傻瓜式操作，不过还是得知道这些内部算法的。这个优化细节，以后有心情的话看完再补充了。</p>
<h1 id="附"><a href="#附" class="headerlink" title="附"></a>附</h1><p>关于坐标系的投影变换，可以看<a href="https://superzlw.github.io/posts/1.html">这个</a>，有直接的例子。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>【1】 视觉SLAM 14讲\<br>【2】 <a href="https://shartoo.github.io/2016/10/25/SVD-decomponent/">https://shartoo.github.io/2016/10/25/SVD-decomponent/</a> \<br>【3】 <a href="https://www.qinxing.xyz/posts/b7ea425d/">https://www.qinxing.xyz/posts/b7ea425d/</a> \<br>【4】 <a href="https://zhuanlan.zhihu.com/p/24651968">https://zhuanlan.zhihu.com/p/24651968</a> \<br>【5】 <a href="https://zhuanlan.zhihu.com/p/87334006">https://zhuanlan.zhihu.com/p/87334006</a></p>
]]></content>
      <categories>
        <category>图像处理</category>
      </categories>
  </entry>
  <entry>
    <title>光流三部曲---1. LK光流</title>
    <url>/posts/25.html</url>
    <content><![CDATA[<p>三部分：LK光流，HS光流，概率法。（占个位，考完试再写）</p>
]]></content>
      <categories>
        <category>图像处理</category>
      </categories>
  </entry>
  <entry>
    <title>数据分析中的一些常用Python指令</title>
    <url>/posts/24.html</url>
    <content><![CDATA[<h1 id="引入">引入</h1>
<p>这里就是杂七杂八地记，方便自己查询。顺序基本没什么规律，或者以后可能会整理，看心情~~。总之就善用"ctrl+f"查询。 <span id="more"></span></p>
<h1 id="可视化">可视化</h1>
<h1 id="参考">参考</h1>
<p>【网址1】：https://www.xiaoheidiannao.com/70120.html</p>
]]></content>
      <categories>
        <category>Data Science</category>
      </categories>
  </entry>
  <entry>
    <title>拟合优度检验</title>
    <url>/posts/23.html</url>
    <content><![CDATA[<h1 id="引入">引入</h1>
<p>拟合优度检验是用来检测提出的模型与数据是否一致，这在数据科学领域很重要，因为通过这种检测可以知道：</p>
<blockquote>
<ol type="1">
<li>数据是否符合我们提出的假设（比如高斯性）;<br />
</li>
<li>两个数据集的分布是否可以假定为相同。 <span id="more"></span></li>
</ol>
</blockquote>
<p>一般来说：</p>
<blockquote>
<ol type="1">
<li><strong>离散分布</strong>对应于<strong>Chi-square test</strong> ;<br />
</li>
<li><strong>连续分布</strong>对应于<strong>Kolmogorov Smirnov</strong>。</li>
</ol>
</blockquote>
<h1 id="chi-square-test">Chi-square test</h1>
<p>这个就是经常能听到的卡方检测，卡方检测一般有两种，分别为拟合度的卡方检验和卡方独立性检验，这里重点介绍第一种。<br />
</p>
<h2 id="拟合度卡方检测">拟合度卡方检测</h2>
<p>简单定义的话，就是使用样本数据检验总体分布形态或者比例的假说，或者说得再清楚一点，检验该样本的分布比例与总体分布比例的拟合程度。<br />
拟合度卡方检测又分为如下两种情况：<br />
</p>
<h3 id="case-1-所有参数都是确定的">case 1: 所有参数都是确定的</h3>
<p>由于这是检验分布比例的，所以零假设和备选假设可以写成这样：</p>
<blockquote>
<ol type="1">
<li><span class="math inline">\(H_0\)</span>: <span class="math inline">\(P\left\{ Y=i\right\}=p_i\)</span> vs <span class="math inline">\(H_1\)</span>: <span class="math inline">\(P\left\{ Y=i\right\}\neq p_i\)</span>, <span class="math inline">\(i\in\)</span> {<span class="math inline">\(k\)</span> discrete outcomes}。<br />
</li>
</ol>
</blockquote>
<p>定义如下变量：<br />
</p>
<blockquote>
<p><span class="math inline">\(X_i\)</span> 为输出结果 <span class="math inline">\(i\)</span> 在 <span class="math inline">\(n\)</span> 次重复试验中出现的次数，单看结果 <span class="math inline">\(i\)</span> 的话，可以看成是二项分布，所以有 <span class="math inline">\(E[X_i]=np_i\)</span>。<br />
</p>
</blockquote>
<p>检测统计量为： <span class="math display">\[
T=\sum_i\frac{(x_i-np_i)^2}{np_i}
\]</span> 当次数 <span class="math inline">\(n\)</span> 很大，且零假设成立时，<span class="math inline">\(T\)</span> 可近似看作是 <span class="math inline">\(k-1\)</span> 个自由度的卡方分布，就可以查阅卡方表进行比较了。<br />
</p>
<p>最后根据设置的 <span class="math inline">\(\alpha\)</span> 值，当 <span class="math inline">\(T \geq \chi_{\alpha,k-1}^2\)</span> 时，拒绝零假设，否则接受。<br />
</p>
<h4 id="一个例子">一个例子</h4>
<p>在一个试验中共有6个可能的输出分别是 <span class="math inline">\(a,b,c,d,e,f\)</span>，现假设其出现概率分别为：0.1，0.1，0.05，0.4，0.2，0.15，试验重复40次所得到的各结果频数分别是：3，3，5，18，4，7。问：一开始那个频率假设是否合理？<br />
<strong>零假设<span class="math inline">\(H_0\)</span></strong>：<br />
<span class="math display">\[
P\left\{ Y=a\right\}=0.1, \quad P\left\{ Y=b\right\}=0.1, \quad P\left\{ Y=c\right\}=0.05
\]</span> <span class="math display">\[
P\left\{ Y=d\right\}=0.4, \quad P\left\{ Y=e\right\}=0.2, \quad P\left\{ Y=f\right\}=0.15
\]</span> <strong>计算统计量</strong><br />
将 <span class="math inline">\(n=40\)</span>，6个<span class="math inline">\(p_i\)</span>代入 <span class="math inline">\(T=\sum_i\frac{(x_i-np_i)^2}{np_i}\)</span> 求出 <span class="math inline">\(T=7.416666667\)</span>。<br />
<strong>查表</strong><br />
设<span class="math inline">\(\alpha=0.05\)</span>，自由度<span class="math inline">\(df=6-1=5\)</span>，查表得: <span class="math inline">\(\chi_{0.05,5}^2=11.070\)</span>。<br />
<strong>决策</strong><br />
由于 <span class="math inline">\(T \leq \chi_{0.05,5}^2\)</span>，所以接受零假设。</p>
<h3 id="case-2-某些参数都是未确定的">case 2: 某些参数都是未确定的</h3>
<p>与上面<span class="math inline">\(n\)</span>和<span class="math inline">\(p_i\)</span>都确定的情况不同，这里有些参数是不确定的，需要我们根据其它办法(比如MLE)近似确定。<br />
假设我们有 <span class="math inline">\(m\)</span> 个未指定的参数，需要我们用MLE来确定，比如说泊松分布 <span class="math inline">\(Y\sim Pois(\lambda)\)</span> 的均值的MLE估计为: <span class="math inline">\(\hat{\lambda}=\frac{1}{n}\sum_{j=1}^ny_j\)</span>;<br />
测试统计量为： <span class="math display">\[
T=\sum_i\frac{(x_i-n\hat{p}_i)^2}{n\hat{p}_i}
\]</span> 这里的<span class="math inline">\(\hat{p}_i\)</span>是估计参数。<br />
比如说泊松分布的<span class="math inline">\(pmf\)</span>为： <span class="math inline">\(P(Y=i)=\frac{e^{-\lambda}\lambda^i}{i!}\)</span><br />
将数据分为<span class="math inline">\(k\)</span>组，每组进行分布统计。<br />
若试验次数<span class="math inline">\(n\)</span>足够大且零假设为真，则<span class="math inline">\(T\)</span>近似为卡方分布，自由度为 <span class="math inline">\(k-1-m\)</span>。<br />
对于设定值<span class="math inline">\(\alpha\)</span>，若 <span class="math inline">\(T \geq \chi_{\alpha,k-1-m}^2\)</span>，则拒绝零假设，否则接受。</p>
<h4 id="一个例子-1">一个例子</h4>
<p>我们有下面30个数据，表明30周里每周车祸发生的次数，现在需要验证一周事故发生次数服从泊松分布。<img src="/images/265.png" /> 我们将数据分成如下5组： <span class="math display">\[
(1).Y=0,\quad (2).Y=1,\quad (3).Y=2\;or\; 3, \quad (4). Y=4\;or\;5,\quad (5). Y&gt;5
\]</span></p>
<p><strong>步骤1</strong><br />
首先我们对数据根据分组进行统计，结果如下： <img src="/images/266.png" /> <strong>步骤2</strong><br />
计算估计参数 <span class="math inline">\(\hat{\lambda}\)</span>；<br />
根据上面泊松分布的MLE计算，这个参数就是数据的均值了，等于：<span class="math inline">\(\hat{\lambda}=3.167\)</span>；<br />
<strong>步骤3</strong><br />
根据求出的估计参数，代入泊松分布的 <span class="math inline">\(pmf\)</span> 中，计算各个组的概率： <span class="math display">\[
\hat{p}_1=0.042,\quad \hat{p}_2=0.133,\quad \hat{p}_3=0.434,\quad \hat{p}_4=0.288, \quad \hat{p}_5=0.102
\]</span> <strong>步骤4</strong><br />
计算验证统计量： <span class="math display">\[
T=\sum_{i=1}^5=\frac{(x_i-30\hat{p}_i)^2}{30\hat{p}_i}=21.99
\]</span> <strong>步骤5</strong><br />
查卡方表，根据 <span class="math inline">\(\alpha=0.05\)</span>，自由度<span class="math inline">\(df=k-1-m=5-1-1=3\)</span> 查得：<span class="math inline">\(\chi_{0.05,3}^2=7.815\)</span>。<br />
<strong>步骤6</strong><br />
决策，由于<span class="math inline">\(21.99\geq 7.815\)</span>，所以拒绝零假设，一周事故频数不服从泊松分布。<br />
</p>
<h2 id="卡方独立性检验">卡方独立性检验</h2>
<p>这里只是简单介绍怎么用这个检验，要具体推导的话看<a href="https://zhuanlan.zhihu.com/p/131286213">这里</a>。<br />
卡方独立性检验是为了检验两个变量是否独立，比如说性别与考试通过率是否独立，或者会不会说英语与收到的offer是否独立等。<br />
这里直接用链接里的这个例子，两个变量分别是性别与分期与否。<br />
观测值如下：</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;">分期</th>
<th style="text-align: center;">不分期</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">男</td>
<td style="text-align: center;">90</td>
<td style="text-align: center;">110</td>
<td style="text-align: center;">200</td>
</tr>
<tr class="even">
<td style="text-align: center;">女</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">70</td>
<td style="text-align: center;">100</td>
</tr>
<tr class="odd">
<td style="text-align: center;"></td>
<td style="text-align: center;">120</td>
<td style="text-align: center;">180</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>这里的 90，110，30，70分别用<span class="math inline">\(o_1,o_2,o_3,o_4\)</span>表示。<br />
做零假设：两变量独立。<br />
根据这个零假设，我们期望的值应该是（就是比例相同）：</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;">分期</th>
<th style="text-align: center;">不分期</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">男</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">120</td>
<td style="text-align: center;">200</td>
</tr>
<tr class="even">
<td style="text-align: center;">女</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">100</td>
</tr>
<tr class="odd">
<td style="text-align: center;"></td>
<td style="text-align: center;">120</td>
<td style="text-align: center;">180</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>这里的80，120，40，60分别用<span class="math inline">\(e_1,e_2,e_3,e_4\)</span>表示。<br />
计算卡方统计量： <span class="math display">\[
X=\sum_{i=1}^4\frac{(o_i-e_i)^2}{e_i}=6.25
\]</span> 查表验证。</p>
<h1 id="kolmogorov-smirnov">Kolmogorov Smirnov</h1>
<p>这种检测方法是基本思路就是，比较理论的经验累积分布与观测的经验累积分布，求出最大偏离值，然后判断这种偏离值是不是偶然出现的。<br />
比如说现在有一个连续分布的样本数据：<span class="math inline">\(y_1,...,y_n\)</span>。<br />
我们做出零假设：<span class="math inline">\(F\)</span> 是总体连续分布。<br />
现在验证这个零假设，有两种方法：</p>
<blockquote>
<ol type="1">
<li>将该分布分成不同区间，然后用上面的卡方检测；<br />
</li>
<li>用 K-S 检验。</li>
</ol>
</blockquote>
<p>举个例子，现在有<span class="math inline">\(n=5\)</span>个数据：<span class="math inline">\(y_1,y_2,y_3,y_4,y_5\)</span>，根据这5个数据得出的经验累积函数为： <span class="math display">\[
F_e(x)=\frac{\#i:y_i \leq x}{n}
\]</span> 零假设对应的函数为<span class="math inline">\(F\)</span>，画图如下：<img src="/images/267.png" /> 如果零假设成立，则<span class="math inline">\(F_e(x)\)</span>应该很接近于<span class="math inline">\(F(x)\)</span>。<br />
K-S 检测的统计量为（就是最大距离）： <span class="math display">\[
D \equiv Maximum_x |F_e(x)-F(x)|
\]</span> 下图是sample test: Use the One sample Kolmogorov Smirnov table：<img src="/images/268.png" /> (至于sample tests use the two sample Kolmorogov Smirnov table ，在<a href="https://www.real-statistics.com/statistics-tables/two-sample-kolmogorov-smirnov-table/">这里</a>)<br />
根据<span class="math inline">\(\alpha\)</span>和<span class="math inline">\(n\)</span>就可以读出临界值，两者比较，当最大值小于临界值时，接受零假设，否则拒绝。<br />
<strong>简单说一下双样本的</strong> 双样本检测也差不多，只是多了一组样本，表也多了一个维度，计算距离也不太一样，就，样本数据很大时，距离公式为： <span class="math display">\[
D_{\alpha}=c(\alpha)\sqrt{\frac{n_1+n_2}{n_1n_2}}
\]</span> 系数<span class="math inline">\(c(\alpha)\)</span>为：<img src="/images/269.png" /></p>
<h1 id="总结">总结</h1>
<p>直接附图了这里 <img src="/images/270.png" /></p>
<h1 id="参考">参考</h1>
<p>【1】：https://zhuanlan.zhihu.com/p/131286213</p>
<p>【2】：https://qinqianshan.com/math/significance_test/kolmogorovsmirnov/</p>
]]></content>
      <categories>
        <category>Data Science</category>
      </categories>
  </entry>
  <entry>
    <title>方差分析</title>
    <url>/posts/22.html</url>
    <content><![CDATA[<h1 id="引入">引入</h1>
<p>对于比较两个总体的均值，t-test 是个不错的选择，但假如我们现在有很多个总体呢，t-test 好像就变得不那么理想了，毕竟比较所有成分效率太低。<br />
我们需要一种检测方法，它可以告诉我们在这些总体的任意一个地方是否有任何显著的差异，如果没有的话，那我们就没必要进一步探究了。<br />
要分析均值间的方差，可以用 F test，或者又叫方差分析（Way Analysis of Variance，ANOVA）。 <span id="more"></span></p>
<h1 id="单因素方差分析">单因素方差分析</h1>
<p>在试验过程中，只有一个因素在改变，称为单因素试验，而单因素方差分析主要用来验证这种试验中两组或两组以上的样本均值是否有显著性差异。<br />
举个例子，工厂有5台机器生产同一种零件，我们想知道这几台机器生产的零件重量（或者其它指标）是否一致，就可以转化为验证这5个总体的均值是否一致，这里考察的就是机器这一因素对零件重量有无影响。<br />
如下图（这个图跟这个例子不对应，但意思是一样的，即<strong>组内方差和组间方差</strong>，因为现在还不懂在这里要怎么画图。。）：<img src="/images/259.png" /> 也就是说，F可以写为： <span class="math display">\[
F=\frac{Between \quad group\quad  variability}{Within \quad group\quad variability}=\frac{MS_b}{MS_w}
\]</span> 当：</p>
<blockquote>
<ol type="1">
<li><span class="math inline">\(F=1\)</span>: <span class="math inline">\(H_0\)</span> 为真，每组平均值之间有相同的变化量，与机器无关；</li>
<li><span class="math inline">\(F&gt;1\)</span>: <span class="math inline">\(H_0\)</span> 为假，平均值之间的巨大差异可能不是偶然造成的，表明至少存在一个机器与其它机器不同。</li>
</ol>
</blockquote>
<p>上面只是一个简单的例子，具体地，我们要根据样本的关系进行如下分类。</p>
<h2 id="样本大小相等">样本大小相等</h2>
<p>现假设我们有 <span class="math inline">\(m\)</span> 组独立样本，每组样本有 <span class="math inline">\(n\)</span> 个元素，即可以写成： <span class="math display">\[
X_{ij}\sim \mathcal{N}(\mu_i,\sigma^2)\quad i=1,...,m;\quad j=1,...,n
\]</span> 我们做出如下假设：</p>
<blockquote>
<p><span class="math inline">\(H_0\)</span>: <span class="math inline">\(\mu_1=\mu_2=...=\mu_m\)</span> vs <span class="math inline">\(H_1\)</span>: 并非所有均值都相等。<br />
</p>
</blockquote>
<p>继续往下，方差分析这里最重要的就是计算组内以及组间方差，为此需要计算如下两种均值：</p>
<blockquote>
<ol type="1">
<li>组内均值：令 <span class="math inline">\(\overline{x}_{i\cdot}\)</span> 为第 <span class="math inline">\(i\)</span> 组样本的均值，即： <span class="math display">\[
\overline{x}_{i\cdot}=\sum_{j=1}^n\frac{x_{ij}}{n}
\]</span></li>
<li>总体均值：用<span class="math inline">\(\overline{x}_{\cdot \cdot}\)</span>表示对<span class="math inline">\(\mu\)</span>的估计，即： <span class="math display">\[
\overline{x}_{\cdot \cdot} = \frac{\sum_{i=1}^m\sum_{j=1}^nx_{ij}}{nm}=\frac{\sum_{i=1}^m\overline{x}_{i\cdot}}{m}
\]</span></li>
</ol>
</blockquote>
<p>然后通过下面表格就可以计算出F的值：<img src="/images/260.png" /></p>
<h2 id="样本大小不等">样本大小不等</h2>
<p>现假设我们有 <span class="math inline">\(m\)</span> 组独立样本，每组样本有 <span class="math inline">\(n_1,...,n_m\)</span> 个元素，即可以写成： <span class="math display">\[
X_{ij}\sim \mathcal{N}(\mu_i,\sigma^2)\quad i=1,...,m;\quad j=1,...,n
\]</span> 同样我们做出如下假设：</p>
<blockquote>
<p><span class="math inline">\(H_0\)</span>: <span class="math inline">\(\mu_1=\mu_2=...=\mu_m\)</span> vs <span class="math inline">\(H_1\)</span>: 并非所有均值都相等。<br />
</p>
</blockquote>
<p>依旧计算两种均值：</p>
<blockquote>
<ol type="1">
<li>组内均值：<span class="math inline">\(\overline{x}_{i\cdot}=\sum_{j=1}^{n_i}\frac{x_{ij}}{n_i}\)</span><br />
<br />
</li>
<li>总体均值：<span class="math inline">\(\overline{x}_{\cdot \cdot} = \frac{\sum_{i=1}^m\sum_{j=1}^{n_i}x_{ij}}{\sum_{i=1}^mn_i}\)</span></li>
</ol>
</blockquote>
<p>令 <span class="math inline">\(N = \sum_in_i-m\)</span>。<br />
然后通过下面表格就可以计算出F的值：<img src="/images/261.png" /></p>
<h2 id="一个例子">一个例子</h2>
<p>下图表示从5种不同的方法中采集的数据，现在我们要判断这5种不同的方法是否会给出不同的结果。<br />
<img src="/images/262.png" /> 均值之类的都已经算好了，直接带入上面表格里的式子可得： <span class="math display">\[
SS_b=150.50 \quad SS_w=87.43
\]</span> 则均方根(mean Squares) 为： <span class="math display">\[
MS_b=\frac{150.50}{4}=37.63 \quad MS_w=\frac{87.43}{26}=3.36
\]</span> 测试统计量为：<span class="math inline">\(\frac{MS_b}{MS_w}=11.19\)</span><br />
通过查找F分布表可得：<span class="math inline">\(F_{4,26,0.05}=2.743\)</span><br />
由于11.19 &gt; 2.743，所以拒绝零假设。</p>
<h1 id="双因素方差分析">双因素方差分析</h1>
<p>字面意思，这里的方差分析涉及到两个因素，比如说，我们要探讨不同温度及营养元素对培养皿微生物繁殖的影响，就可以使用双因素方差分析。<br />
另一个例子，如下图：<img src="/images/263.png" /> 给出的是不同温度与材料下发动机的寿命(月)。<br />
计算跟单方差分析还是挺类似的，定义以下变量：<span class="math inline">\(\mu,\alpha_i,\beta_j,\gamma_{ij},i=1,...,m,j=1,...,n\)</span>，并有如下关系：</p>
<blockquote>
<ol type="1">
<li><span class="math inline">\(\mu=\mu_{\cdot\cdot}\)</span>：总体平均值（即所有元素）;<br />
</li>
<li><span class="math inline">\(\alpha_i=\mu_{i\cdot}-\mu_{\cdot\cdot}\)</span>：第<span class="math inline">\(i\)</span>行的影响;<br />
</li>
<li><span class="math inline">\(\beta_j=\mu_{\cdot j}-\mu_{\cdot\cdot}\)</span>：第<span class="math inline">\(j\)</span>列的影响;<br />
</li>
<li><span class="math inline">\(\gamma_{ij}=\mu_{ij}-(\mu+\alpha_i+\beta_j)=\mu_{ij}-\mu_{i\cdot}-\mu_{\cdot j}+\mu_{\cdot\cdot}\)</span>：行𝑖和列𝑗的相互作用。<br />
（这里 <span class="math inline">\(\mu_{ij}=E[X_{ij}]\)</span>）</li>
</ol>
</blockquote>
<p>零假设这里有三个：</p>
<blockquote>
<ol type="1">
<li><span class="math inline">\(H_0^r: \alpha_i=0\)</span>, for all i（即是行没有影响）;<br />
</li>
<li><span class="math inline">\(H_0^c: \beta_j=0\)</span>, for all j（即是列没有影响）;<br />
</li>
<li><span class="math inline">\(H_0^{int}: \gamma_{ij}=0\)</span>, for all i, j（即是行列没有相互作用）。</li>
</ol>
</blockquote>
<p>更一般的情况，也就是每行每列多个观测值。<br />
现假设每行每列有 <span class="math inline">\(l\)</span> 个观测值，比如上面那张图里 <span class="math inline">\(l=3\)</span>，并假设所有观测值都是独立的正态随机变量，方差均为 <span class="math inline">\(\sigma^2\)</span>。<br />
假设数据表示为 <span class="math inline">\(X_{ijk}\)</span>, <span class="math inline">\(i=1,...,m \quad j=1,...,n \quad k=1,...,l\)</span>，则无偏估计为：</p>
<blockquote>
<p><span class="math inline">\(\hat{\mu}=\overline{x}_{\cdot\cdot\cdot}\)</span><br />
<span class="math inline">\(\hat{\alpha}_i=\overline{x}_{i\cdot\cdot}-\overline{x}_{\cdot\cdot\cdot}\)</span><br />
<span class="math inline">\(\hat{\beta}_j=\overline{x}_{\cdot j \cdot}-\overline{x}_{\cdot \cdot\cdot}\)</span><br />
<span class="math inline">\(\hat{\gamma}_{ij}=\overline{x}_{ij\cdot}-\overline{x}_{\cdot j \cdot}+\overline{x}_{\cdot\cdot\cdot}\)</span></p>
</blockquote>
<p>最后就可以根据下图，计算，查表然后判断了：<img src="/images/264.png" /> 可以拿上面那个发动机的数据计算，这里就不写了，就只是代公式而已，最后结果会是：拒绝<span class="math inline">\(H_0^c\)</span>, 接受另外两个零假设。<br />
</p>
]]></content>
      <categories>
        <category>Data Science</category>
      </categories>
  </entry>
  <entry>
    <title>假设性检测 (t-test, z-test)</title>
    <url>/posts/21.html</url>
    <content><![CDATA[<h1 id="引入">引入</h1>
<p>在实际生活中，我们处理的往往是很大量的数据，分析时经常要对这些数据的某种规律提出一个假设，但我们有时没法直接用总数据来验证这个假设，而是通过样本数据来推断，决定是否拒绝这一假设，这样的统计活动成为假设检验。<br />
假设检测方法的均值对比方法主要有t-test和z-test，以下介绍这两个检测以及中间涉及到的F-test。 <span id="more"></span></p>
<p>该检测一般可以简单概况为以下4个步骤：</p>
<blockquote>
<ol type="1">
<li>提出零假设<span class="math inline">\(H_0\)</span>以及对应的备选假设<span class="math inline">\(H_1\)</span>(分为以下两种):<br />
  Non-directional(two tailed), e.g. <span class="math inline">\(H_0:\mu=10,H_1:\mu_0\neq 10\)</span>;<br />
  Directional(one tailed), e.g. <span class="math inline">\(H_0:\mu=10,H_1:\mu_0&gt;10\)</span>(or <span class="math inline">\(\mu_0&lt;10\)</span>);<br />
</li>
<li>设定拒绝<span class="math inline">\(H_0\)</span>的标准:<br />
  Set the significance level <span class="math inline">\(\alpha\)</span>, e.g. 0.05;<br />
  Find the criteria for a decision: the critical value in z- or t-distribution;<br />
  Two tailed for non directional alternative hypothesis;<br />
  One tailed for directional alternative hypothesis;<br />
</li>
<li>计算测试统计数据:<br />
  <span class="math inline">\(\sigma\)</span>已知：z-score;<br />
  <span class="math inline">\(\sigma\)</span>未知：t-score;<br />
  <span class="math inline">\(\sigma\)</span>未知但样本数据非常大：z-score。<br />
</li>
<li>决定是否拒绝零假设。<br />
</li>
</ol>
</blockquote>
<h1 id="one-sample-case-for-the-mean">One sample case for the mean</h1>
<h2 id="z-test">z-test</h2>
<p>首先给一个图：<img src="/images/251.png" /> 如果 <span class="math inline">\(\overline{x}\)</span> 是正态分布的话，则<span class="math inline">\(z=\frac{\overline{x}-\mu_0}{\sigma_{\overline{x}}}\)</span>也是正态分布的（这里<span class="math inline">\(\sigma_{\overline{x}}\)</span>是确定或已知的）。<br />
对于z检验，我们使用（累积）正态分布的表格来找出<span class="math inline">\(z_{\alpha}\)</span>的值。<br />
</p>
<h3 id="一个例子">一个例子</h3>
<p>现用一个例子来说明z-test的步骤。<br />
以下20个样本是从已知标准差为5的正态分布中产生的。<img src="/images/252.png" /> 现要检测该群体的均值是否大于6。<br />
</p>
<blockquote>
<ol type="1">
<li><strong>Step 1</strong>: 设置假设：<span class="math inline">\(H_0: \mu=6\)</span> vs <span class="math inline">\(H_1:\mu&gt;6\)</span>;<br />
</li>
<li><strong>Step 2</strong>: 设置<span class="math inline">\(\alpha\)</span>，比如说等于0.05，然后从下表中查出其临界值为：<span class="math inline">\(z_{\alpha}=1.645\)</span> <img src="/images/253.png" /></li>
<li><strong>Step 3</strong>: 计算样本的均值和标准差：<img src="/images/254.png" /></li>
<li><strong>Step 4</strong>: 计算检测统计值： <span class="math display">\[
z=\frac{\overline{x}-\mu_0}{\sigma_{\overline{x}}}=\frac{5.572-6}{6.287/\sqrt{20}}=-0.304
\]</span></li>
<li><strong>Step 5</strong>: 做决策：由于<span class="math inline">\(z &lt; z_{\alpha}\)</span>，所以不拒绝零假设<span class="math inline">\(H_0\)</span>。</li>
</ol>
</blockquote>
<h2 id="t-test">t-test</h2>
<p>依旧先给一个图：<img src="/images/255.png" /> 如果 <span class="math inline">\(\overline{x}\)</span> 是正态分布的话，则<span class="math inline">\(t=\frac{\overline{x}-\mu_0}{s_x}\)</span>服从 t 分布（因为<span class="math inline">\(s_x\)</span>是一个估计值，因此是另一个随机变量的输出）。<br />
这意味着，对于t检验，我们使用t分布的表格来找出𝑡的值。这个表比上面那个累积正态表复杂一丢丢，这里大概说一下怎么查。<br />
首先这个t分布表长这样：<img src="/images/256.png" /> <span class="math inline">\(t_{\alpha}\)</span>是基于<span class="math inline">\((n-1)\)</span>的，这个值称为自由度degree of freedom (df)。<br />
举个例子，比如说样本尺寸是7，单尾，则<span class="math inline">\(t_{0.05}\)</span>为1.9432。<br />
</p>
<h1 id="two-sample-case-for-the-mean">Two sample case for the mean</h1>
<p>上面的都是单样本的情况，这里开始介绍双样本，字面意思，就是两个样本（好像在说废话。。。。）。<br />
跟上面的区别还是有点大的，主要有以下两个方面：<br />
</p>
<blockquote>
<ol type="1">
<li>假设不同，举两个例子：<br />
  女学生的GPA平均值是否要高于男学生；<br />
  在数据科学领域，女性的工资平均值是否高于男性；<br />
</li>
<li>情况不同：<br />
  样本独立：<br />
    方差已知；<br />
    方差未知：<br />
      总体方差未知但总体方差相等；<br />
      总体方差未知但总体方差不相等；<br />
      总体方差未知，也不知道它们的关系。<br />
  样本不独立。</li>
</ol>
</blockquote>
<p>以下对上面情况进行分别讨论。</p>
<h2 id="样本独立方差已知">样本独立，方差已知</h2>
<p>方差已知的话就是z-test了。<br />
两总体均值差异为：<span class="math inline">\(\overline{X}-\overline{Y}\sim \mathcal{N}(\mu_X-\mu_Y,\frac{\sigma_X^2}{n}+\frac{\sigma_Y^2}{m})\)</span>，其中<span class="math inline">\(n,m\)</span>分别为<span class="math inline">\(X,Y\)</span>两样本的大小。<br />
上面这个复杂的正态分布也可以写为：<span class="math inline">\(\frac{\overline{X}-\overline{Y}-(\mu_X-\mu_Y)}{\sqrt{\frac{\sigma_X^2}{n}+\frac{\sigma_Y^2}{m}}}\sim \mathcal{N}(0,1)\)</span>，其实就是z-score。<br />
所以，当<span class="math inline">\(H_0\)</span>为真时，即 <span class="math inline">\(\mu_X-\mu_Y=0\)</span> 时，z-test的统计量为： <span class="math display">\[
z=\frac{\overline{X}-\overline{Y}}{\sqrt{\frac{\sigma_X^2}{n}+\frac{\sigma_Y^2}{m}}}
\]</span></p>
<h2 id="样本独立方差未知">样本独立，方差未知</h2>
<p>方差未知即是用t-test。</p>
<h3 id="case-1方差相等">case 1：方差相等</h3>
<p>方差相等即：<span class="math inline">\(\mu_X^2=\mu_Y^2\)</span>。<br />
测试统计量为：<span class="math inline">\(t=\frac{(\overline{x}-\overline{y})}{s_{\overline{x}-\overline{y}}}\)</span>，其中： <span class="math display">\[
s_{\overline{x}-\overline{y}}=\sqrt{s^2(\frac{1}{n}+\frac{1}{m})}
\]</span> <span class="math display">\[
s^2=\frac{\sum^n_{i=1}(x_i-\overline{x})^2+\sum^m_{j=1}(y_j-\overline{y})^2}{n+m-2}
\]</span> 或： <span class="math display">\[
s^2=\frac{(n-1)s_x^2+(m-1)s_y^2}{n+m-2}
\]</span> <span class="math display">\[
df=n+m-2
\]</span></p>
<h3 id="case-2方差不相等">case 2：方差不相等</h3>
<p>方差不相等即：<span class="math inline">\(\mu_X^2\neq\mu_Y^2\)</span>。<br />
测试统计量为：<span class="math inline">\(t=\frac{(\overline{x}-\overline{y})}{s_{\overline{x}-\overline{y}}}\)</span>，其中： <span class="math display">\[
s_{\overline{x}-\overline{y}}=\sqrt{\frac{s_x^2}{n}+\frac{s_y^2}{m}}=\sqrt{s_{\overline{x}}^2+s_{\overline{y}}^2}
\]</span> <span class="math display">\[
df = \frac{(s_{\overline{x}}^2+s_{\overline{y}}^2)^2}{\frac{(s_{\overline{x}}^2)^2}{n-1}+\frac{(s_{\overline{y}}^2)^2}{m-1}}
\]</span><br />
<strong>上面两个case中：</strong><br />
<span class="math inline">\(s_{\overline{x}}^2=\frac{s_x^2}{n};\quad s_{\overline{y}}^2=\frac{s_y^2}{m}; \quad s_x^2=\frac{\sum_{i=1}^n(x_i-\overline{x})^2}{n-1}; \quad s_y^2=\frac{\sum_{j=1}^m(y_j-\overline{y})^2}{m-1}\)</span></p>
<h3 id="case-3方差关系未知">case 3：方差关系未知</h3>
<p>这里我们不知道 <span class="math inline">\(\sigma_X^2\)</span> 和 <span class="math inline">\(\sigma_Y^2\)</span> 相不相等，首先用 <span class="math inline">\(F_{\max} test\)</span> 检测方差的相等关系。<br />
这里简单补充下 <span class="math inline">\(F_{\max}test\)</span> 的相关内容。(这里仅补充这里需要用到的，更具体的看另一篇【F-test 方差分析】)。<br />
</p>
<h4 id="f_maxtest"><span class="math inline">\(F_{\max}test\)</span></h4>
<p>F 检测是一种方差差异性检测。<br />
测试统计量为：<span class="math inline">\(F=\frac{s_x^2}{s_y^2}\)</span>，一般把较大的放分子上。<br />
F的临界值由以下三部分确定：<br />
</p>
<blockquote>
<ol type="1">
<li>置信度<span class="math inline">\(\alpha\)</span>；</li>
<li>分子的自由度<span class="math inline">\(df\)</span>；</li>
<li>分母的自由度<span class="math inline">\(df\)</span>。</li>
</ol>
</blockquote>
<p>根据这三个条件就可以查表了，由于F检测更多的是确定置信区间，所以要记得下面这个等式。 <span class="math display">\[
\frac{1}{F_{1-\frac{\alpha}{2}},m,n}=F_{\frac{\alpha}{2},n,m}
\]</span> (注意这里 <span class="math inline">\(m,n\)</span> 的顺序)。<br />
决策依据是：<br />
</p>
<blockquote>
<ol type="1">
<li>拒绝<span class="math inline">\(H_0\)</span>: <span class="math inline">\(F&gt;F_{\frac{\alpha}{2}} \quad \rightarrow s_x^2 &gt; s_y^2\)</span>；<br />
</li>
<li>拒绝<span class="math inline">\(H_0\)</span>: <span class="math inline">\(F&lt; F_{1-\frac{\alpha}{2}} \quad \rightarrow s_x^2 &lt; s_y^2\)</span>；<br />
</li>
</ol>
</blockquote>
<p>置信区域如下：<img src="/images/258.png" /> 这里简单推导一下这个等式的由来。<br />
设<span class="math inline">\(F\sim F(n,m)\)</span>，则<span class="math inline">\(\frac{1}{F}\sim F(m,n)\)</span>。若 <span class="math display">\[
P\left\{ F&gt;F_{1-\frac{\alpha}{2}}(n,m)\right\}=1-\frac{\alpha}{2}
\]</span> 即： <span class="math display">\[
P\left\{ F&lt; F_{1-\frac{\alpha}{2}}(n,m)\right\}=\frac{\alpha}{2}
\]</span> 则有： <span class="math display">\[
P\left\{ \frac{1}{F}&gt;\frac{1}{F_{1-\frac{\alpha}{2}}(n,m)}\right\}=\frac{\alpha}{2}
\]</span> 由于<span class="math inline">\(\frac{1}{F}\sim F(m,n)\)</span>，转换一下就有： <span class="math display">\[
P\left\{ \frac{1}{F}&gt;{F_{\frac{\alpha}{2}}(m,n)}\right\}=\frac{\alpha}{2}
\]</span> 所以就得到了上面的等式： <span class="math display">\[
\frac{1}{F_{1-\frac{\alpha}{2}},m,n}=F_{\frac{\alpha}{2},n,m}
\]</span></p>
<p><strong>回到 case 3</strong><br />
运用F-test 进行方差检测，即：<br />
</p>
<blockquote>
<ol type="1">
<li><p><span class="math inline">\(H_0\)</span>: <span class="math inline">\(\sigma_X^2=\sigma_Y^2\)</span> 或 <span class="math inline">\(\frac{\sigma_X^2}{\sigma_Y^2}=1\)</span><br />
</p></li>
<li><p><span class="math inline">\(H_1\)</span>: <span class="math inline">\(\sigma_X^2 \neq \sigma_Y^2\)</span> 或 <span class="math inline">\(\frac{\sigma_X^2}{\sigma_Y^2}\neq 1\)</span><br />
(non-directional / two-tailed)</p></li>
</ol>
</blockquote>
<p>然后根据 F-test 的结果用上面 case 1 或者 case 2 的方法继续下去。</p>
<h4 id="case-3-的一个例子">case 3 的一个例子</h4>
<p>现在有两个导师A，B教同一门课，现在要探究哪个导数教的学生成绩更好。有以下数据：</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;">amount</th>
<th style="text-align: center;">average</th>
<th style="text-align: center;">Standard deviation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">A</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">87</td>
<td style="text-align: center;">15</td>
</tr>
<tr class="even">
<td style="text-align: center;">B</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">10</td>
</tr>
</tbody>
</table>
<p>A导师说他更厉害，那我们现在就想检测他说的是不是真的。<br />
首先我们要检测两总体的方差是否相等：<span class="math inline">\(H_0:\sigma_A^2=\sigma_B^2\)</span> vs <span class="math inline">\(H_1:\sigma_A^2\neq \sigma_B^2\)</span>;<br />
</p>
<blockquote>
<ol type="1">
<li><span class="math inline">\(F_{\max}\)</span>test：<span class="math inline">\(F=\frac{s_A^2}{s_B^2}=\frac{15^2}{10^2}=2.25\)</span>；<br />
</li>
<li>A, B 两类均有7个样本，则自由度均为：<span class="math inline">\(df=n-1=7-1=6\)</span>；<br />
</li>
<li>选取 <span class="math inline">\(\alpha\)</span> 值，这里令 <span class="math inline">\(\alpha = 0.05\)</span>，寻找寻找分子分母自由度均为6的边界值 <span class="math inline">\(F_{cv}\)</span>；<br />
</li>
<li>从下面这个表中读取边界值：<img src="/images/257.png" /> 边界值 <span class="math inline">\(F_{\alpha/2}=5.82\)</span>，即<span class="math inline">\(F_{1-\alpha/2}=\frac{1}{5.82}=0.17\)</span></li>
<li>由于 <span class="math inline">\(F\)</span> 是位于0.17和5.82之间的，所以接受零假设，<span class="math inline">\(\sigma_A^2=\sigma_B^2\)</span>。</li>
</ol>
</blockquote>
<p>我们得出了方差相等，现在继续往下，为验证导师A的话，做出如下新的假设：<br />
</p>
<blockquote>
<ol type="1">
<li><span class="math inline">\(H_0: \mu_A=\mu_B\)</span> vs <span class="math inline">\(H_1: \mu_A&gt;\mu_B\)</span>，用case 1的方法;<br />
</li>
<li>计算：<span class="math inline">\(s_{\overline{A}-\overline{B}}=\sqrt{\frac{(n-1)s_A^2+(m-1)s_B^2}{n+m-2}(\frac{1}{n}+\frac{1}{m})}=6.81\)</span><br />
</li>
<li>t 的统计量为：<span class="math inline">\(t=\frac{87-80}{6.81}=1.03\)</span>，自由度为：7+7-2=12;<br />
</li>
<li>边界值为（one tailed）：<span class="math inline">\(t_{0.05,12}=1.782\)</span>；<br />
</li>
<li>决策：由于 1.03 &lt; 1.782，我们接受零假设，导师A的说法不靠谱。</li>
</ol>
</blockquote>
<p>另外多说一点，如果上面的数据，其它不变，但样本都变成700的话，就会发现导师A的说法是可信的，这个自行计算。<br />
值得注意的是，如果样本 n 很大，我们也可以用 z-test，700已经足够大了，有700个自由度的 t 分布已经很接近于正态分布了。<br />
</p>
]]></content>
      <categories>
        <category>Data Science</category>
      </categories>
  </entry>
  <entry>
    <title>卡尔曼滤波（Kalman Filter）</title>
    <url>/posts/20.html</url>
    <content><![CDATA[<h1 id="写在前面的说明">写在前面的说明</h1>
<p>本文总结自B站博主DR_CAN，他是整个B站我最喜欢的博主没有之一，推荐大家直接去看他的视频，这篇当作学后的回顾，链接在这<span class="math inline">\(\rightarrow\)</span><a href="https://space.bilibili.com/230105574/channel/detail?cid=139198&amp;ctype=0">【传送门】</a>。 <span id="more"></span></p>
<h1 id="引入-递归算法">引入 —— 递归算法</h1>
<p>卡尔曼滤波器可看作是一种“Optimal Recursive Data Processing Algorithm”，即“最优的递归数字处理算法”。与滤波器相比它更像是一种观测器。<br />
卡尔曼滤波器的应用非常广泛，尤其是导航中，这是因为现实生活中充满大量的不确定性，当我们描述一个系统的时候，这个不确定性主要体现在以下三个方面：<br />
</p>
<blockquote>
<ol type="1">
<li>不存在完美的数学模型；<br />
</li>
<li>系统的挠动是不可控的，也很难建模；<br />
</li>
<li>测量传感器存在误差。 </li>
</ol>
</blockquote>
<p>举个例子：<br />
当我们要测量一枚硬币的直径时，设第<span class="math inline">\(k\)</span>次测量结果为<span class="math inline">\(z_k\)</span>，假如三次测量分别为： <span class="math display">\[z_1=50.1mm\]</span> <span class="math display">\[z_2=50.4mm\]</span> <span class="math display">\[z_3=50.2mm\]</span> 若要估计真实数据，很自然地，我们会用平均值来表示，即第<span class="math inline">\(k\)</span>次的估计值<span class="math inline">\(\hat{x}_k\)</span>为：</p>
<p><span class="math display">\[
\begin{split}
\hat{x}_k &amp;= \frac{1}{k}(z_1+...+z_k)\\\\ 
 &amp;= \frac{1}{k}(z_1+...+z_{k-1})+\frac{1}{k}z_k\\\\ &amp;=
\frac{1}{k}\frac{k-1}{k-1}(z_1+...+z_{k-1})+\frac{1}{k}z_k \\\\
&amp;=\frac{k-1}{k}\hat{x}_{k-1}+\frac{1}{k}z_k\\\\
&amp;=\hat{x}_{k-1}-\frac{1}{k}\hat{x}_{k-1}+\frac{1}{k}z_k
\end{split}
\]</span></p>
<p>即是：</p>
<p><span class="math display">\[
\hat{x}_k=\hat{x}_{k-1}+\frac{1}{k}(z_k-\hat{x}_{k-1})
\]</span></p>
<p>由此可见，当<span class="math inline">\(k\)</span>逐渐增加时，<span class="math inline">\(\frac{1}{k}\rightarrow \infty\)</span>，也就是说<span class="math inline">\(\hat{x}_k\rightarrow \hat{x}_{k-1}\)</span>。<br />
用文字描述的话，就是说随着<span class="math inline">\(k\)</span>的增加，之后测量结果也就不再重要了，也就是对前面的真实估计值已经非常有信心了。<br />
相反，当<span class="math inline">\(k\)</span>比较小时，<span class="math inline">\(\frac{1}{k}\)</span>就会很大，也就是说此时<span class="math inline">\(z_k\)</span>的作用会很大。<br />
现在改写一下上面的式子为： <span class="math display">\[
\hat{x}_k=\hat{x}_{k-1}+K_k(z_k-\hat{x}_{k-1})
\]</span> 在卡尔曼滤波器里，这个系数<span class="math inline">\(K_k\)</span>就叫Kalman Gain，中文称为卡尔曼增益/因数。<br />
由这个式子可以看出，跟马尔科夫链类似，当前估计值只与该次测量值以及上一次的估计值有关，这体现了一种递归的思想。<br />
这里先对这个系数作简单的讨论。首先引入两个变量：</p>
<blockquote>
<ol type="1">
<li>估计误差：<span class="math inline">\(e_{EST}\)</span>；<br />
</li>
<li>测量误差：<span class="math inline">\(e_{MEA}\)</span>。</li>
</ol>
</blockquote>
<p>卡尔曼增益就等于： <span class="math display">\[
K_k=\frac{e_{EST_{k-1}}}{e_{EST_{k-1}}+e_{MEA_k}}
\]</span> <strong>这个公式是卡尔曼滤波的核心公式，详细推导见后文。</strong><br />
这里先简单讨论在<span class="math inline">\(k\)</span>时刻两个变量的作用：</p>
<blockquote>
<ol type="1">
<li>当<span class="math inline">\(e_{EST_{k-1}} \gg e_{MEA_k}\)</span>，即<span class="math inline">\(k\rightarrow 1\)</span>时，<span class="math inline">\(\hat{x}_k=\hat{x}_{k-1}+z_k-\hat{x}_{k-1}=z_{k}\)</span>；<br />
</li>
<li>当<span class="math inline">\(e_{EST_{k-1}} \ll e_{MEA_k}\)</span>，即<span class="math inline">\(k\rightarrow 0\)</span>时，<span class="math inline">\(\hat{x}_k=\hat{x}_{k-1}\)</span>。</li>
</ol>
</blockquote>
<p>运用卡尔曼滤波器，真实估计值就可以表示为以下三个步骤：</p>
<blockquote>
<ol type="1">
<li><strong>Step 1</strong>：计算卡尔曼增益：<span class="math inline">\(K_k=\frac{e_{EST_{k-1}}}{e_{EST_{k-1}}+e_{MEA_k}}\)</span>；<br />
</li>
<li><strong>Step 2</strong>：计算当前估计值<span class="math inline">\(\hat{x}_k=\hat{x}_{k-1}+K_k(z_k-\hat{x}_{k-1})\)</span>；</li>
<li><strong>Step 3</strong>：更新<span class="math inline">\(e_{EST_k}=(1-K_k)e_{EST_{k-1}}\)</span>（这个式子后面推导）。</li>
</ol>
</blockquote>
<p>这里举个例子说明上面三个步骤：<br />
假设我们要测量一个长度是<span class="math inline">\(50mm\)</span>的物体，第0次的估计值，也就是相当于先验吧，是<span class="math inline">\(\hat{x}_0=40mm\)</span>，第0次估计误差为<span class="math inline">\(e_{EST_0}=5mm\)</span>，第一次测量为<span class="math inline">\(z_1=51mm\)</span>，测量误差一直为<span class="math inline">\(e_{MEA_k}=3mm\)</span>。<br />
填入起始值，如下表：<br />
</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;"><span class="math inline">\(k\)</span></th>
<th style="text-align: center;"><span class="math inline">\(z_k\)</span></th>
<th style="text-align: center;"><span class="math inline">\(e_{MEA_k}\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\hat{x}_k\)</span></th>
<th style="text-align: center;"><span class="math inline">\(K_k\)</span></th>
<th style="text-align: center;"><span class="math inline">\(e_{EST_k}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">40</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">5</td>
</tr>
<tr class="even">
<td style="text-align: center;">1</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>借助上面三个步骤：<br />
<span class="math inline">\(k=1\)</span>时： <span class="math display">\[
K_k=\frac{5}{5+3}=0.625
\]</span> <span class="math display">\[
\hat{x}_k=40+0.625(51-40)=46.875
\]</span> <span class="math display">\[
e_{EST}=(1-0.625)\times 5 = 1.875
\]</span> 然后<span class="math inline">\(k=2\)</span>时，设此次测量值为<span class="math inline">\(z_2=48\)</span>，重复上面的步骤，就可以填满表格了。如下：</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;"><span class="math inline">\(k\)</span></th>
<th style="text-align: center;"><span class="math inline">\(z_k\)</span></th>
<th style="text-align: center;"><span class="math inline">\(e_{MEA_k}\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\hat{x}_k\)</span></th>
<th style="text-align: center;"><span class="math inline">\(K_k\)</span></th>
<th style="text-align: center;"><span class="math inline">\(e_{EST_k}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">40</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">5</td>
</tr>
<tr class="even">
<td style="text-align: center;">1</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">46.88</td>
<td style="text-align: center;">0.625</td>
<td style="text-align: center;">1.875</td>
</tr>
<tr class="odd">
<td style="text-align: center;">2</td>
<td style="text-align: center;">48</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">47.31</td>
<td style="text-align: center;">0.385</td>
<td style="text-align: center;">1.154</td>
</tr>
<tr class="even">
<td style="text-align: center;">3</td>
<td style="text-align: center;">47</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">47.22</td>
<td style="text-align: center;">0.278</td>
<td style="text-align: center;">0.833</td>
</tr>
</tbody>
</table>
<p>这样不断下去，最后这个真实估计值会收敛于<span class="math inline">\(50mm\)</span>。<br />
借用博主视频里的数值结果图片，其表现如下： <img src="/images/250.png" /> 其中蓝色线表示测量值，红色线表示估计值。</p>
<h1 id="数据融合协方差矩阵状态空间方程">数据融合，协方差矩阵，状态空间方程</h1>
<p>这部分是复习基础知识，可跳过。</p>
<h2 id="数据融合">数据融合</h2>
<p>简单讲，就是从多个测量数据中找到一个最优的。<br />
举个例子，我们在称量一个物体的时候，两次称量结果如下： <span class="math display">\[
z_1=30g, \quad \sigma_1=2g
\]</span> <span class="math display">\[
z_2=32g, \quad \sigma_2=4g
\]</span> 根据正态分布的性质，对于第一个测量结果，真实值落在区间[28,32]的概率是68.4%；对第二个测量结果，真实值落在区间[28,36]的概率是68.4%，如果这时候要我们估计真实值的话，我们会把真实值估计在区间[30,32]之间，而且由于第一个测量标准差较小，所以真实值会更靠近于30。<br />
如果要我们在数学上得到一个比较准确的值，就可以用上面递归的方法，写出如下等式： <span class="math display">\[
\hat{z}=z_1+K(z_2-z_1)
\]</span> 其中 <span class="math inline">\(K\in [0,1]\)</span>，当<span class="math inline">\(K=0\)</span>时，<span class="math inline">\(\hat{z}=z_1\)</span>；当<span class="math inline">\(K=1\)</span>时，<span class="math inline">\(\hat{z}=z_2\)</span>。<br />
现在我们要求<span class="math inline">\(K\)</span>，使得 <span class="math inline">\(\sigma_{\hat{z}}\)</span> 最小，即是要 <span class="math inline">\(Var(\hat{z})\)</span> 最小。<br />
而这个方差我们可以写成： <span class="math display">\[
\sigma_{\hat{z}}^2=Var(z_1+K(z_2-z_1))=Var((1-K)z_1+Kz_2)
\]</span> 由于括号里的两项 <span class="math inline">\((1-K)z_1\)</span> 和 <span class="math inline">\(Kz_2\)</span> 相互独立(毕竟是两个不同的称，称量结果肯定不会相互影响)，根据方差的性质，上面式子可以继续往下写： <span class="math display">\[
Var((1-K)z_1+Kz_2)=Var((1-K)z_1)+Var(Kz_2)=(1-K)^2Var(z_1)+K^2Var(z_2)
\]</span> 将上面两个方差代入得： <span class="math display">\[
(1-K)^2Var(z_1)+K^2Var(z_2)=(1-K)^2\sigma_1^2+K^2\sigma_2^2
\]</span> 也就是说： <span class="math display">\[
\sigma_{\hat{z}}^2=(1-K)^2\sigma_1^2+K^2\sigma_2^2
\]</span> 现在要求最小值，所以就是对<span class="math inline">\(K\)</span>进行求导： <span class="math display">\[
\frac{d\sigma_{\hat{z}}^2}{dK}=0
\]</span> (中间过程自行计算)<br />
最后求出<span class="math inline">\(K\)</span>的值为： <span class="math display">\[
K=\frac{\sigma_1^2}{\sigma_1^2+\sigma_2^2}=0.2
\]</span> 把<span class="math inline">\(K\)</span>代入上面式子，其最佳估计值为： <span class="math display">\[
\hat{z}=z_1+K(z_2-z_1)=30+0.2(32-30)=30.4
\]</span> 其方差为： <span class="math display">\[
\sigma_{\hat{z}}^2=(1-0.2)^22^2+0.2^24^2=3.2
\]</span> 标准差为：<span class="math inline">\(\sqrt{3.2}=1.79\)</span><br />
这个计算过程就是数据融合。<br />
</p>
<h2 id="协方差矩阵">协方差矩阵</h2>
<p>协方差矩阵是将矩阵，协方差在一个矩阵中表示出来，表明变量空间的联动关系。比如说下面这个表格：</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">名字</th>
<th style="text-align: center;">身高<span class="math inline">\((x)\)</span></th>
<th style="text-align: center;">体重<span class="math inline">\((y)\)</span></th>
<th style="text-align: center;">年龄<span class="math inline">\((z)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">A</td>
<td style="text-align: center;">179</td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">33</td>
</tr>
<tr class="even">
<td style="text-align: center;">B</td>
<td style="text-align: center;">187</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">31</td>
</tr>
<tr class="odd">
<td style="text-align: center;">C</td>
<td style="text-align: center;">175</td>
<td style="text-align: center;">71</td>
<td style="text-align: center;">28</td>
</tr>
<tr class="even">
<td style="text-align: center;">均值</td>
<td style="text-align: center;">180.3</td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">30.7</td>
</tr>
</tbody>
</table>
<p>计算出变量<span class="math inline">\(x,y,z\)</span>的方差为： <span class="math display">\[
\sigma_x^2=24.89,\quad \sigma_y^2=14,\quad\sigma_z^2=4.22
\]</span> 计算协方差为： <span class="math display">\[
\begin{aligned}
\sigma_x\sigma_y &amp;= \frac{1}{3}((179-180.3)(74-75)+(187-180.3)(80-75)+(175-180.3)(71-75))\\
&amp;= 18.7=\sigma_y\sigma_x
\end{aligned}
\]</span> 同理： <span class="math display">\[
\sigma_x\sigma_z=4.4=\sigma_z\sigma_x, \quad \sigma_y\sigma_z=3.3=\sigma_z\sigma_y
\]</span> (协方差矩阵是对称矩阵。)<br />
协方差矩阵形式如下： <span class="math display">\[
 \left[
 \begin{matrix}
   \sigma_x^2 &amp; \sigma_{x,y} &amp; \sigma_{x,z} \\
   \sigma_{y,x} &amp; \sigma_y^2 &amp; \sigma_{y,z} \\
   \sigma_{z,x} &amp; \sigma_{z,y} &amp; \sigma_z^2
  \end{matrix}
  \right]
\]</span> 代入数据： <span class="math display">\[
 \left[
 \begin{matrix}
   24.89 &amp; 18.7 &amp; 4.4 \\
   18.7 &amp; 14 &amp; 3.3 \\
   4.4 &amp; 3.3 &amp; 4.22
  \end{matrix}
  \right]
\]</span> <strong>矩阵计算的方法</strong> 在写代码的时候，用矩阵计算方法计算协方差明显更容易书写，其形式如下：<br />
过渡矩阵： <span class="math display">\[
a=
 \left[
 \begin{matrix}
   x_1 &amp; y_1 &amp; z_1 \\
   x_2 &amp; y_2 &amp; z_2 \\
   x_3 &amp; y_3 &amp; z_3
 \end{matrix}
 \right]
-
\frac{1}{3}
 \left[
 \begin{matrix}
   1 &amp; 1 &amp; 1 \\
   1 &amp; 1 &amp; 1 \\
   1 &amp; 1 &amp; 1
  \end{matrix}
  \right]
   \left[
 \begin{matrix}
   x_1 &amp; y_1 &amp; z_1 \\
   x_2 &amp; y_2 &amp; z_2 \\
   x_3 &amp; y_3 &amp; z_3
  \end{matrix}
  \right]
\]</span> 协方差矩阵： <span class="math display">\[
P=\frac{1}{3}a^Ta
\]</span> 现在我们观察上面得出的那个协方差矩阵，先看对角线，前两个数字都很大（其实第三个数字不应该这么小的，但毕竟只取了三个数据，偶然性太大），表明数据波动是比较大的，说明身高体重在这个群体里影响不大；接着看其它数据，比如这个18.7，说明身高跟体重是成正比的，且相关性较大，也符合常理；然后再看看4.4，说明身高跟年龄关系不大，也对，到一定年龄身高跟年龄关系就不大了……<br />
</p>
<h2 id="状态空间方程">状态空间方程</h2>
<p>比如说对于一个弹簧阻尼系统：<img src="/images/271.png" /> 我们可以很容易写出如下微分方程(<span class="math inline">\(x\)</span>为位移)： <span class="math display">\[
m\ddot{x}+c\dot{x}+kx=F
\]</span> 状态量为： <span class="math display">\[
x_1=x;\quad x_2=\dot{x}
\]</span> 观测量为： <span class="math display">\[
z_1=x=x_1;位置\quad z_2=\dot{x}=x_2速度
\]</span> 设<span class="math inline">\(F\)</span>为输入值，等于<span class="math inline">\(u\)</span>。<br />
写成一阶微分矩阵形式如下： <span class="math display">\[
   \left[
\begin{matrix}
   \dot{x_1}  \\
   \dot{x_2} 
  \end{matrix}
  \right]
=
   \left[
\begin{matrix}
   0 &amp; 1  \\
   -\frac{k}{m} &amp; -\frac{c}{m} 
  \end{matrix}
  \right]
     \left[
\begin{matrix}
   x_1  \\
   x_2
  \end{matrix}
  \right]
+
   \left[
\begin{matrix}
   0  \\
   \frac{1}{m} 
  \end{matrix}
  \right]
u
\]</span> 观测矩阵为： <span class="math display">\[
\left[
\begin{matrix}
   z_1  \\
   z_2 
\end{matrix}
\right]
=
   \left[
\begin{matrix}
   1 &amp; 0  \\
   0 &amp; 1 
  \end{matrix}
  \right]
      \left[
\begin{matrix}
   x_1  \\
   x_2
  \end{matrix}
  \right]
\]</span> 以上两个矩阵表达式就是状态空间表达式，归纳一下可以写为： <span class="math display">\[
\dot{X}_t=AX_t+Bu_t
\]</span> <span class="math display">\[
Z_t=HX_t
\]</span> 这是一种连续的表达式，左边<span class="math inline">\(X\)</span>是对时间求导。<br />
写成离散形式为： <span class="math display">\[
X_k=AX_{k-1}+Bu_{k-1}
\]</span> <span class="math display">\[
Z_k=HX_k
\]</span> 其下标<span class="math inline">\(k.k+1,k+2...\)</span>为时间单位，即是采样。<br />
在实际生活中到处充满着不确定性，上面那个离散的状态空间表达式可以写为： <span class="math display">\[
X_k=AX_{k-1}+Bu_{k-1}+w_{k-1}
\]</span> <span class="math display">\[
Z_k=HX_k+v_k
\]</span> 其中<span class="math inline">\(w_{k-1}\)</span>为过程噪音，<span class="math inline">\(v_k\)</span>为测量噪音。<br />
根据这些如何去估计一个<span class="math inline">\(\hat{X}_k\)</span>呢？这就是卡尔曼滤波器要解决的问题。</p>
<h1 id="卡尔曼增益的数学推导">卡尔曼增益的数学推导</h1>
<p>在一开始我们用了卡尔曼增益，但却没有解释为什么这个增益是这种形式，这部分就是对它的推导。<br />
将上面那个离散情况下带噪音的状态空间方程写下来如下： <span class="math display">\[
X_k=AX_{k-1}+Bu_{k-1}+w_{k-1}
\]</span> <span class="math display">\[
Z_k=HX_k+v_k
\]</span> 两个方程的噪音部分是我们不可测的，但在自然界中我们可以把这噪声假设成正态分布(why? 只能说自然界就是这么神奇)。所以我们可以把过程噪音写成： <span class="math display">\[
P(w) \sim (0,Q)
\]</span> 这里0是期望，<span class="math inline">\(Q\)</span>是协方差矩阵。这个<span class="math inline">\(Q\)</span>可以用下面的式子计算： <span class="math display">\[
Q=E[ww^T]
\]</span> 举个例子，假如现在<span class="math inline">\(X_k\)</span>由两部分组成，即是： <span class="math display">\[
X_k=
\left[
  \begin{matrix}
   x_1  \\
   x_2
  \end{matrix}
\right]
\]</span> 则对应的过程噪音就是： <span class="math display">\[
\left[
\begin{matrix}
   w_1  \\
   w_2
  \end{matrix}
  \right]
\]</span> 这个噪音的协方差矩阵为： <span class="math display">\[
E[
\left[
\begin{matrix}
   w_1  \\
   w_2
  \end{matrix}
  \right]
\left[
\begin{matrix}
   w_1 &amp; w_2
  \end{matrix}
  \right]
]
=
\left[
\begin{matrix}
   E[w_1^2] &amp; E[w_1w_2]  \\
   E[w_2w_1] &amp; E[w_2^2]
  \end{matrix}
  \right]
\]</span> 另外，对于方差我们有： <span class="math display">\[
Var(x)=E(x^2)-E^2(x)
\]</span> 在这里噪音的期望为0，所以：<span class="math inline">\(Var(x)=E(x^2)\)</span>，上面的矩阵可以写为：<br />
<span class="math display">\[
E[
\left[
\begin{matrix}
   w_1  \\
   w_2
  \end{matrix}
  \right]
\left[
\begin{matrix}
   w_1 &amp; w_2
  \end{matrix}
  \right]
]
=
\left[
\begin{matrix}
   \sigma_{w_1}^2 &amp; \sigma_{w_1,w_2}  \\
   \sigma_{w_2,w_1} &amp; \sigma_{w_2}^2
  \end{matrix}
  \right]
\]</span> 噪音的协方差矩阵就出来了，所以这个式子： <span class="math display">\[
Q=E[ww^T]
\]</span> 是正确的。 同理，对于测量噪音<span class="math inline">\(v_k\)</span>，也是一样的： <span class="math display">\[
p(v)\sim (0,R)
\]</span> <span class="math inline">\(R\)</span>是协方差矩阵，同样用上面的方法也可以计算。<br />
在实际建模中，这两个噪音我们是无法建模出来的，我们能计算的只是下面这个东西： <span class="math display">\[
X_k^-=AX_{k-1}+Bu_{k-1} \tag{1}
\]</span> 少了噪音那项，像这种我们直接计算出来的我们称为先验估计值，表示为估计值上面带个减号。<br />
对于测量部分，我们有： <span class="math display">\[
\hat{X}_{k,mea}=H^{-1}Z_{k} \tag{2}
\]</span> 这是测量估计值。<br />
现在我们有两个结果，分别是先验估计值和测量估计值，但这两个都不具备噪声的影响，都是不准确的，然后卡尔曼滤波器的作用就出来了：通过两个不太准确的结果得出一个比较准确的结果。<br />
根据上面【数据融合】那部分的知识，后验估计值(就是我们想要的)可以写成： <span class="math display">\[
\hat{X}_k=\hat{X}_{k}^-+G(H^{-1}Z_k-\hat{X}_{k}^-)
\]</span> 这里<span class="math inline">\(G\in [0,1]\)</span>，当<span class="math inline">\(G=0\)</span>时，<span class="math inline">\(\hat{X}_k=\hat{X}_{k}^-\)</span>，表明更相信计算的结果，也就是先验估计值；如果<span class="math inline">\(G=1\)</span>，<span class="math inline">\(\hat{X}_k=H^{-1}Z_{k}\)</span>，表明更相信测量的结果。<br />
在其它地方上面的式子会写成这样： <span class="math display">\[
\hat{X}_k=\hat{X}_k^-+K_k(Z_k-H\hat{X}_{k}^-)
\]</span> 这里的<span class="math inline">\(G=K_kH\)</span>，其实一个一声，但要注意的是，这里的<span class="math inline">\(K\in[0,H^{-1}]\)</span>。<br />
现在我们要做的就是寻找这个<span class="math inline">\(K_k\)</span>，使得<span class="math inline">\(\hat{X}_k\)</span>趋近于<span class="math inline">\(X_{k}\)</span>。这里我们定义一个误差： <span class="math display">\[
e_k=X_k-\hat{X}_k
\]</span> 这里的误差<span class="math inline">\(e_k\)</span>也是一个符合正态分布的矩阵：<span class="math inline">\(P(e_{k})\sim (0,P)\)</span>(这个应该很容易理解)，依旧按照上面的方法求这个协方差矩阵： <span class="math display">\[
P=E[ee^T]=
\left[
\begin{matrix}
   \sigma_{e_1}^2 &amp; \sigma_{e_1,e_2}  \\
   \sigma_{e_2,e_1} &amp; \sigma_{e_2}^2
  \end{matrix}
  \right]
\]</span> 我们这个估计值<span class="math inline">\(\hat{X}_k\)</span>越接近于实际值，就说明这个误差的方差越小，也就说明越接近于0。现在问题就变成了我们需要选取适当的<span class="math inline">\(K_k\)</span>，使协方差矩阵的迹最小，即<span class="math inline">\(tr(P)\)</span>最小。(元素分开考虑，所以是对角线)。 <span class="math display">\[
tr(P)=\sigma_{e_1}^2+\sigma_{e_2}^2
\]</span> 现在我们首先把协方差矩阵求出来，对于误差有： <span class="math display">\[
e_k=X_k-\hat{X}_k
\]</span> 将其代入协方差矩阵的式子得： <span class="math display">\[
\begin{aligned}
P  &amp;= E[ee^T]\\
    &amp;=E[(X_k-\hat{X}_k)(X_k-\hat{X}_{k})^T]
\end{aligned}
\]</span> 将 <span class="math display">\[
\hat{X}_k=\hat{X}_k^-+K_k(Z_k-H\hat{X}_{k}^-)
\]</span> <span class="math display">\[
Z_k=HX_k+v_k
\]</span> 代入得： <span class="math display">\[
\begin{aligned}
X_k-\hat{X}_k &amp;= (X_k-\hat{X}_{k}^-)-K_kH(X_k-\hat{X}_{k}^-)-K_kv_k \\
&amp;=(I-K_kH)(X_k-\hat{X}_{k}^-)-K_kv_k
\end{aligned}
\]</span> 定义先验误差 <span class="math inline">\(e_k^-=(X_k-\hat{X}_{k}^-)\)</span>。<br />
所以上面的协方差矩阵可以写为： <span class="math display">\[
\begin{aligned}
P_k  &amp;= E[ee^T]\\
    &amp;= E[(X_k-\hat{X}_k)(X_k-\hat{X}_{k})^T]\\
    &amp;= E[[(I-K_kH)e_k^--K_kv_k][(I-K_kH)e_k^--K_kv_k]^T]\\
    &amp;= E[(I-K_kH)e_k^-{e_k^-}^T(I-K_kH)^T]-E[(I-K_kH)e_k^-v_k^TK_k^T]
        -E[K_kv_k{e_k^-}^T(I-K_kH)^T]+E[K_kv_kv_k^TK_k^T]
\end{aligned}
\]</span> 由于<span class="math inline">\((I-K_kH)\)</span>是常数，且对于独立的<span class="math inline">\(A,B\)</span>，有<span class="math inline">\(E(AB)=E(A)E(B)\)</span>，<span class="math inline">\(E(e_K^-)=E(v_k^T)=0\)</span>，所以上式的中间两项为0。上式可以继续化简： <span class="math display">\[
\begin{aligned}
P_k  &amp;= E[ee^T]\\
    &amp;= E[(X_k-\hat{X}_k)(X_k-\hat{X}_{k})^T]\\
    &amp;= E[[(I-K_kH)e_k^--K_kv_k][(I-K_kH)e_k^--K_kv_k]^T]\\
    &amp;= E[(I-K_kH)e_k^-{e_k^-}^T(I-K_kH)^T]-E[(I-K_kH)e_k^-v_k^TK_k^T]
        -E[K_kv_k{e_k^-}^T(I-K_kH)^T]+E[K_kv_kv_k^TK_k^T]\\
    &amp;= (I-K_kH)E[(e_k^-{e_k^-}^T)](I-K_kH)^T+K_kE[v_kv_k^T]K_k^T
\end{aligned}
\]</span> 由于<span class="math inline">\(E[(e_k^-{e_k^-}^T)]\)</span>为先验误差的协方差矩阵<span class="math inline">\(P_k^-\)</span>，<span class="math inline">\(R=E[v_kv_k^T]\)</span>上面等式就可以继续化简： <span class="math display">\[
\begin{aligned}
P_k  &amp;= E[ee^T]\\
    &amp;= E[(X_k-\hat{X}_k)(X_k-\hat{X}_{k})^T]\\
    &amp;= E[[(I-K_kH)e_k^--K_kv_k][(I-K_kH)e_k^--K_kv_k]^T]\\
    &amp;= E[(I-K_kH)e_k^-{e_k^-}^T(I-K_kH)^T]-E[(I-K_kH)e_k^-v_k^TK_k^T]
        -E[K_kv_k{e_k^-}^T(I-K_kH)^T]+E[K_kv_kv_k^TK_k^T]\\
    &amp;= (I-K_kH)E[(e_k^-{e_k^-}^T)](I-K_kH)^T+K_kE[v_kv_k^T]K_k^T\\
    &amp;=(P_k^--K_kHP_k^-)(I-H^TK_k^T)+K_kRK_k^T\\
    &amp;=P_k^--K_kHP_k^--P_k^-H^TK_k^T+K_kHP_k^-H^TK_k^T+K_kRK_k^T
\end{aligned}
\]</span> 现在就求出这个协方差矩阵了，接下来是求迹。我们看右边第三项，对其转置： <span class="math display">\[
((P_k^-H^T)K_k^T)^T=K_k(P_k^-H^T)^T=K_kH{P_k^-}^T
\]</span> 也就是说，第三项的转置等于第二项，所以第三项的迹与第二项相同。所以协方差矩阵的迹为： <span class="math display">\[
tr(P_k)=tr(P_k^-)-2tr(K_kHP_k^-)+tr(K_kHP_k^-H^TK_k^T)+tr(K_kRK_k^T)
\]</span> 现在要求迹最小时对应的<span class="math inline">\(K_k\)</span>，即是求导等于0。<br />
这里先说个东西： <span class="math display">\[
\frac{dtr(AB)}{dA}=B^T
\]</span> <span class="math display">\[
\frac{d(ABA^T)}{dA}=2AB
\]</span> (这个自己验证)<br />
所以： <span class="math display">\[
\frac{dtr(P_k)}{dK_k}=0-2(HP_k^-)^T+2K_kHP_k^-H^T+2K_kR=0
\]</span> 即： <span class="math display">\[
-P_k^-H^T+K_k(HP_k^-H^T+R)=0
\]</span> <span class="math display">\[
K_k=\frac{P_k^-H^T}{HP_k^-H^T+R}
\]</span> 这就是卡尔曼滤波器最核心的公式，分析一下，当<span class="math inline">\(R\)</span>特别大时，即测量噪声特别大时，<span class="math inline">\(K_k\)</span>就趋近于0，此时估计值<span class="math inline">\(\hat{X}_k\)</span>就等于先验估计<span class="math inline">\(\hat{X}_{k}^-\)</span>，我们就更愿意相信计算的先验估计；当<span class="math inline">\(R\)</span>很小时，<span class="math inline">\(K_k\)</span>就趋近于<span class="math inline">\(H^{-1}\)</span>，估计值就等于测量估计。</p>
<h1 id="误差协方差矩阵">误差协方差矩阵</h1>
<p>上面我们得出了卡尔曼增益的表达式： <span class="math display">\[
K_k=\frac{P_k^-H^T}{HP_k^-H^T+R}
\]</span> 在这个表达式中<span class="math inline">\(P_k^-\)</span>我们还不知道，这部分就是对这个的推导。<br />
我们已知： <span class="math display">\[
P_k^-=E[e_k^--{e_k^-}^T]
\]</span> <span class="math display">\[
\begin{aligned}
e_k^- &amp;=X_k-\hat{X}_k^-\\
    &amp;= AX_{k-1}+Bu_{k-1}+w_{k-1}-A\hat{X}_{k-1}-Bu_{k-1}\\
    &amp;= A(X_{k-1}-\hat{X}_{k-1})+w_{k-1}\\
    &amp;= Ae_{k-1}+w_{k-1}
\end{aligned}
\]</span> 代入上面的协方差矩阵： <span class="math display">\[
\begin{aligned}
P_k^- &amp;=E[e_k^--{e_k^-}^T]\\
    &amp;= E[(Ae_{k-1}+w_{k-1})((Ae_{k-1})^T+(w_{k-1})^T)]\\
    &amp;= E[Ae_{k-1}e_{k-1}^TA^T]+E[Ae_{k-1}w_{k-1}^T]+E[w_{k-1}e_{k-1}^TA^T]+E[w_{k-1}w_{k-1}^T]
\end{aligned}
\]</span> 这里，由于<span class="math inline">\(e_{k-1}\)</span>和<span class="math inline">\(w_{k-1}\)</span>是相互独立的（<span class="math inline">\(e_{k-1}\)</span>与<span class="math inline">\(w_{k-1}\)</span>无关而与<span class="math inline">\(w_{k-2}\)</span>有关），所以相乘的期望等于期望的相乘，都等于0，上面式子中间两项等于0，可以写为： <span class="math display">\[
\begin{aligned}
P_k^- &amp;=E[e_k^--{e_k^-}^T]\\
    &amp;= E[(Ae_{k-1}+w_{k-1})((Ae_{k-1})^T+(w_{k-1})^T)]\\
    &amp;= E[Ae_{k-1}e_{k-1}^TA^T]+E[Ae_{k-1}w_{k-1}^T]+E[w_{k-1}e_{k-1}^TA^T]+E[w_{k-1}w_{k-1}^T]\\
    &amp;= AE[e_{k-1}e_{k-1}^T]+E[w_{k-1}w_{k-1}^T]\\
    &amp;= AP_{k-1}A^T+Q
\end{aligned}
\]</span> 至此，就可以用卡尔曼滤波器来估计状态变量的值了。这个过程分为两步：</p>
<blockquote>
<p><strong>预测：</strong><br />
 计算先验：<span class="math display">\[\hat{X}_k^-=A\hat{X}_{k-1}+Bu_{k-1} \tag{1}\]</span><br />
 先验误差协方差：<span class="math display">\[P_k^-=AP_{k-1}A^T+Q \tag{2}\]</span></p>
</blockquote>
<blockquote>
<p><strong>校正：</strong><br />
 卡尔曼增益： <span class="math display">\[
K_k=\frac{P_k^-H^T}{HP_k^-H^T+R} \tag{3}
\]</span>  后验估计： <span class="math display">\[
\hat{X}_k=\hat{X}_{k}^-+K_k(Z_k-H\hat{X}_k^-) \tag{4}
\]</span></p>
</blockquote>
<p>由于在预测中用了上一次的误差协方差，所以要更新这个参数，在上一部分我们得出了： <span class="math display">\[
P_k=P_k^--K_kHP_k^--P_k^-H^TK_k^T+K_kHP_k^-H^TK_k^T+K_kRK_k^T
\]</span> 将算出的<span class="math inline">\(K_k\)</span>代入得： <span class="math display">\[
P_k=P_k^--K_kHP_k^-=(I-K_kH)P_k^- \tag{5}
\]</span> 上面公式(1) -- (5)就是卡尔曼滤波器最重要的5个公式了，在最开始的时候我们要初始化<span class="math inline">\(\hat{X}_0\)</span> 和 <span class="math inline">\(P_{0}\)</span>。</p>
]]></content>
      <categories>
        <category>学习总结</category>
      </categories>
  </entry>
  <entry>
    <title>【ML-2020/21】Generative Adversarial Network (GAN) --- Part-2 Conditional GAN</title>
    <url>/posts/19.html</url>
    <content><![CDATA[<h1 id="写在前面的说明">写在前面的说明</h1>
<p>这个系列【ML-2020/21】大部分是课上内容的简单复述，之前上过但因为笔记写得很乱就忘了很多，所以重来一遍。与其看我这篇，不如直接去看视频，讲得还更生动。视频系列链接<span class="math inline">\(\rightarrow\)</span><a href="https://www.youtube.com/watch?v=OP5HcXJg2Aw">这里</a>。 <span id="more"></span></p>
<h1 id="引入">引入</h1>
<p>根据上一篇【基本介绍】的内容可知，给GAN输入是一个向量，但这在实际中很不实用，我们更习惯用的是输入常见的信息比如语言文字，这就是Conditional GAN 所研究的东西。 举个例子，比如输入‘The Dog is running’，我们就希望生成如下图片：<img src="/images/243.jpg" /> 这就是这篇要讲的东西。</p>
<h1 id="text-to-image">Text-to-Image</h1>
<h2 id="traditional-supervised-approach">Traditional Supervised Approach</h2>
<p>首先，这可以看作是一个传统的监督学习，我们要的首先是一大堆数据集（图片），每张图片都需要一个对应的文字进行描述。简单来说流程如下：<img src="/images/244.png" /> 但用这种方法会产生一个问题，比如说我们现在有一个文字描述为“car”，可能同时对应到以下两张图片，它们都是汽车只是方向不同：<img src="/images/245.png" /> 我们用上面的方法训练出来的模型输出既要像左边也要像右边，最后就会产生这种现象：输入“car”，输出的图片是上面两种图片的平均，这可能会是一张没有意义的图。 所以这种传统的方法不行。</p>
<h2 id="conditional-gan">Conditional GAN</h2>
<p>然后现在就要引进今天的主角了。 回顾一下，如果按照之前介绍的最原始的GAN，这个训练流程大致是这样的：</p>
<blockquote>
<ol type="1">
<li>首先输入一组随机向量给Generation，产生数据（图片）；<br />
</li>
<li>将产生的图片输入Discriminator，借助真实图片，利用定义的function更新Discriminator的参数;<br />
</li>
<li>随机再取一组噪音数据，利用上面更新后的Discriminator更新Generation的参数；<br />
</li>
<li>重复上面两个步骤。</li>
</ol>
</blockquote>
<p>然而在Conditional-GAN中，我们输入给Generation的不再是单一的向量，同时还有（Condition）文字信息比如所“car”，借助这两个信息产生新的数据（图片）<span class="math inline">\(x\)</span>；<br />
对于Discriminator，其输入除了之前产生的图片<span class="math inline">\(x\)</span>，还有(Condition)文字信息<span class="math inline">\(c\)</span>，输出是一个标量，这个标量描述以下两个方面：</p>
<blockquote>
<ol type="1">
<li><span class="math inline">\(x\)</span>是否是一张真实的图片（这点跟最原始的GAN一样）；<br />
</li>
<li><span class="math inline">\(c\)</span>和<span class="math inline">\(x\)</span>是否符合。</li>
</ol>
</blockquote>
<p>这个标量的评分标准如下：</p>
<blockquote>
<ol type="1">
<li>高分（1分）：<br />
  a). 正确的文字与真实的图片。<br />
</li>
<li>低分（0分）：<br />
  a). 正确的文字与生成的图片；<br />
  b). 错误的文字与真实的图片。</li>
</ol>
</blockquote>
<p>基本的训练流程如下：</p>
<blockquote>
<p><strong>D-Learning:</strong><br />
  1. 从database中取<span class="math inline">\(m\)</span>个样本：<span class="math inline">\(\left\{ (c^1,x^1),(c^2,x^2),...,(c^m,x^m)\right\}\)</span> (这个对于Discriminator来说是要给高分的)；<br />
  2. 随机取样<span class="math inline">\(m\)</span>个噪音样本：{<span class="math inline">\(z^1,...,z^m\)</span>}，这<span class="math inline">\(m\)</span>个样本加上上面的文字信息<span class="math inline">\(c\)</span>组成新的数据<span class="math inline">\((c^i,z^i)\)</span>；<br />
  3. 通过Generation获取生成数据：{<span class="math inline">\(\widetilde{x}^1,...,\widetilde{x}^m\)</span>}，<span class="math inline">\(\widetilde{x}^i=G(c^i,z^i)\)</span><br />
  4. 从database中取样新的<span class="math inline">\(m\)</span>个数据{<span class="math inline">\(\hat{x}^1,...,\hat{x}^m\)</span>}<br />
  5. 更新Discriminator的参数<span class="math inline">\(\theta_d\)</span>，以最大化下面这个方程：<span class="math display">\[\widetilde{V}=\frac{1}{m}\sum_{i=1}^m\log D(c^i,x^i)+\frac{1}{m}\sum_{i=1}^m\log(1 - D(c^i,\widetilde{x}^i))-\frac{1}{m}\sum_{i=1}^m\log(1 - D(c^i,\hat{x}^i))\]</span> <span class="math display">\[\theta_d \leftarrow \theta_d +\eta \nabla \widetilde{V}(\theta_d)\]</span></p>
</blockquote>
<blockquote>
<p><strong>G-Leaarning:</strong><br />
  1. 取样<span class="math inline">\(m\)</span>个噪声样本：{<span class="math inline">\(z^1,...,z^m\)</span>}；<br />
  2. 从database中取<span class="math inline">\(m\)</span>个condition：{<span class="math inline">\(c^1,...,c^m\)</span>}；<br />
  3. 更新Generator的参数<span class="math inline">\(\theta_g\)</span>，以最大化下面这个方程：<span class="math display">\[\widetilde{V}=\frac{1}{m}\sum_{i=1}^m\log(D(G()c^i,z^i))\]</span> <span class="math display">\[\theta_g \leftarrow \theta_g +\eta \nabla \widetilde{V}(\theta_g)\]</span></p>
</blockquote>
<h2 id="discriminator-架构">Discriminator 架构</h2>
<p>常见的Discriminator架构如下： <img src="/images/246.png" /> 同时，又有人提出另一种架构： <img src="/images/247.png" /></p>
<h2 id="更进一步-stack-gan">更进一步 —— Stack GAN</h2>
<p>Stack-GAN技术能让模型的性能更加优秀，其基本的想法是：先产生小张的图，再根据小张的图产生大张的图。<br />
比如说，我们要产生<span class="math inline">\(256\times 256\)</span>的图，但如果直接生成这么大的图，图很可能会坏，所以Stack-GAN在train的时候把整个训练过程拆分为两部分，大概流程是：</p>
<blockquote>
<ol type="1">
<li>输入一个文字以及一段噪声，拼接，通过第一个Generator产生一个<span class="math inline">\(64\times 64\)</span>的图；<br />
</li>
<li>这个图片进入第一个Discriminator，判断其和文字是否match；<br />
</li>
<li>然后第二部分，输入一个文字信息以及<span class="math inline">\(64\times 64\)</span>的图片，产生一张<span class="math inline">\(256\times 256\)</span>的图片；<br />
</li>
<li>最后第二个Discriminator判断这个<span class="math inline">\(256\times 256\)</span>的图片是不是足够真实。</li>
</ol>
</blockquote>
<p><em>具体可参看paper：StackGAN:Text to Photo-realistic Image Synthesis with Stack Generative Adversarial Network. 【ICCV, 2017】</em></p>
<h1 id="image-to-image">Image-to-Image</h1>
<p>这种即是：输入图片，输出也是图片。比如说黑白图片变彩色，白天变黑夜。</p>
<h2 id="traditional-supervised-approach-1">Traditional Supervised Approach</h2>
<p>理论上也是可以用传统的监督学习实现，但也会遇到跟上面类似的问题，导致最终产生出来的图片特别模糊。</p>
<h2 id="conditional-gan-1">Conditional GAN</h2>
<p>用GAN的话，以下图为例<img src="/images/249.png" /> 也就是输入左边简单的图片，希望产生右边真实复杂的图。<br />
其基本流程就是：将简单的图片与噪音一起输入Generator，产生一张图片；接着将这张图片与原来简单的图片一起输入Discriminator，产生一个标量。这个标量可参看上面的解释。<br />
单单这么做的话，产生的图像虽然清晰，正确率也不错，但图片里可能会产生一些奇奇怪怪的部分。为了消除这种现象，我们可以加额外的约束，比如：在Generator的输出中，我们将产生的图片与真实图片作对比，也就是说，我们希望Generator产生的图片，既能骗过Discriminator，又跟目标图片相差不要太大，如此以来，模型性能会好很多。<br />
</p>
<h2 id="patch-gan">Patch GAN</h2>
<p>如果图片很大张，那么Discriminator就不能直接接受整张Image，因为参数会很多，容易过拟合。所以措施是分区域输入，每次检查一小块，看这一小块是好的还是坏的，至于区域设置多大，这个就自己调了。</p>
]]></content>
      <categories>
        <category>ML-2020/21</category>
      </categories>
  </entry>
  <entry>
    <title>【ML-2020/21】Generative Adversarial Network (GAN) --- Part-1 基本介绍</title>
    <url>/posts/18.html</url>
    <content><![CDATA[<h1 id="写在前面的说明">写在前面的说明</h1>
<p>这个系列【ML-2020/21】大部分是课上内容的简单复述，之前上过但因为笔记写得很乱就忘了很多，所以重来一遍。与其看我这篇，不如直接去看视频，讲得还更生动。视频系列链接<span class="math inline">\(\rightarrow\)</span><a href="https://www.youtube.com/watch?v=OP5HcXJg2Aw">这里</a>。 <span id="more"></span> # 基本概念 Generative Adversarial Network (GAN) 中文名是【生成式对抗网络】，用这个网络的目的是为了让机器生成东西（比如图片，文章等），其基本网络结构包括两大部分：</p>
<blockquote>
<ol type="1">
<li>Generation：生成器<br />
</li>
<li>Discriminator：鉴别器</li>
</ol>
</blockquote>
<h2 id="generation">Generation</h2>
<p>在generation过程中我们需要做的就是训练一个生成器，比如在影像生成中，我们希望得到一个生成器，随便输入一个向量，通过这个生成器能得到一张对应图片，不同的向量对应不同的图片；在文字生成中也是一样，输入一个向量能得到对应的语句。 基本形式就像下图一样： <img src="https://img-blog.csdnimg.cn/d57961ffdf8e42ea8734ca364681a041.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 一个Generation就是一个神经网络，更简单来说，就是一个方程，输入一个向量，就能得到一个更高维度的向量，将其进行通道，图片尺寸进行重组后就是一张新的图片。 以图像生成为例，通常输入向量的每一个dimension会对应到图像的某一个特征，比如头发长度，眼镜大小，颜色等。</p>
<h2 id="discriminator">Discriminator</h2>
<p>Discriminator也是一个神经网络，或者说方程，输入一张图片（或者一个句子），输出一个数值，这个数值越大，则说明这个输入的真实性越高。比如说下面这张图片所示： <img src="https://img-blog.csdnimg.cn/1680feeda4444fc1b29eb7b98e4c9e1f.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /></p>
<h2 id="generation-vs.-discriminator">Generation vs. Discriminator</h2>
<p>在GAN中这两个神经网络可以看作是一种对抗的关系，比如说在第一个Generation中生成了一组图像，然后Discriminator根据某一个判断标准比如说图像是不是彩色的来判断这组图像是否足够真实；然后第二个Generation根据第一个Discriminator的结果产生了一组新的彩色图片，然而第二个Discriminator又根据另一个判断标准比如是否有眼睛来判断这组图片是否足够真实；再接着第三个Generation又根据第二个Discriminator的结果……这样一直“对抗”下去，直到生成的图片可以骗过Discriminator的时候，就完成了。</p>
<h2 id="algorithm">Algorithm</h2>
<p>根据上面它们两者的关系就可以大致写出这个算法的流程了：</p>
<blockquote>
<ol type="1">
<li>初始化generator和discriminator</li>
<li>进入循环，每个循环进行如下步骤：   <strong>step-1</strong>：固定住generator G，更新discriminator D（再说具体一点的话就是，用当前的G产生一组图片，在D中与真实的图片（已有的database）作对比，训练D，使真实图片高分，生成图低分。）   <strong>step-2</strong>：固定住D，更新G（具体就是，把一个向量输入进G，产生一张图片，根据当前的D进行评分，训练G，使得到比较高评分） （一般来说是把G和D一起当作是一个巨大的神经网络，即输入向量得到一个数值）</li>
</ol>
</blockquote>
<p>说得再具体一点就是：</p>
<blockquote>
<ol type="1">
<li><p>初始化<span class="math inline">\(D\)</span>和<span class="math inline">\(G\)</span>的参数分别为<span class="math inline">\(\theta_d\)</span>和<span class="math inline">\(\theta_g\)</span>；<br />
</p></li>
<li><p><span class="math inline">\(D\)</span>的学习过程：<br />
 ( a ). 从数据集database中选取<span class="math inline">\(m\)</span>个样本 {<span class="math inline">\(x^1,x^2,...,x^m\)</span>}；<br />
 ( b ). 从一个分布（高斯或均匀或其它）中采取<span class="math inline">\(m\)</span>个噪音样本 {<span class="math inline">\(z^1,z^2,...,z^m\)</span>}，其中噪音样本维度自己定，属于超参数；<br />
 ( c ). 利用<span class="math inline">\(G\)</span>获取生成数据 {<span class="math inline">\(\widetilde{x}^1,\widetilde{x}^2,...,\widetilde{x}^m\)</span>}，<span class="math inline">\(\widetilde{x}^i = G(z^i)\)</span>；<br />
 ( d ). 更新<span class="math inline">\(D\)</span>的参数<span class="math inline">\(\theta_d\)</span>，以便最大化<span class="math inline">\(\widetilde{V}\)</span>，其中：<span class="math display">\[\widetilde{V}=\frac{1}{m}\sum_{i=1}^m\log D(x^i)+\frac{1}{m}\sum_{i=1}^m\log(1-D(\widetilde{x}^i))\]</span> (简单说一下这个<span class="math inline">\(\widetilde{V}\)</span>，第一项就是让真实图片的得分越大越好，第二项就是让生成的图片得分越小越好，其中<span class="math inline">\(D(.)\)</span>一般是经过sigmoid的介于0~1的数)<span class="math display">\[\theta_d \leftarrow \theta_d + \eta \nabla \widetilde{V}(\theta_d)\]</span>(这里在说一句，一般这里更新不会是只update一次的，至于更新几次也属于超参数，要自己调的)<br />
</p></li>
<li><p><span class="math inline">\(G\)</span>的学习过程：<br />
 ( a ). 另外从一个分布（高斯或均匀或其它）中采取<span class="math inline">\(m\)</span>个噪音样本 {<span class="math inline">\(z^1,z^2,...,z^m\)</span>}；<br />
 ( b ). 更新参数<span class="math inline">\(\theta_g\)</span>以最大化<span class="math inline">\(\widetilde{V}\)</span>，其中<span class="math display">\[\widetilde{V}=\frac{1}{m}\sum_{i=1}^m\log(D(G(z^i)))\]</span><span class="math display">\[\theta_g\leftarrow\theta_g+\eta\nabla\widetilde{V}(\theta_g)\]</span></p></li>
<li><p>上面2，3步骤反复执行。</p></li>
</ol>
</blockquote>
<h2 id="补充">补充</h2>
<p>GAN可以看作是一个Structured Learning的方法，基本可以看作下面两个部分组成： <img src="https://img-blog.csdnimg.cn/238b184dc150483fbd987a3a3d4dad15.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> Bottom Up 是指一个一个部件生成的办法，这种方法会失去全局观，也就是说不知道各个部件之间的联系；下面的Top Down 是直接观察整体，然后进行判断，generator没法用这种方法。</p>
<blockquote>
<p>问题1：能不能不用Discriminator？</p>
</blockquote>
<p>严格说来是可以的，用Auto-encoder的技术，但由于同个layer神经元之间没有联系，为了让它们产生联系，就必须多加layer，这样会把神经网络弄得很复杂，所以在同种效果下，GAN的技术会更实用。</p>
<blockquote>
<p>问题2：能不能不用Generation, 就只用Discriminator来做生成？</p>
</blockquote>
<p>也是可以的。但也很麻烦，要穷举所有可能的<span class="math inline">\(x\)</span>，使<span class="math display">\[\widetilde{x}=arg\max D(x)\]</span>然后还要让Discriminator学会如何分别好的和坏的图片，但实际上我们的database里只有好的，不过这个问题有办法解决，如下：</p>
<blockquote>
<ol type="1">
<li>随机产生一些坏的图片，比如随机噪音；<br />
</li>
<li>用好的图片和坏的图片去训练<span class="math inline">\(D\)</span>；<br />
</li>
<li>用当前<span class="math inline">\(D\)</span>产生一组新的图片，充当坏的图片继续与好的图片去训练<span class="math inline">\(D\)</span></li>
</ol>
</blockquote>
<p>步骤2-3循环。 效果如下： <img src="https://img-blog.csdnimg.cn/0ac5f9c70c8142c092fa644d0526eb5b.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /></p>
<p>所以实际可以这么理解，generator就是用来解<span class="math inline">\(arg\max\)</span>问题的，也就是用generator产生坏的图片。 <strong>两者比较</strong> Generator：</p>
<blockquote>
<p>优点：<br />
  1. 容易生成东西。</p>
</blockquote>
<blockquote>
<p>缺点：<br />
  1. 不容易考虑部件（component）之间的关系；<br />
  2. 只是模仿表象。</p>
</blockquote>
<p>Discriminator</p>
<blockquote>
<p>优点：<br />
  1. 考虑整张图片。</p>
</blockquote>
<blockquote>
<p>缺点：<br />
  1. 不容易生成东西；<br />
  2. 解那个<span class="math inline">\(arg\max\)</span>问题不容易。</p>
</blockquote>
]]></content>
      <categories>
        <category>ML-2020/21</category>
      </categories>
  </entry>
  <entry>
    <title>深度学习中的正则化</title>
    <url>/posts/17.html</url>
    <content><![CDATA[<h1 id="引入">引入</h1>
<p>与其它机器学习方法类似，DNN在训练过程中也会遇到过拟合的现象，尤其是当参数数量多于输入数据量时。为了防止过拟合现象，除了增加训练样本外，最常见的就是各种正则化方法，比如：数据增强、<span class="math inline">\(L1\)</span> 正则化、<span class="math inline">\(L2\)</span> 正则化、Dropout、DropConnect 和早停法（Early stopping）等。 下面对这些方法进行逐一介绍。 <span id="more"></span> # 正则化方法</p>
<h2 id="数据增强">1. 数据增强</h2>
<p>以图像处理为例，我们可以预先将图像做翻转拉伸，亮度调节，随机平移等操作，从而增大模型的训练数据集，增加模型的泛化能力。一般来说，现在在进行深度学习时数据增强已经成为一个必要的操作了。 但要注意的是，这种方法也不能用得太过分，一般来说用这种方法将数据集拓展到两倍已经很够了，如果再继续拓展，意义不大而且会浪费很多时间。 对于图像的数据增强方法可参见另一篇博文【图像的各种预处理方式】。</p>
<h2 id="l1-l2正则">2. <span class="math inline">\(L1 / L2\)</span>正则</h2>
<p>L1 / L2正则原理相似，是在损失函数后面加个正则化项，以此对损失函数进行约束，感觉就是用拉格朗日解带约束的优化问题。现假设我们的损失函数为<span class="math inline">\(L(y_i,\hat{y}_i)\)</span>。 这里先写三个概念：</p>
<blockquote>
<ol type="1">
<li><span class="math inline">\(L0\)</span>正则：向量中非零元素的个数，记作：<span class="math inline">\(||W||_0\)</span>；<br />
</li>
<li><span class="math inline">\(L1\)</span>正则：向量中元素绝对值之和，记作：<span class="math inline">\(||W||_1\)</span>；<br />
</li>
<li><span class="math inline">\(L2\)</span>正则：也就是模，记作：<span class="math inline">\(||W||_2\)</span></li>
</ol>
</blockquote>
<p>对于单个数据，假设其特征项<span class="math inline">\(X\)</span>为：<span class="math inline">\(x_0,x_1,...,x_N\)</span>，对应的权重项<span class="math inline">\(W\)</span>为：<span class="math inline">\(w_0,w_1,...,w_N\)</span>，为了防止过拟合，即是要减少数据的特征项数<span class="math inline">\(N\)</span>，这里选择通过控制权重项<span class="math inline">\(W\)</span>来控制特征项数<span class="math inline">\(N\)</span>，为什么不通过特征项<span class="math inline">\(X\)</span>呢？因为我们不确定下一个输入数据的特征项有多少，没法控制。也就是说，现在我们要做的就是控制权重项<span class="math inline">\(W\)</span>的项数，使其数目最少。</p>
<h3 id="l1正则">2.1 <span class="math inline">\(L1\)</span>正则</h3>
<p>最开始用的其实是<span class="math inline">\(L0\)</span>正则，也就是说我们要同时让损失函数<span class="math inline">\(L(y_i,\hat{y}_i)\)</span>以及<span class="math inline">\(||W||_0\)</span>正则项最小，即是求它们之和最小。</p>
<p>补充一点点东西</p>
<blockquote>
<ol type="1">
<li>这里我们也就是要实现参数矩阵<span class="math inline">\(W\)</span>稀疏，现在实现稀疏基本都是用<span class="math inline">\(L1\)</span>正则，不用<span class="math inline">\(L0\)</span>正则；<br />
</li>
<li>参数矩阵稀疏通常是为了特征选择和易于解释方面的考虑</li>
</ol>
</blockquote>
<p>但由于<span class="math inline">\(L0\)</span>正则不好计算，所以我们转而求<span class="math inline">\(L1\)</span>正则，通过上面两概念可以知道，这两者在这里的计算意义是类似的，也就是最优凸近似（具体推导看不下去，有兴趣的参看Emmanuel Candes的paper）。 加上<span class="math inline">\(L1\)</span>正则项后新的损失函数如下：<span class="math display">\[L^{new}=L(y_i,\hat{y}_i)+\alpha ||W||_1\]</span>有些地方会写得更具体，如下：<span class="math display">\[J(w,b)=\frac{1}{m}\sum_{i=1}^mL(y_i,\hat{y}_i)+\frac{\lambda}{2m}||w||_1\]</span>反正意思都一样。这里的<span class="math inline">\(\lambda\)</span>属于超参数，也就是我们要自己调的，越大正则化越明显。 由于损失函数变化了，梯度也会跟着变化，也就是会带来梯度更新方向的不同，之后的计算跟以前一样。</p>
<h3 id="l2正则">2.2 <span class="math inline">\(L2\)</span>正则</h3>
<p><span class="math inline">\(L2\)</span>是求模，也就是矩阵各元素求平方和再开方，采用<span class="math inline">\(L2\)</span>正则的目的是让各参数趋近于<span class="math inline">\(0\)</span>，也就是让他们最小化。（<span class="math inline">\(L1\)</span>则是让它们等于<span class="math inline">\(0\)</span>，也就是稀疏） 那为什么让参数最小化可以有效防止过拟合呢？通过<span class="math inline">\(L2\)</span>正则可以构造出一个参数都比较小的模型，一方面，对于那些实在不重要的特征，其权重会非常接近于0，但没有等于，影响已经很小了；另一方面，当参数很小时，既使数据变化比较大，其对结果的影响也不会很大，即模型的抗干扰能力会比较强。 其形式和<span class="math inline">\(L1\)</span>基本一样，只要把后面的项改成<span class="math inline">\(L2\)</span>正则即可。</p>
<h3 id="l1与l2的总结比较">2.3 <span class="math inline">\(L1\)</span>与<span class="math inline">\(L2\)</span>的总结比较</h3>
<p><span class="math inline">\(L1\)</span>,<span class="math inline">\(L2\)</span>在一些地方也写作LASSO和岭回归（在之前的【线性回归】也有提到一些）。 在各种防止过拟合的正则化中，<span class="math inline">\(L2\)</span>防过拟合效果都要优于<span class="math inline">\(L1\)</span>正则，所以与<span class="math inline">\(L1\)</span>相比，正则化一般都是用<span class="math inline">\(L2\)</span>，但当<span class="math inline">\(L1\)</span>正则中的系数<span class="math inline">\(\alpha\)</span>比较大时，也会得到系数极小的最优解，这时的<span class="math inline">\(L1\)</span>也具有防止过拟合的作用。 <span class="math inline">\(L1\)</span>正则由于可以实现稀疏矩阵，所以可以用来当作特征的选择。 有一点或许是需要注意的，在进行<span class="math inline">\(L2\)</span>正则化中，我们把权重变得很小，所以在进入激活函数的时候值是在<span class="math inline">\(0\)</span>附近的，以Sigmoid函数为例，当值在<span class="math inline">\(0\)</span>附近时，其激活函数可看作线性，也就是说整个深度神经网络进行线性化近似了（不确定能不能用这种说法），这即是优点也是缺点，优点是我们把神经网络简化了，这也是实现这种正则化方法的基础；但，由于我们计算的是这种近似线性的东西，当我们要做非常复杂的决策时，这种正则化方法是不适用的。</p>
<h2 id="dropout">3. Dropout</h2>
<p>这个就简单写了，因为基本思路不难。Dropout的基本思路是随机去掉神经元，也就是在进行一批数据训练时，我们随机去掉一些神经元，既可以是隐藏层的，也可以是输入层的，然后进行训练，更新参数；然后在下一批数据进来的时候我们要先恢复回最原来的网络结构，再随机去掉一些神经元，更新参数，继续。。。。 需要注意的是，Dropout方法每次更新的都是同一套参数，也就是不会产生新的参数（这点跟Bagging不同，Bagging是每次训练都有自己单独的一套）。 因为Dropout将原始数据分批迭代，用这种正则化方法前提是训练样本足够大，否则会产生欠拟合现象。</p>
<h2 id="dropconnect">4. DropConnect</h2>
<p>DropConnect跟Dropout其实很像，Dropout是通过去掉一些神经元来防止过拟合的，而DropConnect则是通过随机选择权重的子集设为<span class="math inline">\(0\)</span>，两种方法都能增强模型的泛化能力，都在模型中引入了稀疏性，但不同的是DropConnect是在权重中引入稀疏性而不是在层的输出向量中引入。</p>
<h2 id="早停法early-stopping">5. 早停法（Early stopping）</h2>
<p>训练过神经网络的都知道，当训练次数过多时，模型容易发生过拟合现象，那么我们只要在过拟合之前停止训练就可以了。这就是早停法，也是深度学习中非常常用的一种方法，因为简单又有效。用这种方法我们就要时刻检测验证集的损失，当其开始<strong>持续</strong>上升时，就该停止训练了。</p>
<p><strong><em>END</em></strong><br />
先写到这，另外写到后面总觉得这篇有点问题，我好像把正则化和防止过拟合这两个概念混为一谈了？</p>
<h1 id="参考网站来源">参考网站来源</h1>
<p>网站1：https://developer.aliyun.com/article/632944</p>
<p>网站2：https://cloud.tencent.com/developer/article/1486732</p>
<p>网站3：http://imgtec.eetrend.com/blog/2019/100045162.html</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>【ML-2020/21】CNN &amp; Self-Attention</title>
    <url>/posts/16.html</url>
    <content><![CDATA[<h1 id="写在前面的说明">写在前面的说明</h1>
<p>这个系列【ML-2020/21】大部分是课上内容的简单复述，之前上过但因为笔记写得很乱就忘了很多，所以重来一遍。与其看我这篇，不如直接去看视频，讲得还更生动。视频系列链接<span class="math inline">\(\rightarrow\)</span><a href="https://www.youtube.com/watch?v=OP5HcXJg2Aw">这里</a>。</p>
<p>这里介绍两个常见的Network架构，分别为CNN 和 Self-Attention。 <span id="more"></span></p>
<h1 id="cnn">CNN</h1>
<p>CNN主要是用来处理图像的，对于Fully Connected Network，每个神经元都要观察整张图片，这明显不是高效率的做法，所以更常见的是让每个神经元处理某一特定的pattern,，比如说就像下图： <img src="https://img-blog.csdnimg.cn/d439d0b3fc954e7ab460385d90d50a23.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 我们就希望能简化这个网络，这里有两种解释。</p>
<h2 id="两种解释">两种解释</h2>
<h3 id="receptive-field">1. Receptive field</h3>
<p>就像下图那样： <img src="https://img-blog.csdnimg.cn/170e557b01da49bc93680928517a2df0.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /></p>
<p>每个神经元只负责其中一个区域的信息，并且，同个区域可以有多个不同的神经元负责。 另外，这个Receptive field可以有大有小，也可以让Receptive field只考虑某一特定的channel，比如rgb里的红色区域。 一般来说，最经典的size大小是<span class="math inline">\(3\times 3\)</span>，然后对于同一个Receptive field，一般会有一组神经元（比如64或者128个）去处理它，而且一般来说这个Receptive field的排布是会重叠的，以保证覆盖整个图像，如下： <img src="https://img-blog.csdnimg.cn/1229bff146434d25b5e3639067b5ed76.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /></p>
<h4 id="parameter-sharing">parameter sharing</h4>
<p>对于下面这种情况： <img src="https://img-blog.csdnimg.cn/9c209adfd54248839f667259ba391907.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 鸟喙出现在图中的不同位置，由于每个pattern都有很多个神经元去处理，所以理所当然地，这两种情况都能把鸟喙给检测出来，但对于同种作用的检测器，我们明显不希望它们分属两个不同的东西，不然参数会很多，所以就希望，对于相同作用的检测器，它们的参数应该是相同的（即参数共享）。 至此，就一步步地将模型简化了，如下图所示： <img src="https://img-blog.csdnimg.cn/d006bd63940c41afbf60c143a3d3902e.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 这就是convolutional layer，而用了convolutional layer的就是CNN。另外，CNN的<span class="math inline">\(bias\)</span>一般会比较大。</p>
<h3 id="filter">2. Filter</h3>
<p>对于convolutional layer的另一种解释是用Filter。如下图： <img src="https://img-blog.csdnimg.cn/bd774849c64f4b7d916fc491c7939ef4.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 以右上角这个filter为例，从图片左上角一直滑到右下角，对应相乘后相加。对于每个卷积层都有很多个Filters。 另外，虽然这个filter只是<span class="math inline">\(3\times 3\)</span>的，但由于不断进行卷积，每一个卷积层都把它的考虑范围扩大了，如下图，下面那层的<span class="math inline">\(3\times 3\)</span>等于是考虑了上面那层的<span class="math inline">\(5\times 5\)</span>范围： <img src="https://img-blog.csdnimg.cn/fa0ca16f08df47cf89241d5df3be1e5b.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 也就是说，network越深考虑的范围越大。</p>
<blockquote>
<p>上面的parameter sharing实际就是这里的filter扫过各个区域过程。 上面的receptive filter实际就是这里的不同的filter。 所以两种解释其实是一模一样的。</p>
</blockquote>
<h2 id="pooling">Pooling</h2>
<p>pooling的思想源于降采样，即是对于一幅图，我们把它进行将采用，比如把一幅图的偶数像素都删除，并不妨碍我们识别这幅图。如下图： <img src="https://img-blog.csdnimg.cn/c61338aae46b4c5a88911345f1aa50ff.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> pooling是不需要学习的，就跟激活函数类似。</p>
<h3 id="max-pooling">Max Pooling</h3>
<p>除了Max pooling，还有mean Pooling之类的，都差不多。 基本概念就是选各个区域最大的那个数，比如下图： <img src="https://img-blog.csdnimg.cn/3cb5104423b940a78fec669d4a55b660.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 选完之后就变成下面这样了： <img src="https://img-blog.csdnimg.cn/1dcccadc91294bf5bc1e2a9e39708a55.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 这里区域大小也是自己定的，不一定要用<span class="math inline">\(2\times 2\)</span>。通过这个操作就能把”图片“变小。这个池化层一般是和卷积层交替使用的。 &gt;pooling 是为了减少运算量，但现在由于运算能力的发展，越来越多地方开始去除pooling层了，这也是一种趋势。</p>
<p>至此就有了CNN的基本架构了： <img src="https://img-blog.csdnimg.cn/b5dea29b55ef46c6bc23242e6a4e6bcf.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /></p>
<h1 id="self-attention">Self-Attention</h1>
<p>在CNN中，我们的输入都是一个向量，但如果遇到输入是一组向量的话该怎么办？比如说语言处理，输入的单词长度都不一样，Self-Attention 就是解决这种问题的一种办法。 在这种架构中对于输出一般有以下三种情况：</p>
<blockquote>
<ol type="1">
<li><span class="math inline">\(N\)</span>个输入对应<span class="math inline">\(N\)</span>个输出，比如说输入一句话，我们要分析每个单词对应的词性。</li>
<li><span class="math inline">\(N\)</span>个输入对应<span class="math inline">\(1\)</span>个输出，比如输入一句话，分析其是褒义还是贬义。</li>
<li><span class="math inline">\(N\)</span>个输入对应<span class="math inline">\(M\)</span>个输出，比如翻译。</li>
</ol>
</blockquote>
<p>这里先介绍第一种，即同维度输入输出。以输入一句话为例。 第一种最直接的想法是用Fully Connected，由于要考虑单词间的联系，就有了如下这种结构： <img src="https://img-blog.csdnimg.cn/7739d772a5e948c98fdd41d5c82a9a71.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 这确实能做，某些情况下效果也不坏，但如果我们要考虑整句话，而不是图中的红色框框呢？又一个直接的想法是：扩大window的大小以囊括整句话。 但，这里每句话的长度是不相等的，如果要用这种方法，那就要首先检测数据集中最长那句话有多长，但这样运算量会非常大，而且容易overfitting，而且在测试中也不一定适用。 另一种更好的方法就是Self-Attention，其基本结构如下： <img src="https://img-blog.csdnimg.cn/93e6bba2e2574e2c9c5cea382972a37c.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 也就是说，我们先把整句话用self-attention处理，对应产生新的向量，这时候的每个向量就已经是考虑整句话后的向量了。 这两种网络是可以交替使用的，比如下面这个： <img src="https://img-blog.csdnimg.cn/fa7650543232479f877f07d793263602.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /></p>
<h2 id="运作">运作</h2>
<p>假设我们的输入（或者中间某一层的输出）为这四个向量<span class="math inline">\([a^1,a^2,a^3,a^4]\)</span>，现在我们想要做的就是每一个向量与其它向量的关联程度。 以两个向量为例，计算关联度有很多种方法，比如下面两种： <img src="https://img-blog.csdnimg.cn/cf7e419319e54afebcd5aaadc7724300.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> <strong>以下都用左边那种</strong> 对于四个向量，计算与<span class="math inline">\(a^1\)</span>的关联度如下图： <img src="https://img-blog.csdnimg.cn/f79769229bcc44af8cd9135f94d11a67.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /></p>
<blockquote>
<p>注意，这里也要计算与自身的关联度。</p>
</blockquote>
<p>这里的激活函数不一定要用Soft-max，也可以用其他的比如ReLu。 接下来计算新的向量，也就是根据关联性计算，如下： <img src="https://img-blog.csdnimg.cn/2275dfc0a96049f19baae048018e16bf.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 以上过程可以用矩阵乘法表示。</p>
<h3 id="矩阵乘法表示">矩阵乘法表示</h3>
<p>（这部分很无聊可以直接跳过）<img src="https://img-blog.csdnimg.cn/288c02b8a420479f9e272d7d552e0f28.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> <img src="https://img-blog.csdnimg.cn/6f9c645c1c114706ba87a7d1b06bfeb5.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> <img src="https://img-blog.csdnimg.cn/7ead13e9d4be40ff989996100ac166d7.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> <img src="https://img-blog.csdnimg.cn/c4b2fb755df94b35b09b8753761128bb.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 这其中只有<span class="math inline">\(W^q,W^k,W^v\)</span>是需要学习的。</p>
<h2 id="multi-head-self-attention">Multi-head-Self-attention</h2>
<p>这里以两个head为例： <img src="https://img-blog.csdnimg.cn/d881b6f1cb0f4337a0e9ada6ba36db3d.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 对于两个head计算出的<span class="math inline">\(b^{i,1},b^{i,2}\)</span>可通过新引入的矩阵进行结合： <img src="https://img-blog.csdnimg.cn/17615172e1b343028283242f4f910432.png#pic_center" alt="在这里插入图片描述" /></p>
<h2 id="position-encoding">Position Encoding</h2>
<p>现在还剩一个问题，就是我们虽然考虑了整个句子，但我们没有考虑各个单词的相对位置。 这里需要引入一个位置参数<span class="math inline">\(e^i\)</span>，每个向量都要加上特定的位置参数，如下： <img src="https://img-blog.csdnimg.cn/c5c7c0b35d9c471ba36a69f54dbb2ff4.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 最早的<span class="math inline">\(e^i\)</span>（也就是在paper”Attention all you need“里）长这样： <img src="https://img-blog.csdnimg.cn/f961ce0ac8284eb1938aed959001fc4a.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 这个图的每一列代表一个<span class="math inline">\(e^i\)</span>。位置参数不一定要用这种，可以自己研究。</p>
]]></content>
      <categories>
        <category>ML-2020/21</category>
      </categories>
  </entry>
  <entry>
    <title>非线性优化</title>
    <url>/posts/15.html</url>
    <content><![CDATA[<h1 id="引入">引入</h1>
<p>一个经典的SLAM模型由一个运动方程和一个观测方程构成</p>
<span id="more"></span>
<p>如下式所示：<span class="math display">\[\pmb{x}_k=f(\pmb{x}_{k-1},\pmb{u}_k)+\pmb{w}_k\\\pmb{z}_{k,j}=h(\pmb{y}_j,\pmb{x}_k)+\pmb{v}_{k,j}\]</span>其中<span class="math inline">\(\pmb{u}_k\)</span>是运动传感器的读数或者输入；<span class="math inline">\(\pmb{w}_k\)</span>是该过程中加入的噪声；<span class="math inline">\(f\)</span>是一般函数，用来描述这个过程的；<span class="math inline">\(\pmb{x}\)</span>是相机的位姿，可用SE(3)来描述；<span class="math inline">\(\pmb{y}\)</span>是路标点；<span class="math inline">\(\pmb{z}\)</span>是观测数据；<span class="math inline">\(\pmb{v}_{k,j}\)</span>是这次观测里的噪声。由于观测所用的传感器形式很多，这里的观测数据<span class="math inline">\(\pmb{z}\)</span>和观测方程<span class="math inline">\(h\)</span>也有很多形式。 现假设观测方程由针孔模型给定，在<span class="math inline">\(\pmb{x}_k\)</span>处对路标<span class="math inline">\(\pmb{y}_j\)</span>进行了一次观测，对应到图像上的像素位置为<span class="math inline">\(\pmb{z}_{k,j}\)</span>，则观测方程可以表示为：<span class="math display">\[s\pmb{z}_{k,j}=\pmb{K}(\pmb{R}_k\pmb{y}_j+\pmb{t}_K)\]</span>其中<span class="math inline">\(\pmb{K}\)</span>为相机内参，<span class="math inline">\(s\)</span>为像素点距离，也是<span class="math inline">\((\pmb{R}_k\pmb{y}_j+\pmb{t}_k)\)</span>的第三个分量，若用变换矩阵<span class="math inline">\(\pmb{T}_k\)</span>来描述位姿，则路标<span class="math inline">\(\pmb{y}_j\)</span>要用齐次坐标。</p>
<p>—————————————————————————————————————</p>
<p>简单回顾下相机内参的相关知识<br />
相机内参数是与相机自身特性相关的参数，比如相机的焦距、像素大小等，这些在相机出厂后是固定的。由于镜头的安装精度，形状，传感器上的像素等因素，使镜头的光轴不再穿过图像的正中间。内参矩阵一般是如下形式：<span class="math display">\[\begin{pmatrix} f_x &amp; 0 &amp; c_x \\ 0 &amp; f_y &amp; c_y \\ 0&amp;0&amp;1 \end{pmatrix}\]</span>其中<span class="math inline">\(f_x,f_y\)</span>表示<span class="math inline">\(u,v\)</span>轴的缩放，<span class="math inline">\(c_x,c_y\)</span>表示原点平移的距离。</p>
<p>—————————————————————————————————————</p>
<p>要知道数据在受到噪声影响后会发生什么变化，通常会假设两个噪声项<span class="math inline">\(\pmb{w}_k,\pmb{v}_{k,j}\)</span>，使其满足零均值的高斯分布，即：<span class="math display">\[\pmb{w}_k\sim N(0,\pmb{R}_k)\quad \pmb{v}_k\sim N(0,\pmb{Q}_{k,j})\]</span>其中<span class="math inline">\(\pmb{R}_k,\pmb{Q}_{k,j}\)</span>为协方差矩阵。 通过带噪声的数据<span class="math inline">\(\pmb{z}\)</span>和<span class="math inline">\(\pmb{u}\)</span>来推断位姿<span class="math inline">\(\pmb{x}\)</span>和地图<span class="math inline">\(\pmb{y}\)</span>（及其概率分布），构成一个状态估计问题，即：<span class="math display">\[P(\pmb{x},\pmb{y}|\pmb{z},\pmb{u})\]</span>用贝叶斯法则为：<span class="math display">\[P(\pmb{x},\pmb{y}|\pmb{z},\pmb{u})=\frac{P(\pmb{z},\pmb{u}|\pmb{x},\pmb{y})P(\pmb{x},\pmb{y})}{P(\pmb{z},\pmb{u})}\varpropto P(\pmb{z},\pmb{u}|\pmb{x},\pmb{y})P(\pmb{x},\pmb{y})\]</span>这就是一个求MAP的问题：<span class="math display">\[(\pmb{x},\pmb{y})^*_{MAP}=arg\max P(\pmb{x},\pmb{y}|\pmb{z},\pmb{u}) = arg\max P(\pmb{z},\pmb{u}|\pmb{x},\pmb{y})P(\pmb{x},\pmb{y})\]</span>如果我们不知道先验的话，就变成了求MLE的问题，即：<span class="math display">\[(\pmb{x},\pmb{y})^*_{MLE}=arg\max P(\pmb{z},\pmb{u}|\pmb{x},\pmb{y})\]</span>根据上面，我们假设了噪声项为零均值的高斯分布，所以观测数据的条件概率为：<span class="math display">\[P(\pmb{z}_{j,k}|\pmb{x}_k,\pmb{y}_j)=N(h(\pmb{y}_j,\pmb{x}_k),\pmb{Q}_{k,j})\]</span>(前提：观测是高斯分布的) 然后就是常见的展开取负对数，求最大变成求最小：<span class="math display">\[-\ln(P(\pmb{x}))=\frac{1}{2}\ln((2\pi)^N\det(\Sigma))+\frac{1}{2}(\pmb{x}-\pmb{\mu})^T\Sigma^{-1}(\pmb{x}-\pmb{\mu})\]</span>右边第一项与<span class="math inline">\(\pmb{x}\)</span>无关，就不用管了，只关注第二项那个二次型。 将SLAM的观测模型带入得： <span class="math display">\[\begin{aligned}
 (\pmb{x}_k,\pmb{y}_j)^* 
 &amp;= arg\max N(h(\pmb{y}_j,\pmb{x}_k),\pmb{Q}_{k,j})\\ 
 &amp;= arg\min ((\pmb{z}_{k,j}-h(\pmb{x}_k,\pmb{y}_j))^T\pmb{Q}^{-1}_{k,j}(\pmb{z}_{k,j}-h(\pmb{x}_k,\pmb{y}_j)))
  \end{aligned}\]</span> 在处理<strong>批量</strong>数据时我们通常是假设各个时刻的输入<span class="math inline">\(\pmb{u}\)</span>和观测<span class="math inline">\(\pmb{z}\)</span>相互独立，即是有： <span class="math display">\[P(z,u|x,y)=\prod_k P(u_k|x_{k-1},x_k)\prod_{k,j}P(z_{k,j}|x_k,y_j)\]</span>然后我们定义相关误差，运动误差为：<span class="math display">\[e_{u,k}=x_k-f(x_{k-1},u_k)\]</span>观测误差为：<span class="math display">\[e_{z,j,k}=z_{k,j}-h(x_k,y_j)\]</span>用跟上面同样的方法也能写出<span class="math inline">\(P(u_k|x_{k-1},x_{k})\)</span>的最小二乘表达式。现在我们要让这两个误差最小，使用误差平方和定义一个cost function，如下：<span class="math display">\[\min J(x,y)=\sum_ke^T_{u,k}R^{-1}_ke_{u,k}+\sum_k\sum_je^T_{z,k,j}Q^{-1}_{k,j}e_{z,k,j}\]</span>这就是我们要处理的最小二乘问题，其实就是两个MLE结合。这个明显是非线性的最小二乘问题，没法直接计算，现介绍以下几种迭代方法。</p>
<h1 id="非线性最小二乘问题">非线性最小二乘问题</h1>
<p>现讨论最广泛的情况，即以<span class="math inline">\(f(x)\)</span>作为二乘项，我们要解决以下这个问题：<span class="math display">\[\min_{x}\frac{1}{2}||f(x)||^2 \quad x \in R^n \]</span>当函数<span class="math inline">\(f\)</span>很简单时，直接求导等于0，比较各个极值点即可；但当<span class="math inline">\(f\)</span>很复杂，求导不容易计算时，可用迭代的方法。 迭代的基本思想是：</p>
<blockquote>
<ol type="1">
<li>给定初值<span class="math inline">\(x_0\)</span>；<br />
</li>
<li>对于第<span class="math inline">\(k\)</span>次迭代，寻找一个增量<span class="math inline">\(\Delta x_k\)</span>，使得<span class="math inline">\(||f(x_k+\Delta x_k)||_2^2\)</span>达到极小值；<br />
</li>
<li>若<span class="math inline">\(\Delta x_k\)</span>足够小，则停止；<br />
</li>
<li>否则，令<span class="math inline">\(x_{k+1}=x_k+\Delta x_k\)</span>，返回步骤2。</li>
</ol>
</blockquote>
<p>现在的问题是如何确定这个增量<span class="math inline">\(\Delta x_k\)</span>。</p>
<h2 id="一阶-二阶梯度法">一阶/ 二阶梯度法</h2>
<p>这就是常见的Gradient Descent。 首先根据泰勒展开（对平方展开）可写为：<span class="math display">\[||f(x+\Delta x)||^2_2 \approx ||f(x)||_2^2 + J(x)\Delta x+\frac{1}{2}\Delta x^TH\Delta x\]</span>其中<span class="math inline">\(J,H\)</span>分别为雅可比和海森矩阵。</p>
<h3 id="一阶最速下降法steepest-method">一阶（最速下降法，Steepest Method）</h3>
<p>即只保留一阶梯度：<span class="math display">\[\min_{\Delta x}||f(x)||_2^2+J\Delta x\]</span>增量方向为<span class="math inline">\(\Delta x^*=-J^T(x)\)</span>，通常还要计算步长。</p>
<h4 id="二阶牛顿法">二阶（牛顿法）</h4>
<p>即同时保留一阶和二阶：<span class="math display">\[\Delta x^8=arg\min ||f(x)||_2^2+J(x)\Delta x+\frac{1}{2}\Delta x^TH\Delta x\]</span>令上式关于<span class="math inline">\(\Delta x\)</span>的导数为0则有：<span class="math display">\[H\Delta x=-J^T\]</span></p>
<h3 id="方法分析优缺点">方法分析（优缺点）</h3>
<p>最速下降和牛顿虽然直观，但两者均有一些缺点：</p>
<ol type="1">
<li>最速下降法由于过于贪婪会碰到zigzag问题，如下图： <img src="https://img-blog.csdnimg.cn/c01029543cbd45a6bcab62e4cdc0bf10.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 每次迭代均是往梯度下降最快的方向进行，对于简单的问题依旧需要很多次迭代才能达到最优点。</li>
<li>牛顿法由于是二阶的，可以沿着曲线直接到最优点，但却需要计算复杂的海森矩阵。</li>
</ol>
<p>所以这两种方法都不太实用，我们希望可以在不计算海森矩阵的前提下实现二阶梯度，就有了以下两种方法</p>
<h2 id="gauss-newton">Gauss-Newton</h2>
<p>一阶近似<span class="math inline">\(f(x)\)</span>为：<span class="math display">\[
f(x+\Delta x) \approx f(x)+J(x)\Delta x\]</span>其平方误差为：<span class="math display">\[
\begin{aligned}
\frac{1}{2}||f(x)+J(x)\Delta x||^2 &amp;=\frac{1}{2}(f(x)+J(x)\Delta x)^T(f(x)+J(x)\Delta x)\\&amp;=\frac{1}{2}(||f(x)||_2^2+2f(x)^TJ(x)\Delta x+\Delta x^TJ(x)^TJ(x)\Delta x)
\end{aligned}
\]</span>令其对<span class="math inline">\(\Delta x\)</span>的导数为零得：<span class="math display">\[
2J(x)^Tf(x)+2J(x)^TJ(x)\Delta x=0
\]</span>即：<span class="math display">\[
J(x)^TJ(x)\Delta x = -J(x)^Tf(x)
\]</span>我们可以把这个式子写为：<span class="math display">\[
H\Delta x=g
\]</span>与上面的二阶（牛顿）法做对比，可以发现这里实际是用<span class="math inline">\(J(x)^TJ(x)\)</span>近似海森矩阵。 所以这个迭代过程可以写为：</p>
<blockquote>
<ol type="1">
<li>给定初始值<span class="math inline">\(x_0\)</span>；</li>
<li>对于第<span class="math inline">\(k\)</span>次迭代，求出当前的雅可比矩阵<span class="math inline">\(J(x_k)\)</span>和误差<span class="math inline">\(f(x_k)\)</span>；</li>
<li>求解增量方程：<span class="math inline">\(H\Delta x=g\)</span>，即<span class="math inline">\(\Delta x_k=H^{-1}g\)</span>；</li>
<li>若<span class="math inline">\(\Delta x_k\)</span>足够小，则停止，否则令<span class="math inline">\(x_{k+1}=x_k+\Delta x_k\)</span>，并返回步骤2。</li>
</ol>
</blockquote>
<p><strong>总结</strong>： 由于海森矩阵不容易算，所以这里用雅可比的平方近似海森矩阵。这种方法简单实用，但<span class="math inline">\(\Delta x_k=H^{-1}g\)</span>中无法保证<span class="math inline">\(H\)</span>可逆（二次近似不可靠）。 然后Levenberg_Marquadt方法在一定程度上改善了它。</p>
<h2 id="levenberg_marquadt">Levenberg_Marquadt</h2>
<p>与G-N方法不同，这里引进了一个参考值，或者说区域，用于描述近似程度：<span class="math display">\[
\rho=\frac{f(x+\Delta x)-f(x)}{J(x)\Delta x}
\]</span>这个式子可以理解为：<strong>实际下降 / 近似下降</strong>，若太大，则减小近似范围，若太小，则增加近似范围。 其迭代过程可以描述为：</p>
<blockquote>
<ol type="1">
<li>给定初始值<span class="math inline">\(x_0\)</span>以及初始优化半径<span class="math inline">\(\mu\)</span>；</li>
<li>对于第<span class="math inline">\(k\)</span>次迭代，求解：<span class="math display">\[\min_{\Delta x_k}\frac{1}{2}||f(x_k)+J(x_k)\Delta x_k||^2, \quad s.t. ||D\Delta x_k||^2\le\mu\]</span>这里<span class="math inline">\(\mu\)</span>是信赖区域的半径，<span class="math inline">\(D\)</span>的话可以理解为形状参数，当<span class="math inline">\(D=1\)</span>时，表示取一个球，后来普遍令<span class="math inline">\(D\)</span>取非负对角矩阵，表示椭球。</li>
<li>计算<span class="math inline">\(\rho\)</span>；</li>
<li>若<span class="math inline">\(\rho&gt;\frac{3}{4}\)</span>，则<span class="math inline">\(\mu=2\mu\)</span>；</li>
<li>若<span class="math inline">\(\rho&lt;\frac{1}{4}\)</span>，则<span class="math inline">\(\mu=0.5\mu\)</span>；（这两个都是经验值）</li>
<li>如果<span class="math inline">\(\rho\)</span>大于某个阈值，认为近似可行，令<span class="math inline">\(x_{k+1}=x_k+\Delta x_k\)</span></li>
<li>判断算法是否收敛，如果不收敛则返回步骤2，否则结束。</li>
</ol>
</blockquote>
<p>最后，在上面的步骤中还剩一个问题，就是第二步的带约束的优化问题，依旧用拉格朗日进行计算即可：<span class="math display">\[
\min_{\Delta x_k}\frac{1}{2}||f(x_k)+J(x_k)\Delta x_k||^2+\frac{\lambda}{2}||D\Delta x||^2
\]</span>参照G-N展开，增量方程为：<span class="math display">\[
(H+\lambda D^TD)\Delta x=g 
\]</span>在Levenberg方法中，取<span class="math inline">\(D=I\)</span>，则：<span class="math display">\[(H+\lambda I)\Delta x=g\]</span>这里的<span class="math inline">\(H\)</span>依旧是<span class="math inline">\(J(x)^TJ(x)\)</span>。 对比G-N方法，这里前面加上了一项，相当于在G-N方法的基础上增强了<span class="math inline">\(H\)</span>的正定性。</p>
<h2 id="g-n-和-l-m的对比">G-N 和 L-M的对比</h2>
<blockquote>
<ol type="1">
<li>G-N方法属于线性搜索方法：先找到方向，再确定长度；<br />
</li>
<li>L-M方法属于信赖区域方法（Trust Region），认为近似只在区域内可靠。<br />
</li>
<li>LM相比于GN，能够保证增量方程的正定性，即认为近似只在一定范围内成立，如果近似不好则缩小范围。收敛能够更好一点。<br />
</li>
<li>从增量方程上来看，可以看成一阶和二阶的混合，参数<span class="math inline">\(\lambda\)</span>控制两边的权重。以<span class="math display">\[(H+\lambda I)\Delta x=g\]</span>为例，去掉<span class="math inline">\(H\)</span>则变成一阶的，去掉<span class="math inline">\(\lambda\)</span>项则变成二阶的。<br />
</li>
<li>一般来说，LM的计算量会比GN大一些，但收敛性也更好。所以简单的情况用GN，复杂的用LM。</li>
</ol>
</blockquote>
<p><strong><em>注：本文参考自《视觉SLAM十四讲》</em></strong></p>
]]></content>
      <categories>
        <category>视觉SLAM相关</category>
      </categories>
  </entry>
  <entry>
    <title>【学习随记】时间空间复杂度</title>
    <url>/posts/14.html</url>
    <content><![CDATA[<h1 id="时间复杂度">时间复杂度</h1>
<p>常用的时间复杂度有7种： <span id="more"></span></p>
<blockquote>
<ol type="1">
<li>常数时间复杂度；<span class="math inline">\(O(1)\)</span><br />
</li>
<li>对数时间复杂度；<span class="math inline">\(O(\log n)\)</span><br />
</li>
<li>线性时间复杂度；<span class="math inline">\(O(n)\)</span><br />
</li>
<li>平方时间复杂度；<span class="math inline">\(O(n^2)\)</span><br />
</li>
<li>立方时间复杂度；<span class="math inline">\(O(n^3)\)</span><br />
</li>
<li>指数时间复杂度；<span class="math inline">\(O(2^n)\)</span><br />
</li>
<li>阶乘时间复杂度；<span class="math inline">\(O(n!)\)</span></li>
</ol>
</blockquote>
<p><strong>注：从上到下时间复杂度越来越大</strong> 常规的时间复杂度都容易分析，麻烦的是递归，遇到递归时一般需要把状态树画出来。比如说代码是：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">fib</span><span class="params">(n)</span></span>&#123;</span><br><span class="line">	<span class="keyword">if</span>(n&lt;<span class="number">2</span>)	<span class="keyword">return</span> n;</span><br><span class="line">	<span class="keyword">return</span> fib(n-<span class="number">1</span>) + fib(n-<span class="number">2</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>当输入为6时，状态树为： <img src="https://img-blog.csdnimg.cn/img_convert/d621cf89d64f72fc092eb1f2982c68bc.png#pic_center" alt="在这里插入图片描述" /> 结果是<span class="math inline">\(O(2^n)\)</span>，分析就很麻烦。</p>
<p>对于递归，常见的情况有4种</p>
<blockquote>
<ol type="1">
<li>二分查找<br />
  在有序数列中查找目标数，每次查找都一分为二，最后时间复杂度是<span class="math inline">\(O(\log n)\)</span>。<br />
</li>
<li>二叉树遍历<br />
 虽然每次也都一分为二，但每边都已相同的时间复杂度继续下去，或者说，二叉树每个节点都且仅遍历一次，所以时间复杂度是<span class="math inline">\(O(n)\)</span>。比如二叉树的前，中，后序遍历，都是线性复杂度。<br />
</li>
<li>有序的二维矩阵查找<br />
  时间复杂度是<span class="math inline">\(O(n)\)</span>。<br />
</li>
<li>归并排序<br />
  时间复杂度是<span class="math inline">\(O(n\log n)\)</span></li>
</ol>
</blockquote>
<p>关于具体的数学推导的话参看链接: <a href="https://www.zhihu.com/question/21387264">数学推导</a></p>
<p>关于主定理的话，参看这个：<a href="https://zh.wikipedia.org/wiki/%E4%B8%BB%E5%AE%9A%E7%90%86">主定理</a></p>
<h1 id="空间复杂度">空间复杂度</h1>
<p>这个就比时间复杂度善良很多，一般就考虑两种情况：</p>
<blockquote>
<ol type="1">
<li>新开数组的长度<br />
</li>
<li>递归的深度 若递归里新开数组，我们就考虑大的那个复杂度就行。</li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>学习随记</category>
      </categories>
  </entry>
  <entry>
    <title>支持向量机（SVM）</title>
    <url>/posts/13.html</url>
    <content><![CDATA[<h1 id="引入">引入</h1>
<p>对于上一篇提到的风险（Risk）：<span class="math display">\[R(w)\leq R_{emp}(w)+\epsilon(N,p^*,h)\]</span>其中<span class="math inline">\(N\)</span>是训练数据量，<span class="math inline">\(p^*\)</span>是到达边界的概率，<span class="math inline">\(h\)</span>是VC维度。 <span id="more"></span><br />
为了最小化风险，经典的机器学习算法是：</p>
<blockquote>
<p>固定<span class="math inline">\(\epsilon(...)\)</span>，最小化经验风险<span class="math inline">\(R_{emp}\)</span><br />
其中<span class="math inline">\(\epsilon(...)\)</span>是通过保证一些模型参数不变以固定的，比如说神经网络的隐藏层数量。</p>
</blockquote>
<p>支持向量机（SVM）的做法则是：</p>
<blockquote>
<p>保证<span class="math inline">\(R_{emp}(w)\)</span>固定以最小化<span class="math inline">\(\epsilon\)</span><br />
当数据可分时，<span class="math inline">\(R_{emp}(w)=0\)</span>。<br />
通过改变VC维度以控制<span class="math inline">\(\epsilon\)</span>。</p>
</blockquote>
<p>支持向量机一般分为三类：</p>
<blockquote>
<ol type="1">
<li>线性可分支持向量机（linear support vector machine in linearly separable case ），采用的方法是硬间隔最大化（hard margin maximization）</li>
<li>线性支持向量机（linear supportvector machine），是指训练数据近似线性可分时，采用的方法是软间隔最大化（soft margin maximization）</li>
<li>非线性支持向量机（non-linear support vector machine），是指训练数据线性不可分时，采用核技巧（kernel trick）及软间隔最大化</li>
</ol>
</blockquote>
<p>对于二分类问题：</p>
<blockquote>
<p>输入空间：欧式空间或离散集合<br />
特征空间：欧式空间或希尔伯特空间</p>
</blockquote>
<p>线性可分支持向量机、线性支持向量机：</p>
<blockquote>
<p>假设这两个空间的元素一一对应，并将输入空间中的输入映射为特征空间中的特征向量；</p>
</blockquote>
<p>非线性支持向量机：</p>
<blockquote>
<p>利用一个从输入空间到特征空间的非线性映射将输入映射为特征向量；</p>
</blockquote>
<p><strong>支持向量机的学习是在特征空间进行的</strong>。</p>
<p>（这篇会有不少补充推导，这些过程其实看不看都没事，不想看的话就直接记结论。看了，能减少学习过程中的疑惑，不看，也不会对SVM的理解产生大的影响）</p>
<h1 id="线性svm">线性SVM</h1>
<h2 id="硬间隔最大化">硬间隔最大化</h2>
<p>直白地说，就是找一个（超）平面，使其距离最近数据点的距离最大，就是最大化下图这个<span class="math inline">\(margin\)</span>。 <img src="https://img-blog.csdnimg.cn/2021072008164997.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 为了最大化这个margin，我们首先要找到一个超平面，使数据得到线性分离：<span class="math display">\[y_i(\pmb{w}^T\pmb{x}_i+b)\ge1 \quad \forall i\]</span>并且让至少一个点满足<span class="math inline">\(y_i(\pmb{w}^T\pmb{x}_i+b)=1\)</span>。 然后这个间隔就是<span class="math inline">\(\frac{1}{||w||}\)</span>。</p>
<p>———————————————————————————————————————</p>
<p>补充1</p>
<p><strong>上面的（不）等式怎么来的，这里的1又是什么，为什么间隔会是这个？</strong> 我们要找到最大间隔分类器，就是要找一个合适的参数<span class="math inline">\(w,b\)</span>，使其满足：<span class="math display">\[\max margin(w,b)\]</span> <span class="math display">\[s.t.\quad y_i(\pmb{w}^T\pmb{x}_i+b)&gt;0\]</span>稍微解释一下这个约束条件：这里有个前提是我们已经把数据分类好了，因为线性可分，其经验风险为0，<span class="math inline">\(y_i\)</span>与括号里的式子同号，相乘大于0。 再多说一点，点到超平面的距离表示分类预测的确信程度，<span class="math inline">\(w^Tx+b\)</span>与标签符号是否一致表示分类的准确性，所以<span class="math inline">\(y(w^Tx+b)\)</span>表示分类的正确性和确信度。 这里定义一下这个<span class="math inline">\(margin\)</span>函数如下：<span class="math display">\[margin(w,b)=\min\limits_{w,b,x_i,i=1...N} distant(w,b,x_i)\]</span>也就是说我们要找到距离（超）平面最近的那个点所对应的距离。 然后点到（超）平面的距离为（距离公式之前那篇分类问题里有用到）：<span class="math display">\[distant=\frac{1}{||\pmb{w}||}|\pmb{w}^Tx_i+b|\]</span>也就是说，间隔公式可以写为：<span class="math display">\[margin(w,b)=\min\limits_{w,b,x_i,i=1...N} \frac{1}{||\pmb{w}||}|\pmb{w}^Tx_i+b|\]</span>即是说，最上面我们要处理的问题就变为：<span class="math display">\[\max\limits_{w,b} \min\limits_{x_i,i=1...N}\frac{1}{||\pmb{w}||}|\pmb{w}^Tx_i+b|\]</span> <span class="math display">\[s.t.\quad y_i(\pmb{w}^T\pmb{x}_i+b)&gt;0\]</span>因为绝对值符号有点碍眼，根据其约束条件我们可以将其改写成：<span class="math display">\[\max\limits_{w,b} \min\limits_{x_i,i=1...N}\frac{1}{||\pmb{w}||}y_i(\pmb{w}^Tx_i+b)\]</span>因为我们要求的是最大值对应的参数，所以这里加个系数也无所谓（或许这里写成<span class="math inline">\(argmax\)</span>之类的会更好理解？）。<br />
继续改写！<br />
前面那个<span class="math inline">\(\frac{1}{||\pmb{w}||}\)</span>跟<span class="math inline">\(\min\)</span>无关，所以可以前移，然后对于约束条件，因为左式大于0，所以我们肯定能找到一个<span class="math inline">\(\gamma\)</span>，使左式最小值等于<span class="math inline">\(\gamma\)</span>，即是：<span class="math display">\[\max\limits_{w,b} \frac{1}{||\pmb{w}||}\min\limits_{x_i,i=1...N}y_i(\pmb{w}^Tx_i+b)\]</span> <span class="math display">\[s.t.\quad \exists \gamma   \min\limits_{x_i,y_i,i=1,...,N} y_i(\pmb{w}^T\pmb{x}_i+b)=\gamma\]</span> 令<span class="math inline">\(\gamma=1\)</span>，将约束条件带入我们要处理的式子，就可以写成：<span class="math display">\[\max\limits_{w,b} \frac{1}{||\pmb{w}||}\]</span> <span class="math display">\[s.t.\quad   \min\limits_{x_i,y_i,i=1,...,N} y_i(\pmb{w}^T\pmb{x}_i+b)=1\]</span> 为什么这里可以让<span class="math inline">\(\gamma=1\)</span>呢？因为这相当于超平面的等比例缩放，取1只是为了方便计算而已，要有闲情逸致的话，取个3.1415926都行。<br />
继续改写约束条件!<br />
这个最小值的等式我们可以用不等式代替，然后这个求最大值的我们也可以用最小值代替，分子分母换位而已，即：<span class="math display">\[arg\min\limits_{w,b} \frac{1}{2}w^Tw\]</span> <span class="math display">\[s.t.\quad   y_i(\pmb{w}^T\pmb{x}_i+b)\ge1 \quad for \quad \forall i=1,...,N\]</span>这里乘上<span class="math inline">\(\frac{1}{2}\)</span>，又把根号去掉，只是为了方便后面计算，毕竟我们要的是<span class="math inline">\(w,b\)</span>，不要求计算实际最大值，所以这无所谓。这是一个典型的凸二次规划问题：有<span class="math inline">\(N\)</span>个约束条件，目标函数是二次的。<br />
至此，我们就知道上面的1和（不）等式是什么东西，以及间隔为什么会是<span class="math inline">\(\frac{1}{||w||}\)</span>了。</p>
<p>——————————————————————————————————————</p>
<p><strong>支持向量（Support vectors）</strong>：所有位于边缘的点，即满足：<span class="math inline">\(y_i(\pmb{w}^T\pmb{x}_i+b)=1\)</span>的点。 现在计算以下优化问题(写法上跟上面补充内容有一丁点不一样，但意思是一样的)：<span class="math display">\[arg\min\limits_{w,b} \frac{1}{2}||w||^2\]</span> <span class="math display">\[s.t. \quad y_i(\pmb{w}^T\pmb{x}_i+b)-1\ge0 \quad \forall i\]</span>带约束条件的优化问题依旧是拉格朗日,如下： <img src="https://img-blog.csdnimg.cn/20210720095426527.png#pic_center" alt="在这里插入图片描述" /> 要最小化上面这个<span class="math inline">\(L(w,b,\alpha)\)</span>，即是要同时对<span class="math inline">\(b,w\)</span>求偏导并令偏导等于0，如下： <img src="https://img-blog.csdnimg.cn/20210720100108960.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 现在还剩个<span class="math inline">\(\alpha_i\)</span>。 在这里，只有在边缘点<span class="math inline">\(\alpha_i\)</span>是不为0的，其余点的<span class="math inline">\(\alpha_i\)</span>都等于0，比如下面这张图： <img src="https://img-blog.csdnimg.cn/20210720100607658.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 只有圈出的那三个点的<span class="math inline">\(\alpha_i\)</span>是有意义的。所以SVM是一个稀疏（sparse）的学习机，分类只取决于少数的点。 要求解这个<span class="math inline">\(\alpha_i\)</span>，我们就要引进这个问题的对偶问题。</p>
<p>———————————————————————————————————————</p>
<p>补充2——拉格朗日对偶</p>
<p>在约束最优化问题中，常常利用拉格朗日对偶性(Lagrange duality)将原始问题转换为对偶问题，通过解对偶问题得到原始问题的解。 比如说，现有原始问题： 设<span class="math inline">\(f(x),c(x),h(x)\)</span>是定义在<span class="math inline">\(\mathcal{R}^n\)</span>上的连续可微函数：<span class="math display">\[\min\limits_{x\in \mathcal{R}^n}f(x)\]</span> <span class="math display">\[s.t. \quad c_i(x)\leq0,\quad i=1,...,k\]</span> <span class="math display">\[h_j(x)=0,\quad j=1,...,l\]</span> 引入拉格朗日函数<span class="math inline">\(\alpha_i,\beta_j\)</span>为乘子<span class="math inline">\(\alpha_i\ge0\)</span>:<span class="math display">\[L(x,\alpha,\beta)=f(x)+\sum_{i=1}^{k}\alpha_ic_i(x)+\sum_{j=1}^{l}\beta_jh_j(x)\]</span>考虑<span class="math inline">\(x\)</span>的函数，<span class="math inline">\(P\)</span>为原始问题：<span class="math display">\[\theta_P(x)=\max\limits_{\alpha,\beta,\alpha_i\ge0}L(x,\alpha,\beta)\]</span>假定给定一个<span class="math inline">\(x\)</span>，如果<span class="math inline">\(x\)</span>违反约束条件，即：<span class="math display">\[c_i(x)&gt;0\quad h_j(x)\ne0\]</span>则 <span class="math display">\[\theta_P(x)=\max\limits_{\alpha,\beta,\alpha_i\ge0}[f(x)+\sum_{i=1}^{k}\alpha_ic_i(x)+\sum_{j=1}^{l}\beta_jh_j(x)]=+\infty\]</span>即（这个结论下面需要用到）： <img src="https://img-blog.csdnimg.cn/20210720162958878.png#pic_center" alt="在这里插入图片描述" /> 考虑极小问题：<span class="math display">\[\min\limits_{x}\theta_P(x)=\min\limits_{x}\max\limits_{\alpha,\beta,\alpha_i\ge0}L(x,\alpha,\beta)\]</span>这个极小问题与原始的最优化问题等价<span class="math display">\[p^*=\min\limits_{x}\theta_P(x)\]</span> <strong>原始问题</strong>变为：<span class="math display">\[\min\limits_{x}\theta_P(x)=\min\limits_{x}\max\limits_{\alpha,\beta,\alpha_i\ge0}L(x,\alpha,\beta)\]</span>称为广义拉格朗日函数的极小极大问题，定义最优解为<span class="math inline">\(p^*=\min\limits_{x}\theta_P(x)\)</span>。</p>
<p><strong>对偶问题</strong>为： 定义：<span class="math inline">\(\theta_D(\alpha,\beta)=\min L(x,\alpha,\beta)\)</span> 所以：<span class="math display">\[\max\limits_{\alpha,\beta,\alpha_i\ge0}\theta_D(\alpha,\beta)=\max\limits_{\alpha,\beta,\alpha_i\ge0}\min\limits_xL(x,\alpha,\beta)\]</span>称为广义拉格朗日函数的极大极小问题，约束条件为<span class="math inline">\(\alpha_i\ge0,i=1,...,k\)</span>，这个称为原始问题的对偶问题，对偶问题的最优值为：<span class="math display">\[d^*=\max\limits_{\alpha,\beta,\alpha_i\ge0}\theta_D(\alpha,\beta)\]</span>若原始问题和对偶问题都有最优值，则：<span class="math display">\[d^*=\max\limits_{\alpha,\beta,\alpha_i\ge0}\min\limits_xL(x,\alpha,\beta)\leq \min\limits_{x}\max\limits_{\alpha,\beta,\alpha_i\ge0}L(x,\alpha,\beta)=p^*\]</span></p>
<p>在SVM这里，对偶问题其实就可以表示为很直接的一个式子：<span class="math display">\[\max\min f(x)\leq \min\max f(x)\]</span>这个式子直观上就能理解，所以这里不解释。 这个不等式称为弱对偶关系，但在SVM我们要强对偶关系，也就是：<span class="math display">\[\max\min f(x)=\min\max f(x)\]</span>而凸优化的二次规划问题恰恰就满足强对偶条件。</p>
<p>————————————————————————————————————————</p>
<p>也就是说，现在问题变成求以下优化问题：<span class="math display">\[\max\limits_{\alpha}\min\limits_{w,b}L(w,b,\alpha)\]</span><span class="math display">\[s.t. \quad \alpha\ge0\]</span>现在问题就好算了。 在继续往下之前先对上面做个总结（上面的看着挺乱）：</p>
<ol type="1">
<li>首先我们要求解以下这个优化问题：<span class="math display">\[arg \min\limits_{w,b}\frac{1}{2}w^Tw\]</span><span class="math display">\[s.t. \quad y_i(\pmb{w}^T\pmb{x}_i+b)\ge1 \quad for \quad \forall i=1,...,N\]</span></li>
<li>用拉格朗日法进行去约束，变为：<span class="math display">\[\min\limits_{w,b}\max\limits_{\alpha}L(w,b,\alpha)\]</span><span class="math display">\[s.t. \quad \alpha\ge0\]</span>这里再简单说一下，这个优化跟<span class="math inline">\(w,b\)</span>已经无关了（这么说好像不太准确。。。），就是说，如果<span class="math inline">\(y_i(\pmb{w}^T\pmb{x}_i+b)&lt;1\)</span>的话，根据上面补充（关于<span class="math inline">\(+\infty\)</span>）的内容，值是等于<span class="math inline">\(+\infty\)</span>的，再根据前面的最小约束，怎么都取不到这个值，所以不管<span class="math inline">\(w,b\)</span>的值怎么取，结果都是等价于是在约束<span class="math inline">\(y_i(\pmb{w}^T\pmb{x}_i+b)\ge1\)</span>里取，这是被上面拉格朗日的方程限制死的，所以我们不再需要考虑上面的约束了。</li>
<li>根据强对偶关系，优化问题等价于：<span class="math display">\[\max\limits_{\alpha}\min\limits_{w,b}L(w,b,\alpha)\]</span><span class="math display">\[s.t. \quad \alpha\ge0\]</span></li>
</ol>
<p>总结结束，继续<br />
对于这个优化问题就很容易求了，两次求导即可。上面我们已经有过一次求导了，把上面的结论写下来即是： <img src="https://img-blog.csdnimg.cn/20210720193528375.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 将其带入下面这个最开始的拉格朗日方程里： <img src="https://img-blog.csdnimg.cn/20210720195830686.png#pic_center" alt="在这里插入图片描述" /> 得到下面这个式子： <img src="https://img-blog.csdnimg.cn/20210720195959244.png#pic_center" alt="在这里插入图片描述" /> 这就是拉格朗日方程的最小值，再由于： <img src="https://img-blog.csdnimg.cn/20210720200724523.png#pic_center" alt="在这里插入图片描述" /> 继续改写成如下的式子 <img src="https://img-blog.csdnimg.cn/20210720200849578.png#pic_center" alt="在这里插入图片描述" /> 这就是我们要最大化的东西。 这里我们要这么大费周章地推导求解对偶问题，主要是因为：</p>
<blockquote>
<ol type="1">
<li>对偶问题往往更容易求解；<br />
</li>
<li>引入核函数，即为非线性SVM做准备。</li>
</ol>
</blockquote>
<h2 id="软间隔最大化">软间隔最大化</h2>
<p>软间隔最大化问题跟硬间隔的差不多，只是训练集中会有一些异常点（outlier）不能满足约束条件<span class="math inline">\(y_i(\pmb{w}^T\pmb{x}_i+b)\ge1\)</span>。如下图： <img src="https://img-blog.csdnimg.cn/20210720211011111.png#pic_center" alt="在这里插入图片描述" /></p>
<p>一个简单的思路就是，将这些数据点映射到新的特征空间，然后就可以线性区分了，比如用小半径的RBF核函数（这在下面非线性SVM会介绍）。 但这会产生一个问题，就是<span class="math inline">\(VC\)</span>维度特别大，可能会导致过拟合。</p>
<p>另一种解决方法是：选择性地忽略一些点。对每个样本点<span class="math inline">\((x_i,y_i)\)</span>引进一个松弛变量（slack variables）<span class="math inline">\(\xi_i\ge0\)</span>，使： <img src="https://img-blog.csdnimg.cn/20210721215433434.png#pic_center" alt="在这里插入图片描述" /></p>
<p>函数间隔加上松弛变量大于等于1，即约束条件变为：<span class="math display">\[y_i(\pmb{w}^T\pmb{x}_i+b)\ge1-\xi_i\]</span> 目标函数就变为：<span class="math display">\[\frac{1}{2}||w||^2+C\sum_{i=1}^{N}\xi_i\]</span>其中<span class="math inline">\(C&gt;0\)</span>为惩罚参数。<br />
由此可得，线性不可分的线性支持向量机的学习问题为： <img src="https://img-blog.csdnimg.cn/20210721215658777.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /></p>
<p>其中<span class="math inline">\(w\)</span>的解是唯一的，<span class="math inline">\(b\)</span>不是（不作证明）。</p>
<p>最大化边际（margin），同时最小化所有不在边际之<strong>外</strong>的数据点的惩罚 权重 C 允许我们指定权衡。 通常通过交叉验证确定 即使数据是可分离的，最好允许偶尔的惩罚（penalty）。</p>
<p>剩下的解优化问题跟硬间隔几乎是一模一样了，不过还是再过一遍吧。 回到我们上面的学习问题，带约束的优化，用拉格朗日函数如下：<span class="math display">\[L(w,b,\xi,\alpha,\mu)=\frac{1}{2}||w||^2+C\sum_{i=1}^{N}\xi_i-\sum_{i=1}^{N}\alpha_i(y_i(wx_i+b)-1+\xi_i)-\sum_{i=1}^{N}\mu_i\xi_i\]</span>其中: <span class="math inline">\(\alpha_i\ge 0,\mu_i\ge0\)</span>。对偶问题是这个函数的极大极小问题，首先求<span class="math inline">\(L(w,b,\xi,\alpha,\mu)\)</span>对<span class="math inline">\(w,b,\xi\)</span>的极小，即是求导等于0，得： <img src="https://img-blog.csdnimg.cn/20210720212504986.png#pic_center" alt="在这里插入图片描述" /> 即是说，极小值为： <img src="https://img-blog.csdnimg.cn/20210720212541576.png#pic_center" alt="在这里插入图片描述" /> 再对<span class="math inline">\(\alpha\)</span>求极大： <img src="https://img-blog.csdnimg.cn/20210720212652248.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 这里<span class="math inline">\(alpha_i\leq C\)</span>称为box constraint(箱体约束)。</p>
<blockquote>
<p>设<span class="math inline">\(\alpha^*=(\alpha_1^*,...,\alpha_N^*)^T\)</span>是上面对偶问题的一个解，若存在<span class="math inline">\(\alpha\)</span>的一个分量<span class="math inline">\(\alpha_j\)</span>，<span class="math inline">\(0&lt;\alpha_j^*&lt;C\)</span>，则原始问题的解<span class="math inline">\(w,b\)</span>为：<span class="math display">\[w^*=\sum_{i=1}^{N_s}\alpha_i^*y_ix_i\]</span><span class="math display">\[b^*=y_j-\sum_{i=1}^{N_s}y_i\alpha_i^*(x_ix_j)\]</span></p>
</blockquote>
<p>(注：在硬间隔中求出的<span class="math inline">\(\alpha\)</span>也是一个向量。)</p>
<p>———————————————————————————————————————</p>
<p>补充3——合页损失函数（hinge loss function）</p>
<p>线性支持向量机学习还有另一种解释，就是最小化下面这个目标函数：<span class="math display">\[\sum_{i=1}^{N}[1-y_i(wx_i+b)]_++\lambda||w||^2\]</span>第一项：<span class="math inline">\(L(y(wx+b))=[1-y(wx+b)]_+\)</span>就称为合页损失函数，其中：<span class="math display">\[[z]_+=z,\quad z&gt;0\]</span><span class="math display">\[[z]_+=0,\quad z\le0\]</span>最优化问题就可以等价为： <img src="https://img-blog.csdnimg.cn/20210720215356556.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /></p>
<p>———————————————————————————————————————</p>
<h1 id="非线性svm">非线性SVM</h1>
<p>然而很多时候我们得到的数据并不是直接用硬或者软间隔进行线性分类的，比如下面这个： <img src="https://img-blog.csdnimg.cn/20210721163854894.png#pic_center" alt="在这里插入图片描述" /> 明显，不管怎么画直线都没办法把这两类分开，这时候我们可以增加一个维度，从二维变三维，变成下面这种情况： <img src="https://img-blog.csdnimg.cn/20210721164054328.png#pic_center" alt="在这里插入图片描述" /> 这个就可以用（超）平面进行线性划分了。记住一点：<strong>相对于低维，高维更可能可以用线性进行分类</strong>。 也就是说在这里，数据的输入空间不再直接充当特征空间，我们要新构造一个特征空间，就跟之前的线性回归那里的思想是一样的。 我们的输入是<span class="math inline">\(\pmb{x}\)</span>，然后就要找到一个非线性变换<span class="math inline">\(\phi\)</span>，将输入空间映射到特征空间（但这个映射也不能太过分了，不然容易过拟合）。 现在回顾上面的对偶形式： <img src="https://img-blog.csdnimg.cn/20210721165319361.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 通过非线性变换，可以得到下面的方程： <img src="https://img-blog.csdnimg.cn/20210721165401767.png#pic_center" alt="在这里插入图片描述" /> 这个式子可以看出，当两个特征空间的元素<span class="math inline">\(\phi(x_i)\)</span>和<span class="math inline">\(\phi(x_j)\)</span>内积时，就会产生一个标量。 再看判别函数：<span class="math display">\[y(x)=w^T\phi(x)+b\]</span>由于<span class="math display">\[w=\sum_{i=1}^{N_s}\alpha_iy_i\phi(x_i)\]</span>则非线性判别函数可以写为：<span class="math display">\[y(x)=\sum_{i=1}^{N_s}\alpha_iy_i\phi(x_i)^T\phi(x)+b\]</span>其中<span class="math inline">\(N_s\)</span>是支持向量的数目。 又发现，判别函数也可以只用非线性特征的标量积来表示。 但是我们想直接计算它们内积是不太可能的，因为这个非线性转换可能会出现无限维的情况。 这就引出了核技巧（Kernel Trick），即使用核函数取代每一个标量积：<span class="math display">\[K(x_i,x_j)=\phi(x_i)^T\phi(x_j)\]</span>如果我们能找到这么一个核函数，那就可以避免高维空间的映射，转而直接计算这个标量积。 但该怎么判断是否存在这样的核函数呢？</p>
<h2 id="多项式核polynomial-kernel">多项式核（Polynomial Kernel）</h2>
<p>以以下二阶核为例：<span class="math display">\[K(x,y)=(x^Ty)^2\]</span>等同于点积：<span class="math display">\[K(x,y)=(x^Ty)^2=x_1^2y_1^2+2x_1x-2y_1y_2+y_1^2y_2^2\]</span>对应的非线性转换为： <img src="https://img-blog.csdnimg.cn/2021072117411341.png#pic_center" alt="在这里插入图片描述" /> 以这个为例，核方法的一个优点是核计算量的简便，比如这里的计算数为：3（<span class="math inline">\(x,y\)</span>的点积）+1（结果的平方）=4<br />
另外需要注意，一个核函数的非线性变换不是唯一的，比如上面这个核函数，其对应的变换还可能是： <img src="https://img-blog.csdnimg.cn/20210721174440506.png#pic_center" alt="在这里插入图片描述" /> <strong>转换空间维度以及VC维度的确定</strong> 令<span class="math inline">\(C_d(x)\)</span>为将一个向量映射到所有阶数为<span class="math inline">\(d\)</span>的有序单项式空间的变换。<span class="math display">\[K(x,y)=(x^Ty)^d=C_d(x)^TC_d(y)\]</span>转换空间<span class="math inline">\(H\)</span>的维度为：<span class="math inline">\(C_{d+N-1}^{d}\)</span>。 比如说：<span class="math display">\[N=16\times 16=256 \quad d=4\]</span>则转换空间维度为：<span class="math display">\[dim(H)=183181376\]</span>该分类器的<span class="math inline">\(VC\)</span>维度为：<span class="math inline">\(dim(H)+1\)</span>。</p>
<h2 id="径向基函数radial-basis-functions">径向基函数（Radial Basis Functions）</h2>
<p>径向基函数（RBF），通常定义为空间任一点到某一中心的<span class="math inline">\(y\)</span>的欧氏距离的单调函数，最常用的是高斯核函数，定义为：<span class="math display">\[K(x,y)=\exp(-\frac{||x-y||^2}{2\sigma^2})\]</span>换种说法的话，这个核函数可以衡量<span class="math inline">\(x\)</span>和<span class="math inline">\(y\)</span>的相似度的。<strong>其对应的变换空间是无限维的</strong>，所以其<strong>VC维度也是无限维的</strong>。当<span class="math inline">\(x\)</span>和<span class="math inline">\(y\)</span>很接近时值为1，很远时值为0，<span class="math inline">\(\sigma\)</span>为函数的宽度参数，控制函数的径向作用范围。 用这个核函数的效果如下： <img src="https://img-blog.csdnimg.cn/20210721204316240.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /></p>
<h2 id="mercers-condition">Mercer's Condition</h2>
<p>既然核函数能极大地简便我们的计算量，那现在就有个问题，我们该如何判断给定的一个函数<span class="math inline">\(K\)</span>是一个有效的核函数呢，或者说，我们如何判断这个<span class="math inline">\(K\)</span>能够替代<span class="math inline">\(\phi(x_i)^T\phi(x_j)\)</span>？ 这就引出了Mercer定理。</p>
<blockquote>
<p>Mercer定理： 如果函数<span class="math inline">\(K\)</span>是<span class="math inline">\(R^n\times R^n\rightarrow R\)</span>上的映射（也就是从两个<span class="math inline">\(n\)</span>维向量映射到实数域），当且仅当对于任意训练样例<span class="math inline">\([x_1,...,x_n]\)</span>，其相应的核函数矩阵是对称半正定的话，则<span class="math inline">\(K\)</span>是一个有效核函数（也称为Mercer核函数）。</p>
</blockquote>
<p>——————————————————————————————————————</p>
<p>补充4——正定与半正定矩阵</p>
<blockquote>
<p>正定矩阵 给定一个大小为 <span class="math inline">\(n\times n\)</span>的实对称矩阵 <span class="math inline">\(A\)</span> ，若对于任意长度为 <span class="math inline">\(n\)</span> 的非零向量 <span class="math inline">\(x\)</span> ，有<span class="math inline">\(x^TAx&gt;0\)</span> 恒成立，则矩阵 <span class="math inline">\(A\)</span> 是一个正定矩阵。 正定矩阵的特征值都大于0。</p>
</blockquote>
<blockquote>
<p>半正定矩阵 给定一个大小为 <span class="math inline">\(n\times n\)</span>的实对称矩阵 <span class="math inline">\(A\)</span> ，若对于任意长度为 <span class="math inline">\(n\)</span> 的非零向量 <span class="math inline">\(x\)</span> ，有<span class="math inline">\(x^TAx\ge0\)</span> 恒成立，则矩阵 <span class="math inline">\(A\)</span> 是一个半正定矩阵。 也就是说，半正定包含正定，其特征值均为非负，主子式大于等于0。</p>
</blockquote>
<p>———————————————————————————————————————</p>
<p>也就是说，为了证明<span class="math inline">\(K\)</span>是有效核函数，我们不需要去找相应的映射函数<span class="math inline">\(\phi\)</span>，只需要在训练集上求出各个<span class="math inline">\(K_ij\)</span>，判断其是否为半正定即可。 上面的Mercer定理用数学表达的话可以写为：</p>
<blockquote>
<p>对于满足<span class="math inline">\(\int g(x)^2dx&lt;0\)</span>的<span class="math inline">\(g(x)\)</span>，若<span class="math inline">\(K(x,y)\)</span>满足<span class="math inline">\(\int \int K(x,y)g(x)g(y)dxdy\ge0\)</span>，则<span class="math inline">\(K(x,y)\)</span>是一个有效核函数。</p>
</blockquote>
<p>判断一个函数是否满足Mercer定理并不容易，但我们可以根据现有的有效核函数构造出新的，也就是说，如果<span class="math inline">\(K_1(x,y),K_2(x,y)\)</span>是有效核函数，则下面这些也是：<span class="math display">\[cK_1(x,y)\\K_1(x,y)+K_2(x,y)\\K_1(x,y)K_2(x,y)\\f(x)K_1(x,y)f(y)\\...\]</span></p>
<h2 id="相关补充">相关补充</h2>
<p>除了上面的，非齐次多项式核函数也可以用来表示<span class="math inline">\(d\)</span>阶多项式，形式如下：<span class="math display">\[K(\pmb{x},\pmb{y})=(\pmb{x}^T\pmb{y}+c)^d\]</span>除了Gaussian RBF 核函数：<span class="math display">\[K(\pmb{x},\pmb{y})=\exp(-\frac{||\pmb{x}-\pmb{y}||^2}{2\sigma^2})\]</span>还有其它比如Hyperbolic tangent核函数：<span class="math display">\[K(\pmb{x},\pmb{y})=tahn(a\pmb{x}^T\pmb{y}+b)\]</span></p>
]]></content>
      <categories>
        <category>学习总结</category>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>【学习随记】各种空间的简单介绍</title>
    <url>/posts/12.html</url>
    <content><![CDATA[<p>因为经常遇到各种特定的空间，有些没接触过，有些又容易弄混，这里做个记录，这篇应该会不断更新，毕竟空间概念这么多。（注：这里只是最简单的介绍） <span id="more"></span> 先解释相关概念。</p>
<h1 id="相关概念">相关概念：</h1>
<h2 id="完备性">完备性</h2>
<p>简单说的话，就是对极限封闭。也就是说，如果对于空间<span class="math inline">\(S\)</span>内的一点<span class="math inline">\(s_i\)</span>，<span class="math inline">\(\lim_{i\rightarrow\infty}s_i=s\)</span>，<span class="math inline">\(s\)</span>也属于空间<span class="math inline">\(S\)</span>的话，则称该空间具有完备性。</p>
<h2 id="内积">内积</h2>
<p>说到内积，第一反应应该就是向量内积，即：<span class="math display">\[\langle a,b \rangle = |a|*|b|*\cos\theta\]</span>，但更广泛的话，内积应该满足以下三个条件(<span class="math inline">\(f,g\)</span>都是空间元素)：</p>
<blockquote>
<ol type="1">
<li>对称性：<span class="math inline">\(\langle f,g \rangle = \langle g,f \rangle\)</span>；</li>
<li>正定性：<span class="math inline">\(\langle f,f \rangle \ge0\)</span>，当且仅当<span class="math inline">\(f=0\)</span>时等号成立。</li>
<li>线性：<span class="math inline">\(\langle r_1f_1+r_2f_2,g \rangle = r_1\langle f_1,g \rangle+r_2\langle f_2,g \rangle\)</span></li>
</ol>
</blockquote>
<h1 id="空间">空间</h1>
<h2 id="度量距离空间">度量（距离）空间</h2>
<p>设<span class="math inline">\(X\)</span>是非空集合，对于<span class="math inline">\(X\)</span>中的任意两个元素<span class="math inline">\(x,y\)</span>，按某一法则都对应唯一的实数<span class="math inline">\(\rho(x,y)\)</span>，并满足下面三个条件（距离公理）：</p>
<blockquote>
<ol type="1">
<li>非负性：<span class="math inline">\(\rho(x,y)\ge0\)</span>，当且仅当<span class="math inline">\(x=y\)</span>时，<span class="math inline">\(\rho(x,y)=0\)</span>;</li>
<li>对称性：<span class="math inline">\(\rho(x,y)=\rho(y,x)\)</span>;</li>
<li>三角不等式：对任意<span class="math inline">\(x,y,z\)</span>，<span class="math inline">\(\rho(x,y)\leq\rho(x,z)+\rho(z,y)\)</span></li>
</ol>
</blockquote>
<p>则称<span class="math inline">\(\rho(x,y)\)</span>为<span class="math inline">\(x\)</span>与<span class="math inline">\(y\)</span>的距离（或度量），并称<span class="math inline">\(X\)</span>是以<span class="math inline">\(\rho\)</span>为距离的距离空间，记作<span class="math inline">\((X,\rho)\)</span>。 这里的距离不局限于“点空间”内的距离，比如下面两个也满足距离：<span class="math display">\[\rho(x,y)=\max\limits_{1\leq k\leq n}|x_k-y_k|\]</span><span class="math display">\[\rho(x,y)=\sum_{k=1}^{n}|x_k-y_k|\]</span> <span class="math inline">\(L^p[a,b]\)</span>表示区间<span class="math inline">\([a,b]\)</span>绝对值<span class="math inline">\(p\)</span>次幂<span class="math inline">\(L\)</span>可积函数的全体，并把几乎处处相等的函数看成是同一个函数，对于<span class="math inline">\(x,y\in L^p[a,b]\)</span>，规定<span class="math display">\[\rho(x,y)=[\int_a^b|x(t)-y(t)|^pdt]^{1/p},p\ge1\]</span>则<span class="math inline">\(L^p[a,b]\)</span>构成一个距离空间，称为<span class="math inline">\(p\)</span>次幂可积函数空间。</p>
<h2 id="线性空间">线性空间</h2>
<p>直观理解的话就是拥有加法和数乘的非空集合。首先要求非空，其次满足加法运算的4个属性：</p>
<blockquote>
<ol type="1">
<li>加法交换律</li>
<li>加法结合律</li>
<li>存在零元：<span class="math inline">\(x+0=x\)</span></li>
<li>存在逆元：<span class="math inline">\(x+(-x)=0\)</span></li>
</ol>
</blockquote>
<p>满足数乘的4个属性：</p>
<blockquote>
<ol type="1">
<li><span class="math inline">\(1x=x\)</span></li>
<li><span class="math inline">\(a(bx)=(ab)x\)</span></li>
<li><span class="math inline">\((a+b)x=ax+bx\)</span></li>
<li><span class="math inline">\(a(x+y)=ax+ay\)</span></li>
</ol>
</blockquote>
<h2 id="赋范空间">赋范空间</h2>
<p>设<span class="math inline">\(X\)</span>是实（或复）线性空间，如果对于<span class="math inline">\(X\)</span>中的每个元素<span class="math inline">\(x\)</span>，按照一定的法则对应于实数<span class="math inline">\(||x||\)</span>，且满足：</p>
<blockquote>
<ol type="1">
<li><span class="math inline">\(||x||\ge0\)</span>，当且仅当<span class="math inline">\(x\)</span>等于零元（<span class="math inline">\(x=0\)</span>）时<span class="math inline">\(||x||=0\)</span>;</li>
<li><span class="math inline">\(||ax||=|a|||x||\)</span>，<span class="math inline">\(a\)</span>是实（或复）数；</li>
<li><span class="math inline">\(||x+y||\leq||x||+||y||\)</span></li>
</ol>
</blockquote>
<p>则称<span class="math inline">\(X\)</span>是实（或复）赋范线性空间，<span class="math inline">\(||x||\)</span>称为<span class="math inline">\(x\)</span>的范数。</p>
<blockquote>
<p>注：赋范线性空间必然是距离空间。定义<span class="math display">\[\rho(x,y)=||x-y||\]</span></p>
</blockquote>
<p>与距离空间的不同在于：</p>
<ol type="1">
<li>平移不变性：<span class="math inline">\(d(x+a,y+a)=d(x,y)\)</span>，<span class="math inline">\(x,y,a\in X\)</span></li>
<li>齐次性：<span class="math inline">\(d(ax,ay)=|a|d(x,y)\)</span>，<span class="math inline">\(x,y\in X\)</span>，<span class="math inline">\(a\in K\)</span>。（<span class="math inline">\(K\)</span>是实（或复）数域）。</li>
</ol>
<h2 id="banach-空间">Banach 空间</h2>
<p>如果赋范线性空间是完备的，则称该赋范线性空间是Banach 空间。</p>
<h2 id="内积空间">内积空间</h2>
<p>（注：这里的括号全都应该为尖括号） 设<span class="math inline">\(X\)</span>是定义在实（或复）数域<span class="math inline">\(K\)</span>上的线性空间，若对于<span class="math inline">\(X\)</span>任意一对有序元素<span class="math inline">\(x,y\)</span>, 恒对应数域<span class="math inline">\(K\)</span>的值<span class="math inline">\((x, y)\)</span>，且满足：</p>
<blockquote>
<ol type="1">
<li><span class="math inline">\((ax,y)=a(x,y)\)</span></li>
<li><span class="math inline">\((x+y,z)=(x,z)+(y,z)\)</span></li>
<li><span class="math inline">\((x,y)=(y,x)\)</span></li>
<li><span class="math inline">\((x,x)\ge0\)</span>，当且仅当<span class="math inline">\(x=0\)</span>时等号成立</li>
</ol>
</blockquote>
<p>则称<span class="math inline">\(X\)</span>为内积空间，<span class="math inline">\((x,y)\)</span>称为<span class="math inline">\(x,y\)</span>的内积。跟上面内积的概念可以结合理解。</p>
<h2 id="hilbert-空间">Hilbert 空间</h2>
<p>完备的内积空间称为Hilbert空间，且Hilbert空间必为Banach 空间。 或者换种说法： Hilbert空间是：完备的，可能是无限维的，被赋予内积的线性空间。</p>
]]></content>
      <categories>
        <category>学习随记</category>
      </categories>
  </entry>
  <entry>
    <title>线性降维与统计学习理论（Linear Dimensionality Reduction &amp; Statistical Learning Theory）</title>
    <url>/posts/11.html</url>
    <content><![CDATA[<h1 id="线性降维linear-dimensionality-reduction以pca为例">线性降维（Linear Dimensionality Reduction）（以PCA为例）</h1>
<h2 id="引入">引入</h2>
<p>主成分分析（Principal Component Analysis，PCA）是最常见的线性降维方法。 <span id="more"></span> 拿之前的线性回归举例，对于最小二乘法的线性回归，其求解参数为：<span class="math display">\[\hat{w}=(\hat{X}\hat{X})^{-1}\hat{X}y\]</span>其中<span class="math inline">\(\hat{X}\in \mathcal{R}^{d\times n}\)</span>，<span class="math inline">\(y\in \mathcal{R}^{n\times 1}\)</span>。 若直接求解<span class="math inline">\(d\times d\)</span>的逆矩阵，其复杂度为<span class="math inline">\(O(d^3)\)</span>，所以我们就想找到一个新的维度<span class="math inline">\(d^{new}\)</span>，使其远小于原维度，即<span class="math inline">\(d^{new}&lt;&lt;d\)</span>，但又不能对结果造成很大的影响。 这就引出了PCA，我们要抓住数据的”本质“。</p>
<h2 id="具体讲解">具体讲解</h2>
<p>现假设我们的原始数据<span class="math inline">\(X=\left\{x_1,...,x_n\right\}\)</span>，以第<span class="math inline">\(i\)</span>个数据为例，其维度为：<span class="math display">\[x^i\in \mathcal{R}^M\]</span> 然后这第<span class="math inline">\(i\)</span>个数据的低纬度表示法为：<span class="math display">\[a^i\in \mathcal{R}^D, \quad D&lt;&lt;M\]</span>现在要做的就是找到个映射满足：<span class="math display">\[x^i \rightarrow a^i\]</span> 现在我们限制这样的映射为线性方程，即：<span class="math display">\[a^i=Bx^i, \quad B\in \mathcal{R}^{D\times M}\]</span> ———————————————————————————————————————</p>
<p>在继续之前先补充一个知识点，即是说，任何向量我们都可以改写成下面这样：<span class="math display">\[\pmb{x}=\sum_{i=1}^{M}a_i\pmb{u}_i, \quad其中 \pmb{u}_i^T\pmb{u}_j=\delta_{ij}\]</span> 当<span class="math inline">\(i=j\)</span>时，<span class="math inline">\(\delta_{ij}=1\)</span>，否则等于0，<span class="math inline">\(u_i\)</span>与<span class="math inline">\(u_j\)</span>正交。举个例子如下： <img src="https://img-blog.csdnimg.cn/20210717210412307.png#pic_center" alt="在这里插入图片描述" /> 其中标量参数<span class="math inline">\(a\)</span>可作如下表述： <img src="https://img-blog.csdnimg.cn/20210717213720773.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 也就是说，这个<span class="math inline">\(a_i\)</span>是一个投影结果。</p>
<p>————————————————————————————————————————</p>
<p>所以原始向量我们可以写成如下两部分相加的形式：<span class="math display">\[x^n=\sum_{i=1}^{D}a_i\pmb{u}_i+\sum_{j=D+1}^{M}b_j\pmb{u}_j\approx\tilde{x}^n\]</span>其中第一部分就是我们想要的降维后的数据，第二部分是一些<span class="math inline">\(error\)</span>组成的数据。这里的<span class="math inline">\(\tilde{x}^n\)</span>是我们降维之后的数据，也就是第一部分。 我们选择<span class="math inline">\(D\)</span>的标准是最小化数据的均方差，即是： <img src="https://img-blog.csdnimg.cn/2021071722173678.png#pic_center" alt="在这里插入图片描述" /> 我们现在要最小化这个<span class="math inline">\(error\)</span>，首先对<span class="math inline">\(error\)</span>进行改写： <img src="https://img-blog.csdnimg.cn/20210717230441723.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 把上图的结论写下来：<span class="math display">\[E(u)=\sum_{n=1}^N||x^n||^2-(u^Tx^n)^2=\sum^N_{n=1}||x^n||^2-(a^n)^2\]</span> 最小化这个函数就等同于最大化后面的那个部分，也可以称为最大化投影的方差，这也就是PCA的基本思想，可以想象成把数据投影到直线上（跟另一篇分类问题里提到的有些类似），然后让数据尽可能分开，也就是让方差尽量大。 所以现在我们有：<span class="math display">\[\max \frac{1}{N}\sum_{n=1}^{N}a_n^2\]</span>注意，此处我们是假设均值为0的情况，否则则用下面的式子：<span class="math display">\[\max \frac{1}{N}\sum_{n=1}^{N}(a_n-\mu)^2\]</span> 但一般我们不会用上面这个，因为计算太麻烦，所以会事先让每个数据都减去均值，以保证零均值的情况，即是：<span class="math display">\[x^n-\overline{x}\]</span> 现在就可以写出如下形式的方程了： <img src="https://img-blog.csdnimg.cn/2021071801252064.png#pic_center" alt="在这里插入图片描述" /> 根据这个图就能发现，把数据投影到<span class="math inline">\(u_1\)</span>上能保留数据更多的信息。现在的问题就是如何找到这条轴，以及沿这条轴的数据的方差。 以下图为例： <img src="https://img-blog.csdnimg.cn/20210718012915445.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 这里<span class="math inline">\(\lambda_i\)</span>是沿方向<span class="math inline">\(u_i\)</span>的边际方差（marginal variance）。</p>
<p>现假设如下数据，<span class="math display">\[X=[x^1,...,x^n]\in \mathcal{R}^{M\times N}\]</span>是一个有<span class="math inline">\(N\)</span>个<span class="math inline">\(M\)</span>维向量的矩阵，即<span class="math inline">\(x^i\in\mathcal{R}^M\)</span>。另<span class="math inline">\(u\in\mathcal{R}^M\)</span>为一个输入空间的方向，则第<span class="math inline">\(j\)</span>个向量<span class="math inline">\(x^j\)</span>在这个向量<span class="math inline">\(u\)</span>上的投影为：<span class="math display">\[a_j=u^Tx^j=\sum_{i=1}{M}X_{ij}u_i\]</span>目标是找到一个方向<span class="math inline">\(u\)</span>，使所有输入向量的投影的方差达到最大。<br />
首先计算所有投影的均值：<span class="math display">\[\overline{a}=\frac{1}{N}\sum^N_{j=1}a_j=\frac{1}{N}\sum_{j=1}^{N}\sum_{i=1}^{M}X_{ij}u_i=\sum^M_{i=1}u_i\mu_i\]</span>其中<span class="math inline">\(\mu_i=\frac{1}{N}\sum^{N}_{j=1}X_{ij}\)</span> 计算方差为： <img src="https://img-blog.csdnimg.cn/20210718045939294.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 上面的方差也可以用另一个推导，意思一样但比较简洁： <img src="https://img-blog.csdnimg.cn/20210718050457581.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /></p>
<p>其中<span class="math inline">\(\pmb{C}\)</span>为协方差矩阵。在约束<span class="math inline">\(||u||^2=1\)</span>下最大化这个方差，带约束的优化问题就得看拉格朗日了。如下： <img src="https://img-blog.csdnimg.cn/2021071805054020.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 然后就很神奇地出现了特征值-特征向量的等式，其中最大的特征值就是最大方差，其对应的特征向量就是最大方差的方向。<br />
根据上面的结论可得：<span class="math inline">\(\pmb{CU}=\pmb{U}\Lambda\)</span>，其中<span class="math inline">\(\Lambda=diag(\lambda_1,...,\lambda_m)\)</span>，<span class="math inline">\(\pmb{U}=[\pmb{u_1,...,u_M}]\)</span>，分别为特征值构成的对角矩阵以及特征向量构成的矩阵</p>
<blockquote>
<p>注意，这里我们假设矩阵<span class="math inline">\(\pmb{U}\)</span>是正交的，也就是范数为1，且这个对角矩阵的特征值已经按照从大到小排列了</p>
</blockquote>
<p>然后我们就能把协方差矩阵<span class="math inline">\(\pmb{C}\)</span>进行分解：<span class="math display">\[\pmb{C}=\pmb{U}\Lambda\pmb{U}^{-1}\]</span><span class="math display">\[\pmb{C}=\pmb{U}\Lambda\pmb{U}^{T}\]</span> 如下图： <img src="https://img-blog.csdnimg.cn/20210718062453710.png#pic_center" alt="在这里插入图片描述" /> 根据协方差矩阵的定义我们还知道，矩阵<span class="math inline">\(\pmb{C}\)</span>是一个正的实对称矩阵。<br />
对于极小（约等于0）的特征值以及其对应的特征向量，我们就直接舍去，因为他们对于矩阵的信息作用不大。借助提取的特征值我们就能重新构造新的数据集，如下： <img src="https://img-blog.csdnimg.cn/20210718063540655.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" />这种方法计算出来的满足 最小均方误差，即：<span class="math display">\[\min E(\pmb{u_1,...,u_D})=\sum_{n=1}^{N}||\pmb{x}^n-\tilde{\pmb{x}}^n||^2\]</span> 现在这个降维过程就以及大致结束了，做个总结，其步骤大致是：</p>
<blockquote>
<ol type="1">
<li>数据标准化（求解并减去均值）<br />
</li>
<li>求解协方差矩阵并进行分解，提取前<span class="math inline">\(D\)</span>个特征值对应的特征向量（特征值已从大到小排列）<br />
</li>
<li>用如下方法重构数据：<br />
<img src="https://img-blog.csdnimg.cn/20210718064537281.png#pic_center" alt="在这里插入图片描述" /></li>
<li>有时也会将每个维度的方差归一.</li>
</ol>
</blockquote>
<p>现在还剩最后一个问题：怎么选择维度<span class="math inline">\(D\)</span>？<br />
<span class="math inline">\(D\)</span>越大的话近似效果越好，极端情况就是<span class="math inline">\(D=M\)</span>，但这就没意义了。<br />
选择维度<span class="math inline">\(D\)</span>一般有以下两种考量：</p>
<ol type="1">
<li>看表现效果，当选取某维度达到我们需要的效果时，就选这个维度<span class="math inline">\(D\)</span>；<br />
</li>
<li>事先定一个值，比如0.9，然后按照下面公式选取所需维度： <img src="https://img-blog.csdnimg.cn/20210718065326292.png#pic_center" alt="在这里插入图片描述" /></li>
</ol>
<h2 id="pca的总结与应用">PCA的总结与应用</h2>
<p>总结：</p>
<blockquote>
<p>PCA将数据映射到线性子空间内<br />
PCA最大化映射的方差<br />
PCA最小化重构误差</p>
</blockquote>
<p>应用：</p>
<blockquote>
<p>PCA允许我们将高维的输入空间转化为低维的特征空间，同时捕捉到数据的本质<br />
PCA为数据找到一个更自然的坐标系(more natural coordinate)<br />
PCA 常用于高维输入数据的预处理</p>
</blockquote>
<h1 id="统计学习理论statistical-learning-theory">统计学习理论（Statistical Learning Theory）</h1>
<p>统计学习理论的目的是：我们一开始并不知道正确的模型，而是需要从一组模型中选一个最优的。<br />
这里的最优指的是模型的泛化能力，即不能只在训练数据上误差最小。<br />
既然提到了这类优化，就得引入一个概念——风险（Risk）。 现在我们有一个损失函数：<span class="math display">\[L(y,f(x,w))\]</span>则其经验风险（Empirical risk）为：<span class="math display">\[R_{emp}(w)=\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i,w))\]</span>这里的<span class="math inline">\(N\)</span>是数据点的数量。<br />
举个例子，对于二次方的损失函数，可表示为：<span class="math display">\[R_{emp}(w)=\frac{1}{N}\sum_{i=1}^{N}(y_i-f(x_i,w))^2\]</span></p>
<p>但在现实中，我们更关注真实风险（True Risk）：<span class="math display">\[R(w)=\int L(y,f(x,w))p(x,y)dxdy=E_{x,y\sim p(x,y)}[L(y,f(x,w))]\]</span>这里的<span class="math inline">\(p(\pmb{x},y)\)</span>是<span class="math inline">\(\pmb{x}\)</span>和<span class="math inline">\(y\)</span>的联合概率密度。<br />
由这个式子可以看出，这个风险是所有数据集的预期误差，是泛化误差的均值。<br />
但这里有一个问题，虽然<span class="math inline">\(p(x,y)\)</span>是确定的，但我们通常无法知道，也就是说我们不能直接计算这个真实风险。</p>
<p>关于经验误差和真实误差，下面这个图就能很好说明了：</p>
<p><img src="https://img-blog.csdnimg.cn/20210718174731487.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" /> 这三条边界都很好地把数据分开了，也就是说经验误差都是0，但用于测试时效果很可能会有很大差别。</p>
<h2 id="经验风险与真实风险的对比">经验风险与真实风险的对比</h2>
<p><img src="https://img-blog.csdnimg.cn/20210718182451841.png#pic_center" /></p>
<blockquote>
<p>真实风险：<br />
优点：具有衡量泛化的能力<br />
缺点：通常我们不知道<span class="math inline">\(p(\pmb{x},y)\)</span></p>
</blockquote>
<blockquote>
<p>经验风险：<br />
优点：不需要知道<span class="math inline">\(p(\pmb{x},y)\)</span><br />
缺点：无法直接衡量泛化能力<br />
（学习算法通常时最小化经验误差）</p>
</blockquote>
<p>当数据的分布非常集中，经验风险可以通过蒙特卡洛采样和求平均近似表示真实风险，当数据集无限大时，这种近似效果会很好。 现给出如下这个图： <img src="https://img-blog.csdnimg.cn/20210718220214545.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 有如下两个假设：</p>
<blockquote>
<ol type="1">
<li>当训练数据越来越多时，经验风险会收敛于真实风险；<br />
</li>
<li>收敛是均匀的。</li>
</ol>
</blockquote>
<p>也就是说有如下两个式子： <img src="https://img-blog.csdnimg.cn/20210718220521913.png#pic_center" alt="在这里插入图片描述" /> <img src="https://img-blog.csdnimg.cn/20210718220529526.png#pic_center" alt="在这里插入图片描述" /> 其中的inf, sup分别表示下限和上限。 如果收敛是均匀的，则： <img src="https://img-blog.csdnimg.cn/20210718221008431.png#pic_center" alt="在这里插入图片描述" /> （这里多说一句，如果训练数据很多的话，这个<span class="math inline">\(p^*\)</span>是趋近于0的） 对于<span class="math inline">\((1-p^*)\)</span>的情况，则有： <img src="https://img-blog.csdnimg.cn/20210718221431296.png#pic_center" alt="在这里插入图片描述" /> 根据上面式子以及那副曲线图，可得： <img src="https://img-blog.csdnimg.cn/20210718221600918.png#pic_center" alt="在这里插入图片描述" /> 以上就是收敛特性的推导过程。这里有个重要的<strong>充要</strong>条件：均匀收敛。<br />
<strong>经验风险最小化保证了当<span class="math inline">\(N\rightarrow \infty\)</span>时真实风险也最小。</strong></p>
<h2 id="风险边界risk-bound">风险边界（Risk Bound）</h2>
<p>思想就是：根据经验风险确定真实风险的上限，也就是说：<span class="math display">\[R(w)\leq R_{emp}(w)+\epsilon(N,p^*,h)\]</span>其中：<span class="math inline">\(N\)</span>是训练数据总数；<span class="math inline">\(p^*\)</span>是遇到边界的概率；<span class="math inline">\(h\)</span>是学习机的学习能力，称为VC维度（VC-dimension）。 学习能力（Learning Power）可用下面这幅图解释： <img src="https://img-blog.csdnimg.cn/20210718222837981.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 就是模型的复杂程度</p>
<h3 id="vc-dimension">VC-dimension</h3>
<p>VC全程是Vapnik-Chervonenkis，大概定义就是：</p>
<blockquote>
<p>一个函数族的VC维度是指能够被该函数族正确分类的数据点的最大数量（无论数据点的标签配置如何）。 VC维度是对一个分类器的分类能力或者说 "学习能力 "的衡量。</p>
</blockquote>
<p>VC维度的确定：对于<span class="math inline">\(\mathcal{R}^n\)</span>的线性分类器（超平面），VC维度为<span class="math inline">\(n+1\)</span>。比如说，对于下图所示的线性分类器(<span class="math inline">\(\mathcal{R}^2\)</span>)，维度为3. <img src="https://img-blog.csdnimg.cn/20210718223543362.png#pic_center" alt="在这里插入图片描述" /> （注：上面这种确定方法只能说是对大多数情况都有效的，但不是全部。）</p>
<p>(这篇还未写完，后面关于VC维度的其余概念，不太明白在干嘛，先放着，以后有想法了再补充 )</p>
]]></content>
      <categories>
        <category>学习总结</category>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>分类问题（Classification）</title>
    <url>/posts/10.html</url>
    <content><![CDATA[<h1 id="引入">引入</h1>
<p>开始前先回顾一下贝叶斯决策理论。 <span id="more"></span> 给定观测变量<span class="math inline">\(x\)</span>，我们要找到类别<span class="math inline">\(C_k\)</span>的后验概率，有如下关系： <img src="https://img-blog.csdnimg.cn/20210717001003505.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 决策的条件是： 如果<span class="math inline">\(p(C_1|x)&gt;p(C_2|x)\)</span>，则选择<span class="math inline">\(C_1\)</span>，否则<span class="math inline">\(C_2\)</span>。用上面的条件概率则可以写成： <img src="https://img-blog.csdnimg.cn/20210717001233629.png#pic_center" alt="在这里插入图片描述" /> 遵从这一规则的分类器称为贝叶斯最优分类器（Bayes optimal classifier）。<br />
以前我们要计算计算这个分类后验的话，需要根据贝叶斯公式，建立模型并估计条件密度<span class="math inline">\(p(x|C_k)\)</span>和类别先验<span class="math inline">\(p(C_k)\)</span>，通过最大化<span class="math inline">\(p(C_k|x)\)</span>来使错误概率最小。<br />
但现在我们不再需要对密度进行建模，而是直接编码决策边界，然后最小化错误概率。</p>
<h1 id="判别函数discriminant-functions">判别函数（Discriminant Functions）</h1>
<h2 id="基本知识">基本知识</h2>
<p>大概就是通过比较法对数据进行分类。现有数据<span class="math inline">\(x\)</span>，以及<span class="math inline">\(K\)</span>个类别，对应有<span class="math inline">\(K\)</span>个判别函数：<span class="math display">\[y_1(x),...,y_k(x)\]</span>当<span class="math display">\[y_k(x)&gt;y_j(x)\]</span>时，将数据<span class="math inline">\(x\)</span>分类给<span class="math inline">\(C_k\)</span>。 举个例子，根据贝叶斯分类器，判别函数可有如下几种形式：<span class="math display">\[y_k(x)=p(C_k|x)\]</span> <span class="math display">\[y_k(x)=p(x|C_k)p(C_k)\]</span> <span class="math display">\[y_k(x)=\log(x|C_k)+\log p(C_k)\]</span> 再举个例子，当<span class="math inline">\(K=2\)</span>，即是说只有两个类别时，判断为类别<span class="math inline">\(1\)</span>的标准有以下几种形式：<span class="math display">\[y_1(x)&gt;y_2(x)\]</span> <span class="math display">\[y(x)&gt;0\]</span> 其中<span class="math inline">\(y(x)\)</span>可以为以下两种形式：<span class="math display">\[y(x)=p(C_1|x)-p(C_2|x)\]</span> <span class="math display">\[y(x)=\log \frac{p(x|C_1)}{p(x|C_2)}+\log \frac{p(C_1)}{p(C_2)}\]</span></p>
<h2 id="线性判别函数linear-discriminant-functions">线性判别函数（Linear Discriminant Functions）</h2>
<h3 id="二分类">二分类</h3>
<p>在线性判别器中，决策面是（超）平面，其线性决策方程为： <span class="math display">\[y(x)=w^Tx+w_0\]</span> 此处<span class="math inline">\(w\)</span>是法向量，<span class="math inline">\(w_0\)</span>是偏置。 假设数据维度是2的话，其图示为： <img src="https://img-blog.csdnimg.cn/20210717033655649.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 上图那些写得复复杂杂的东西就是在计算距离。稍微写得清楚一点的话就是，假设现有一点<span class="math inline">\(x_0\)</span>，其到分离（超）平面的距离为：<span class="math inline">\(r=\frac{|w*x_0+b|}{||w||}\)</span>，其中<span class="math inline">\(||w||\)</span>为2范数。</p>
<h3 id="多分类">多分类</h3>
<p>对于多分类，如果我们只是简单地运用二分类地决策规则，则可能会产生争议数据（区间），比如说下面两种情况： <img src="https://img-blog.csdnimg.cn/20210717034936687.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 运用判别函数则可以知道每个数据属于某一标签的”可信度“，毕竟”距离“有长有短容易比较。其实跟上面的二分类类似，只是这里比较的东西多一点。如下图： <img src="https://img-blog.csdnimg.cn/20210717035308951.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 如果判别函数是线性的，则决策区域是互相连接且是凸的。</p>
<h1 id="fisher-判别分析fisher-discriminant-analysis">Fisher 判别分析（Fisher Discriminant Analysis）</h1>
<p>Fisher线性判别分析是线性判别分析（LDA模型）的一种，它的目的大概就是：给定一个投影向量<span class="math inline">\(w\)</span>，将数据<span class="math inline">\(x\)</span>投影到<span class="math inline">\(w\)</span>向量上，使得不同类别的<span class="math inline">\(x\)</span>投影后的值<span class="math inline">\(y\)</span>尽可能互相分开。</p>
<h2 id="一个例子用作引出">一个例子用作引出</h2>
<p>在讲解这个判别分析前先来看个例子： 以二维数据为例，也就是上面提过的： <img src="https://img-blog.csdnimg.cn/20210717040640731.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 其数据形式以及分类标准如下：<span class="math display">\[y(x)=+1 \leftrightarrow x\in C_1\]</span> <span class="math display">\[y(x)=-1 \leftrightarrow x\in C_2\]</span> 训练输入：<span class="math inline">\(X=\left\{x_1 \in \mathcal{R}^d,...,x_n\right\}\)</span><br />
训练标签：<span class="math inline">\(Y=\left\{y_1\in[-1,+1],...,y_n\right\}\)</span><br />
<strong>最小二乘法</strong><br />
我们先尝试用最小二乘法。<br />
先写出线性判别函数：<span class="math display">\[x_i^Tw+w_0=y_i,\forall i=1,...,n\]</span>每个训练数据点及标签都有一个线性方程。<br />
把偏置<span class="math inline">\(w_0\)</span>整合进权重<span class="math inline">\(w\)</span>里比较好看，训练数据点也要因此多一个维度，即是：<span class="math display">\[\hat{x}_i=(x_i \quad1)^T\in\mathcal{R}^{d\times1}\]</span> <span class="math display">\[\hat{w}=(w \quad w_0)^T\in\mathcal{R}^{d\times1}\]</span>所以方程改成：<span class="math display">\[\hat{x}_i^T\hat{w}=y_i,\quad \forall i=1,...,n\]</span>表示成矩阵向量形式则如下：<span class="math display">\[\hat{X}^T\hat{w}=y\]</span>此处<span class="math inline">\(\hat{X}=[\hat{x}_1,...,\hat{x}_n]\in \mathcal{R}^{d\times n},\quad y=[y_1,...,y_n]^T\)</span><br />
这里有<span class="math inline">\(n\)</span>个方程和<span class="math inline">\(d+1\)</span>个未知数，属于超定方程（因为一般来说<span class="math inline">\(n\)</span>总是大于<span class="math inline">\(d+1\)</span>的）。<br />
用最小二乘法求解如下： <img src="https://img-blog.csdnimg.cn/2021071704255380.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 这部分和上一篇的线性回归里的是一样的。 用最小二乘法有个问题，就是这种方法对于异常值特别敏感，下面两幅图就很直观了： <img src="https://img-blog.csdnimg.cn/20210717042818947.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /></p>
<h2 id="正式开始">正式开始</h2>
<p>对数据进行投影，这有一幅形象的图： <img src="https://img-blog.csdnimg.cn/20210717043324300.png#pic_center" alt="在这里插入图片描述" /> 这里大抵还是跟前面类似，不过换个名字比较好说明。<br />
投影（Projection）：<span class="math inline">\(y=w^Tx\)</span><br />
对照阈值进行分析：<span class="math inline">\(w^Tx\ge -w_0\)</span> 或<span class="math inline">\(w^Tx+w_0\ge0\)</span><br />
怎样的投影<span class="math inline">\(w\)</span>才是一个好的投影呢？可以最大限度地扩大两类之间的 "距离"，以实现良好的分离。那怎么做？</p>
<h3 id="尝试最大化两个类的均值效果不好">尝试最大化两个类的均值（效果不好）</h3>
<p>首先我们可以试试两个类的均值，让其距离最大： <img src="https://img-blog.csdnimg.cn/2021071704401127.png#pic_center" alt="在这里插入图片描述" /> 将均值投影到一维线上，如下： <img src="https://img-blog.csdnimg.cn/20210717044207217.png#pic_center" alt="在这里插入图片描述" /> 然后最大化两均值距离的平方：<span class="math display">\[\max(m_1-m_2)^2\]</span>即是： <img src="https://img-blog.csdnimg.cn/20210717044425121.png#pic_center" alt="在这里插入图片描述" /> 这里据说有个明显的问题，这个距离会随着<span class="math inline">\(w\)</span>的norm的增加而无限增加，为了限制这种情况，我们加个约束条件，就有了如下式子： <img src="https://img-blog.csdnimg.cn/20210717045331674.png#pic_center" alt="在这里插入图片描述" /> 这是一个带约束条件的优化问题，第一想法肯定是用拉格朗日了，加入拉格朗日算子，各种计算，就有下面的过程： <img src="https://img-blog.csdnimg.cn/20210717045515526.png#pic_center" alt="在这里插入图片描述" /> 解出：<span class="math display">\[w=\frac{m_1-m_2}{||m_1-m_2||}\]</span> 我们能得出下图的结果： <img src="https://img-blog.csdnimg.cn/20210717045650708.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 这又有个问题，由图可以看出，两个类有很大一部分重合了。下面这个图才是我们想要的： <img src="https://img-blog.csdnimg.cn/20210717045819935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 但要怎么做？ 一波自问自答：在尽可能远得分离两均值得同时，也要让每个类得方差尽量小。</p>
<h3 id="同时考虑均值和方差正式引入fisher线性判别法">同时考虑均值和方差（正式引入Fisher线性判别法）</h3>
<p>定义每个类的方差： <img src="https://img-blog.csdnimg.cn/20210717050730975.png#pic_center" alt="在这里插入图片描述" /> 然后是Fisher标准： <img src="https://img-blog.csdnimg.cn/20210717050818222.png#pic_center" alt="在这里插入图片描述" /> 将分子展开： <img src="https://img-blog.csdnimg.cn/20210717050930288.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 这就引入了类别间的协方差。 然后再将分母展开： <img src="https://img-blog.csdnimg.cn/2021071705115256.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 综合以上两个展开，Fisher标准可以写成： <img src="https://img-blog.csdnimg.cn/20210717051310157.png#pic_center" alt="在这里插入图片描述" /> 对<span class="math inline">\(w\)</span>进行微分并等于0则有：<span class="math display">\[(w^TS_Bw)S_Ww=(w^TS_Ww)S_Bw\]</span> 在这里<span class="math inline">\((w^TS_Bw)\)</span>和<span class="math inline">\((w^TS_Ww)\)</span>都是标量，所以有：<span class="math display">\[S_Ww ||S_Bw\]</span>这里<span class="math inline">\(||\)</span>表共线（collinearity）。 然后由上面的式子我们可以知道：<span class="math display">\[S_Bw=(m_1-m_2)(m_1-m_2)^Tw\rightarrow S_Bw||(m_1-m_2)\]</span>(还是因为标量的关系所以共线)。 同理可得：<span class="math display">\[S_Ww\quad ||\quad(m_1-m_2)\]</span> <span class="math display">\[w\quad || \quad S_W^{-1}(m_1-m_2)\]</span> 所以就引出了Fisher 线性判别（Fisher’s Linear Discriminant）：<span class="math display">\[w\propto S_W^{-1}(m_1-m_2)\]</span> 这个线性判别只提供了投影，我们还需要找到阈值，方法有不少，比如贝叶斯分类器和高斯分类条件。<br />
当类条件分布相等，且协方差是对角矩阵，则Fisher线性判别是贝叶斯最优的。<br />
值得注意的是，Fisher线性判别法是最小二乘法的一个特例。另外，这种方法对于噪声依旧很敏感</p>
<h1 id="感知器算法perceptron-algorithm">感知器算法（Perceptron Algorithm）</h1>
<p>感知器算法是一种可以直接得到线性判别函数的线性分类方法，是基于样本线性可分的要求下使用的。<br />
如果我们的类别线性可分，则我们肯定可以找到一个分离（超）平面。<br />
<strong>感受器判别函数（Perceptron discriminant function）</strong>为：<span class="math display">\[y(x)=sign(w^Tx+b)\]</span>此处的<span class="math inline">\(sign(x)=\left\{+1,x&gt;0;0,x=0;-1,x&lt;0\right\}\)</span><br />
即是下图的函数： <img src="https://img-blog.csdnimg.cn/20210717065852632.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 感知器算法的基本步骤如下： <img src="https://img-blog.csdnimg.cn/20210717070135533.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 现在就出现了一个优化问题了，这里最直接的想法是计算误分类的点总数，但这种方法不好微分，就用另一种，计算点到分离（超）平面的距离，就引出了下面这个优化函数： <img src="https://img-blog.csdnimg.cn/20210717170805416.png#pic_center" alt="在这里插入图片描述" /> 在很多地方更常见的是计算最小值，但都一样，只是他们事先加了个负号，这里没加。稍微解释一下，对于误分类的点，比如点<span class="math inline">\(x_i\)</span>，不管是把这个点误分到了哪一类，<span class="math inline">\(y_i*(wx_i+b)\)</span>都是小于0的，毕竟如果大于0就意味着同号即是分类正确了。<br />
把下面那个式子写完全的话应该是这样子：<span class="math display">\[\sum_{x\in X:\left\langle w,x \right\rangle &lt;0}\left\langle w,x \right\rangle =\sum_{...}\frac{1}{||w||}y_i*(wx_i+b)\]</span> 用gradient的方法求解如下：<span class="math display">\[\frac{\partial J}{\partial w}=\sum_{...}x\]</span></p>
<blockquote>
<p>注意，经典的感知器只能做线性二分类。</p>
</blockquote>
<h1 id="逻辑回归logistic-regression">逻辑回归（Logistic Regression）</h1>
<p>（这部分接触过很多，就简单记了）</p>
<h2 id="generative-vs.-discriminative">Generative vs. Discriminative</h2>
<p>对解决分类问题一般有两种方向，Generative 和 Discriminative，这里对两种方法做个大概的对比：</p>
<blockquote>
<p>Generative modelling: 我们先构造条件分布模型<span class="math inline">\(p(x|C_2)\)</span>和<span class="math inline">\(p(x|C_1)\)</span> 运用贝叶斯规则计算其后验分布 例子：Navie Bayes</p>
</blockquote>
<blockquote>
<p>Discriminative modelling: 我们直接构造类的后验比如说<span class="math inline">\(p(C_1|x)\)</span> 我们只关心分类是否正确，而不关心是否符合类的条件。 例子：逻辑回归</p>
</blockquote>
<h2 id="概率判别模型probabilistic-discriminative-models">概率判别模型（Probabilistic Discriminative Models）</h2>
<h3 id="sigmoid函数的引出">Sigmoid函数的引出</h3>
<p>运用贝叶斯定理： <img src="https://img-blog.csdnimg.cn/20210717180508708.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 其图像如下，呈<span class="math inline">\(S\)</span>型，将传入的实数压在<span class="math inline">\([0,1]\)</span>区间内。 <img src="https://img-blog.csdnimg.cn/20210717180723404.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 假设里面这个<span class="math inline">\(\alpha\)</span>是线性的话（当然也可以是其它），后验就可以写为：<span class="math display">\[p(C_1|x)=\sigma(w^Tx+w_0)\]</span>然后就是求权重和偏置了。 求这两个东西的方法也有不少，印象中梯度下降应该是比较常见的，因为太常见了，所以这里不记录，而是记录最大似然： <img src="https://img-blog.csdnimg.cn/20210717182341543.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /></p>
<h1 id="附作业相关代码">（附）作业相关代码</h1>
<p><img src="https://img-blog.csdnimg.cn/2021071718244386.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_split_data</span>(<span class="params">path</span>):</span></span><br><span class="line">    data = np.loadtxt(path)</span><br><span class="line">    C_1 = data[:<span class="number">50</span>]</span><br><span class="line">    C_2 = data[<span class="number">50</span>:<span class="number">93</span>]</span><br><span class="line">    C_3 = data[<span class="number">93</span>:]</span><br><span class="line">    <span class="keyword">return</span> C_1, C_2, C_3, data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_discriminant_analysis</span>(<span class="params">c_1, c_2</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    params:</span></span><br><span class="line"><span class="string">        c_1(ndarray): (n, 2)</span></span><br><span class="line"><span class="string">        c_2(ndarray): (n, 2)</span></span><br><span class="line"><span class="string">    return:</span></span><br><span class="line"><span class="string">        w(ndarray): (2,1)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    n1, d1 = c_1.shape</span><br><span class="line">    n2, d2 = c_2.shape</span><br><span class="line">    m1 = np.mean(c_1, axis=<span class="number">0</span>)<span class="comment">#(1,2)</span></span><br><span class="line">    m2 = np.mean(c_2, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    sw_1 = np.zeros((d1,d1))</span><br><span class="line">    sw_2 = np.zeros((d2,d2))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n1):</span><br><span class="line">        temp_sw = (c_1[[i]] - m1).T @ (c_1[[i]] - m1)</span><br><span class="line">        sw_1 += temp_sw</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n2):</span><br><span class="line">        temp_sw = (c_2[[j]] - m2).T @ (c_2[[j]] - m2)</span><br><span class="line">        sw_2 += temp_sw</span><br><span class="line">    sw = sw_1 + sw_2</span><br><span class="line">    inv_sw = np.linalg.inv(sw)</span><br><span class="line">    w = inv_sw @ (m1 - m2).T<span class="comment">#(2,1)</span></span><br><span class="line">    <span class="keyword">return</span> w</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decide</span>(<span class="params">w, x, m1, m2, N1, N2</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    params:</span></span><br><span class="line"><span class="string">        w(ndarray): weight, (2,1)</span></span><br><span class="line"><span class="string">        x(ndarray): data, (1,2)</span></span><br><span class="line"><span class="string">        m1(ndarray): mean of class1, (1,2)</span></span><br><span class="line"><span class="string">        m2(ndarray): mean of class2, (1,2)</span></span><br><span class="line"><span class="string">        N1(int): nums of class1</span></span><br><span class="line"><span class="string">        N2(int): nums of class2</span></span><br><span class="line"><span class="string">    return:</span></span><br><span class="line"><span class="string">        True: c1</span></span><br><span class="line"><span class="string">        False: c2</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    y = w.T @ x.T</span><br><span class="line">    <span class="comment">#w0 = (N1 * (w.T @ m1.T) + N2 * (w.T @ m2.T)) / (N1 + N2)</span></span><br><span class="line">    w0 = (w.T @ m1.T + w.T @ m2.T) / <span class="number">2</span> </span><br><span class="line">    <span class="keyword">if</span> y &gt; w0:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    path = <span class="string">&quot;./dataSets/ldaData.txt&quot;</span></span><br><span class="line">    C_1, C_2, C_3, data = load_split_data(path)</span><br><span class="line"></span><br><span class="line">    w_12 = linear_discriminant_analysis(C_1, C_2)</span><br><span class="line">    w_23 = linear_discriminant_analysis(C_2, C_3)</span><br><span class="line">    w_13 = linear_discriminant_analysis(C_1, C_3)</span><br><span class="line"></span><br><span class="line">    c1 = []</span><br><span class="line">    c2 = []</span><br><span class="line">    c3 = []</span><br><span class="line"></span><br><span class="line">    N1 = <span class="built_in">len</span>(C_1)</span><br><span class="line">    N2 = <span class="built_in">len</span>(C_2)</span><br><span class="line">    N3 = <span class="built_in">len</span>(C_3)</span><br><span class="line"></span><br><span class="line">    m1 = np.mean(C_1, axis=<span class="number">0</span>)</span><br><span class="line">    m2 = np.mean(C_2, axis=<span class="number">0</span>)</span><br><span class="line">    m3 = np.mean(C_3, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    N, d = data.shape</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">        <span class="keyword">if</span> decide(w_12, data[i], m1, m2, N1, N2):</span><br><span class="line">            <span class="comment"># not c2</span></span><br><span class="line">            <span class="keyword">if</span> decide(w_13, data[i], m1, m3, N1, N3):</span><br><span class="line">                <span class="comment"># c1</span></span><br><span class="line">                c1.append(data[i])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># c3</span></span><br><span class="line">                c3.append(data[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># not c1</span></span><br><span class="line">            <span class="keyword">if</span> decide(w_23, data[i], m2, m3, N2, N3):</span><br><span class="line">                <span class="comment"># c2</span></span><br><span class="line">                c2.append(data[i])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># c3</span></span><br><span class="line">                c3.append(data[i])</span><br><span class="line"></span><br><span class="line">    <span class="comment">#plot orignal_data</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(C_1)):</span><br><span class="line">        plt.scatter(C_1[i, <span class="number">0</span>], C_1[i, <span class="number">1</span>], marker = <span class="string">&quot;v&quot;</span>, color = <span class="string">&quot;r&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(C_2)):</span><br><span class="line">        plt.scatter(C_2[i, <span class="number">0</span>], C_2[i, <span class="number">1</span>], marker = <span class="string">&quot;x&quot;</span>, color = <span class="string">&quot;y&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(C_3)):</span><br><span class="line">        plt.scatter(C_3[i, <span class="number">0</span>], C_3[i, <span class="number">1</span>], marker = <span class="string">&quot;o&quot;</span>, color = <span class="string">&quot;b&quot;</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;orignal_data&quot;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">#plot LDA</span></span><br><span class="line">    <span class="keyword">for</span> sub_data <span class="keyword">in</span> c1:</span><br><span class="line">        plt.scatter(sub_data[<span class="number">0</span>], sub_data[<span class="number">1</span>], marker = <span class="string">&quot;v&quot;</span>, color = <span class="string">&quot;r&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> sub_data <span class="keyword">in</span> c2:</span><br><span class="line">        plt.scatter(sub_data[<span class="number">0</span>], sub_data[<span class="number">1</span>], marker = <span class="string">&quot;x&quot;</span>, color = <span class="string">&quot;y&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> sub_data <span class="keyword">in</span> c3:</span><br><span class="line">        plt.scatter(sub_data[<span class="number">0</span>], sub_data[<span class="number">1</span>], marker = <span class="string">&quot;o&quot;</span>, color = <span class="string">&quot;b&quot;</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;LDA&quot;</span>)</span><br><span class="line">    plt.show()    </span><br><span class="line">            </span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure></p>
<p>效果如下： <img src="https://img-blog.csdnimg.cn/20210717182742175.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> <img src="https://img-blog.csdnimg.cn/20210717182739618.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /></p>
]]></content>
      <categories>
        <category>学习总结</category>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>线性回归（Linear Regression）</title>
    <url>/posts/9.html</url>
    <content><![CDATA[<h1 id="引入">引入</h1>
<p>回归问题和分类问题以前老是弄混，最直接的记法就是，分类问题是针对离散空间的，就比如说给定一张图片，根据其特征判断这张图片是猫还是狗，这就是分类问题；回归问题的话是针对连续空间的，比如预测距离，概率等。 <span id="more"></span></p>
<h1 id="最小二乘法线性回归least-squares-linear-regression">最小二乘法线性回归（Least Squares Linear Regression）</h1>
<h2 id="一次项回归">一次项回归</h2>
<p>给定一组训练数据以及相关的函数值<span class="math inline">\((x_i, y_i)\)</span>（这里的<span class="math inline">\(x_i\)</span>是一组向量）如下：<span class="math display">\[X=\left\{x_1 \in R^d,...,x_n\right\}\]</span><span class="math display">\[Y=\left\{y_1 \in R,...,y_n\right\}\]</span>线性回归的方程如下：<span class="math display">\[x_i^Tw+w_0=y_i\   \forall i =1,...n \]</span>这个等式与最小二乘法分类的等式一样，唯一的区别就是值是连续的。<br />
<span class="math inline">\(w_0\)</span>在这里是一个偏置，即是常见的<span class="math inline">\(bias\)</span>，一般我们会把它整合到权重里，既方便也好算。<br />
最小二乘法线性回归的一般性步骤如下：</p>
<p><strong>Step 1</strong>：Define： <span class="math display">\[\hat{x}_i=\begin{bmatrix} x_i\\ 1\end{bmatrix}\]</span><span class="math display">\[\hat{w}=\begin{bmatrix} w\\ w_0\end{bmatrix}\]</span> <strong>Step 2</strong>: Rewrite: <span class="math display">\[\hat{x}_i^T\hat{w}=y_i\ \forall i=1,...,n\]</span> <strong>Step 3</strong>: Matrix-vector notation <span class="math display">\[\hat{X}^T\hat{w}=y\]</span>这里<span class="math inline">\(\hat{X}=[\hat{x}_1,...,\hat{x}_n]\)</span>，每个<span class="math inline">\(\hat{x}_i\)</span>都是一个向量，且<span class="math inline">\(y=[y_1,...,y_n]^T\)</span>。<br />
<strong>Step 4</strong>: Fine the least squares solution <img src="https://img-blog.csdnimg.cn/20210716080914567.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 这里就直接解出权重<span class="math inline">\(w\)</span>了，在这一系列的计算里，计算量最大的就是那个<span class="math inline">\(R^{D\times D}\)</span>的逆矩阵，直接计算逆矩阵的话，其复杂度是<span class="math inline">\(O(D^3)\)</span>，所以当<span class="math inline">\(D\)</span>很大时计算成本就很高了，所以一般我们会用下面两种方法：</p>
<blockquote>
<ol type="1">
<li>Gradient descent<br />
</li>
<li>Work with fewer dimensions</li>
</ol>
</blockquote>
<p>这两种方法应该都挺熟悉了，所以这里不做介绍。</p>
<h2 id="多项式回归polynomial-regression">多项式回归（Polynomial Regression）</h2>
<p>这是建立在最小二乘回归上的，等式形式跟上面的类似：<span class="math display">\[y(x)=w^T\phi(x)=\sum_{i=0}^{M}w_i\phi_i(x)\]</span> 并且<span class="math inline">\(\phi_0(x)=1\)</span>，<span class="math inline">\(\phi_i(.)\)</span>叫基础方程（basis functions），对于<span class="math inline">\(w\)</span>来说这依旧是个线性方程。<br />
此处的<span class="math inline">\(\phi(x)\)</span>代表多项式，比如说：<span class="math display">\[\phi(x)=(1,x,x^2,x^3)^T\]</span> 举个例子，对于不同degree的多项式，其拟合效果如下： <img src="https://img-blog.csdnimg.cn/20210716084632249.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 对于不同的多项式会出现欠拟合或者过拟合的现象。</p>
<h1 id="回归的最大似然法maximum-likelihood-approach-to-regression">回归的最大似然法（Maximum Likelihood Approach to Regression）</h1>
<h2 id="概率回归probabilistic-regression">概率回归（Probabilistic Regression）</h2>
<p>对于概率回归有以下两个假设：</p>
<p><strong>Assumption 1</strong>：我们的目标函数值是通过向方程添加噪声产生的，即是：<span class="math display">\[y = f(x,w)+\epsilon\]</span> <span class="math inline">\(y\)</span>：目标函数值； <span class="math inline">\(x\)</span>：输入值<br />
<span class="math inline">\(f\)</span>： 回归方程； <span class="math inline">\(w\)</span>：权重或者说参数；<span class="math inline">\(\epsilon\)</span>：噪声<br />
<strong>Assumption 2</strong>：这个噪声是服从高斯分布的随机变量，即：<span class="math display">\[\epsilon \sim N(0,\beta^{-1})\]</span><span class="math display">\[p(y|x,w,\beta)=N(y|f(x,w),\beta^{-1})\]</span>其中<span class="math inline">\(f(x,w)\)</span>是均值，<span class="math inline">\(\beta^{-1}\)</span>是方差（<span class="math inline">\(\beta\)</span>是precision），注意，现在<span class="math inline">\(y\)</span>是一个随机变量，其基本概率分布为<span class="math inline">\(p(y|x,w,\beta)\)</span>。</p>
<p><strong>具体说明</strong><br />
现在给定一组数据点<span class="math inline">\(X=[x_1,...,x_n]\in \mathcal{R}^{d\times n}\)</span>以及相关的函数值<span class="math inline">\(Y=[y_1,...,y_n]^T\)</span><br />
其条件似然为（设数据是i.i.d.的）：<span class="math display">\[p(y|X,w,\beta)=\prod_{i=1}^n N(y_i|f(x_i,w),\beta^{-1})\]</span>带入线性模型：<span class="math display">\[=\prod_{i=1}^n N(y_i|w^T\phi(x_i),\beta^{-1})\]</span>其中<span class="math inline">\(w^T\phi(x_i)\)</span>是广义的线性回归方程，然后现在我们就要计算关于<span class="math inline">\(\beta\)</span>和<span class="math inline">\(w\)</span>的最大化似然了。<br />
用对数似然如下： <img src="https://img-blog.csdnimg.cn/20210716094220653.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 对于<span class="math inline">\(w\)</span>的梯度为： <img src="https://img-blog.csdnimg.cn/20210716094315769.png#pic_center" alt="在这里插入图片描述" /> 我们做如下定义： <img src="https://img-blog.csdnimg.cn/20210716094531974.png#pic_center" alt="在这里插入图片描述" /> 求解上面的式子可以得到： <img src="https://img-blog.csdnimg.cn/20210716202619384.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 由此可见，用最大似然求解的权重值和前面最小二乘法回归的结果一样，我们也可得出这么一个结论：</p>
<blockquote>
<p><strong>最小二乘法等同于假设目标是高斯分布。</strong></p>
</blockquote>
<p>所以要注意，运用最小二乘法是有假设前提的。<br />
然而，与最小二乘法相比，最大似然的运用更广法，因为除了权重<span class="math inline">\(w\)</span>，我们还可以用这个方法估计<span class="math inline">\(\beta\)</span>： <img src="https://img-blog.csdnimg.cn/20210716203235914.png#pic_center" alt="在这里插入图片描述" /> 而且可以衡量我们的估计的不确定性（用loss function）。</p>
<h2 id="回归中的损失函数loss-functions-in-regression">回归中的损失函数（Loss Functions in Regression）</h2>
<p>如果我们现在有一个新的数据<span class="math inline">\(x_t\)</span>，在最小二乘回归中，其对应的值为：<span class="math inline">\(y_t=\hat{x}_t^T\hat{w}\)</span><br />
但在最大似然回归中，我们则是要计算相关概率值：<span class="math inline">\(p(y|x,w,\beta)\)</span>。我们如何准确地估计<span class="math inline">\(y_t\)</span>呢？这就要引入损失函数（loss function）了。 <span class="math display">\[L: \mathcal{R}\times \mathcal{R} \rightarrow \mathcal{R}^+\]</span><span class="math display">\[(y_t,f(x_t))\rightarrow L(y_t,f(x_t))\]</span> 然后我们最小化<strong>预期</strong>损失：<span class="math display">\[E_{x,y\sim p(x,y)}[L]=\int \int L(y,f(x))p(x,y)dxdy\]</span> 举个例子，比如说损失函数是squared loss的话，可进行如下计算： <img src="https://img-blog.csdnimg.cn/20210716204651957.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> <img src="https://img-blog.csdnimg.cn/20210716205115573.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 由此可见，对于squared loss，其最佳回归方程为后验<span class="math inline">\(p(y|x)\)</span>的均值<span class="math inline">\(E[y|x]\)</span>，这也称为均值预测。<br />
所以对于我们的广泛线性回归方程，可写为： <span class="math display">\[f(x)=\int yN(y|w^T\phi(x),\beta^{-1})dy=w^T\phi(x)\]</span>这又可以跟前面 产生联系了。</p>
<h1 id="贝叶斯线性回归bayesian-linear-regression">贝叶斯线性回归（Bayesian Linear Regression）</h1>
<p>现在回到最原来的问题，我们想要避免过拟合和不稳定，然而用最大似然法依旧可能造成过拟合现象（比如说只有一个数据点的情况）。为了解决这个问题，现在就要引入贝叶斯线性回归。<br />
我们在参数<span class="math inline">\(w\)</span>上放置一个先验，以控制不稳定性：<span class="math display">\[p(w|X,y)= p(y|X,w)p(w)\]</span>其中：</p>
<p><span class="math inline">\(p(w)\)</span>为参数先验；</p>
<p><span class="math inline">\(p(y|X,w)\)</span>为给定数据和参数下目标的似然（跟前面定义的一样）；</p>
<p><span class="math inline">\(p(w|X,y)\)</span>为参数后验。</p>
<blockquote>
<p><strong>注：这里得到的不再是参数的单一值，而是参数的概率分布</strong></p>
</blockquote>
<p>最简单的想法就是假设参数<span class="math inline">\(w\)</span>的先验符合高斯分布：<span class="math display">\[w\sim p(w|\alpha)=N(w|0,\alpha^{-1}I)\]</span>这里通过放置一个”软“限制，也就是<span class="math inline">\(\alpha\)</span>，来避免不稳定性。另外，这里设均值为0只是为了方便后面计算，均值可为其它数。<br />
然后就有如下关系： <img src="https://img-blog.csdnimg.cn/2021071621150373.png#pic_center" alt="在这里插入图片描述" /> 求解上面的参数概率可有多种方法，这里介绍两种：</p>
<h2 id="最大后验map">最大后验（MAP）</h2>
<p>取对数后验，然后最大化，如下： <img src="https://img-blog.csdnimg.cn/20210716212004115.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 然后对<span class="math inline">\(w\)</span>求导： <img src="https://img-blog.csdnimg.cn/20210716212611934.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> （注：上面这个图里的式子有错，右边<span class="math inline">\(\beta\)</span>应该在括号里，但我实在不想手打这些式子，只是计算过程而已。。。） 这就得出参数的等式了： <span class="math display">\[w_{map}=(\Phi \Phi^T+\frac{\alpha}{\beta})^{-1}\Phi y\]</span> 与前面的结论不同，这里多了个<span class="math inline">\(\frac{\alpha}{\beta}\)</span>，先验的作用是可以正则化这个伪逆，这个也叫岭回归（ridge regression）。</p>
<blockquote>
<p>既然说到了这个概念，就顺道提一下另一个跟它很像的<span class="math inline">\(LASSO\)</span> 回归，Lasso回归跟岭回归非常相似，它们的差别在于使用了不同的正则化项（LASSO是<span class="math inline">\(l1\)</span>正则化，岭回归是<span class="math inline">\(l2\)</span>）。最终都实现了约束参数从而防止过拟合的效果。另外，Lasso能够将一些作用比较小的特征的参数训练为0，从而获得稀疏解。也就是说用这种方法，在训练模型的过程中实现了降维(特征筛选)的目的。</p>
</blockquote>
<p>下面这张图可以直观显示岭回归和最小二乘回归的差别： <img src="https://img-blog.csdnimg.cn/20210716213954811.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /></p>
<h3 id="map与正则化的最小二乘法的比较">MAP与正则化的最小二乘法的比较</h3>
<p>现在我们在前面的最小二乘法的结论等式上加一个正则化项：<img src="https://img-blog.csdnimg.cn/20210716214450502.png#pic_center" alt="在这里插入图片描述" /> 求解<span class="math inline">\(w\)</span>我们得到一个新的估计：<span class="math display">\[\hat{w}=(\hat{X}\hat{X}^T+\lambda I)^{-1}\hat{X}y\]</span> 此处 <span class="math inline">\(\lambda = \alpha / \beta\)</span>。<br />
也就是说，如果我们在最小二乘回归上添加一个正则化项<span class="math inline">\(\lambda\)</span>，则意味着我们假设目标有一个符合高斯分布的噪音，而且参数也是符合高斯分布的。<br />
以前面的9次多项式回归为例，加上不同的正则化项<span class="math inline">\(\lambda\)</span>后，其效果为： <img src="https://img-blog.csdnimg.cn/20210716215303870.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 由此可见，<span class="math inline">\(\lambda=\alpha / \beta\)</span>控制模型的复杂程度，并决定过拟合的程度。</p>
<h2 id="完全贝叶斯回归full-bayesian-regression">完全贝叶斯回归（Full Bayesian Regression）</h2>
<p>换个思路，其实我们也不是非得求出参数<span class="math inline">\(w\)</span>，我们只需要通过训练数据来预测对应的函数值即可。也就是说可用边缘概率进行计算：<span class="math display">\[$p(y_t|x_t,X,y)=\int p(y_t,w|x_t,X,y)dw\]</span> 用贝叶斯公式可将上面等式写为： <img src="https://img-blog.csdnimg.cn/20210716222918384.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 若先验之类的都是高斯的话，则这个预测分布也将服从高斯分布，即： <img src="https://img-blog.csdnimg.cn/20210716231602167.png#pic_center" alt="在这里插入图片描述" /> （注：上图的等式有一个错误，中间那个等式最右边的<span class="math inline">\(\Phi\)</span>没有转置。）</p>
<h1 id="附作业相关代码">（附）作业相关代码</h1>
<p><img src="https://img-blog.csdnimg.cn/20210716231922529.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> <img src="https://img-blog.csdnimg.cn/20210716232000630.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> <img src="https://img-blog.csdnimg.cn/20210716232025659.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 大概就是按照这篇博文全部实现一遍：（这次的代码大的方面应该是没问题的，但一些细节上可能会有问题，改得有点心累，先这样吧）<br />
(a): <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_features</span>(<span class="params">X_train, X_test, y_train, y_test, _lambda = <span class="number">0.01</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    param:</span></span><br><span class="line"><span class="string">        X_train(ndarray): shape = (N, 1)</span></span><br><span class="line"><span class="string">        y_train(ndarray): shape = (N, 1)</span></span><br><span class="line"><span class="string">        _lambda(float)</span></span><br><span class="line"><span class="string">    return:</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_dim_bias</span>(<span class="params">X</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        [x1, x2, x3, ... , xn] -&gt; [[x1,1], [x2,1], ... ,[xn,1]]</span></span><br><span class="line"><span class="string">        param:</span></span><br><span class="line"><span class="string">            X(ndarray): train_data (N_train, 1)</span></span><br><span class="line"><span class="string">        return:</span></span><br><span class="line"><span class="string">            new_X(ndarray): train_data (2, N_train)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        N_x = X.shape[<span class="number">0</span>]</span><br><span class="line">        bias_dim = np.ones((N_x, <span class="number">1</span>))</span><br><span class="line">        new_X = np.c_[X, bias_dim].T</span><br><span class="line">        <span class="keyword">return</span> new_X</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss_fn</span>(<span class="params">X, y, N</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        calculate RMSE</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        rmse = np.sqrt(np.<span class="built_in">sum</span>((X.T @ w - y) ** <span class="number">2</span>) / N)</span><br><span class="line">        <span class="keyword">return</span> rmse</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">    X_train = add_dim_bias(X_train)</span><br><span class="line">    X_test = add_dim_bias(X_test)</span><br><span class="line">    N_train = X_train.shape[<span class="number">1</span>]</span><br><span class="line">    N_test = X_test.shape[<span class="number">1</span>]</span><br><span class="line">    w = np.linalg.inv(X_train @ X_train.T + _<span class="keyword">lambda</span> * np.eye(X_train.shape[<span class="number">0</span>])) @ X_train @ y_train <span class="comment"># w(2, 1)</span></span><br><span class="line">    rmse_train = loss_fn(X_train, y_train, N_train)</span><br><span class="line">    rmse_test = loss_fn(X_test, y_test, N_test)</span><br><span class="line"></span><br><span class="line">    plt.figure()</span><br><span class="line">    x = np.linspace(X_train[<span class="number">0</span>,:].<span class="built_in">min</span>(), X_train[<span class="number">0</span>,:].<span class="built_in">max</span>(), <span class="number">100</span>)</span><br><span class="line">    x = add_dim_bias(x)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(X_train.shape[<span class="number">1</span>]):</span><br><span class="line">        plt.scatter(X_train[<span class="number">0</span>,i], y_train[i, <span class="number">0</span>], marker = <span class="string">&quot;o&quot;</span>, color = <span class="string">&quot;black&quot;</span>)</span><br><span class="line">    plt.plot(x[[<span class="number">0</span>]].flatten(), (w.T @ x).flatten(), color = <span class="string">&quot;blue&quot;</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;linear_features&quot;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> rmse_train, rmse_test</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main_a</span>():</span></span><br><span class="line">    rmse_train, rmse_test = linear_features(X_train, X_test, y_train, y_test, _<span class="keyword">lambda</span> = <span class="number">0.01</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;root mean squared error of the training data: &quot;</span>, rmse_train)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;root mean squared error of the test data: &quot;</span>,rmse_test)</span><br></pre></td></tr></table></figure> (b): <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">polynomial_features</span>(<span class="params">X_train, X_test, y_train, y_test, _lambda = <span class="number">0.01</span>, degrees = [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    param:</span></span><br><span class="line"><span class="string">        X_train(ndarray): shape(N, 1)</span></span><br><span class="line"><span class="string">        y_train(ndarray): shape(N, 1)</span></span><br><span class="line"><span class="string">        _lambda(floar): ridge coefficient</span></span><br><span class="line"><span class="string">        degrees(list): list of degrees</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_degree</span>(<span class="params">X, degree</span>):</span></span><br><span class="line">        N = X.shape[<span class="number">0</span>]</span><br><span class="line">        new_X = np.ones((N, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, degree+<span class="number">1</span>):</span><br><span class="line">            temp = X ** i</span><br><span class="line">            new_X = np.c_[new_X, temp]</span><br><span class="line">        new_X = new_X.T </span><br><span class="line">        <span class="keyword">return</span> new_X</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss_fn</span>(<span class="params">w, X, y</span>):</span></span><br><span class="line">        N = <span class="built_in">len</span>(y)</span><br><span class="line">        rmse = np.sqrt(np.<span class="built_in">sum</span>((X.T @ w - y) ** <span class="number">2</span>) / N)</span><br><span class="line">        <span class="keyword">return</span> rmse</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    w_lst = []</span><br><span class="line">    rmse_train = []</span><br><span class="line">    rmse_test = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(degrees)):</span><br><span class="line">        tmp_X = add_degree(X_train, degrees[i])</span><br><span class="line">        tmp_w = np.linalg.inv(tmp_X @ tmp_X.T + _<span class="keyword">lambda</span> * np.eye(tmp_X.shape[<span class="number">0</span>])) @ tmp_X @ y_train</span><br><span class="line">        w_lst.append(tmp_w)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(degrees)):</span><br><span class="line">        tmp_X_train = add_degree(X_train, degrees[i])</span><br><span class="line">        tmp_X_test = add_degree(X_test, degrees[i])</span><br><span class="line">        tmp_rmse_train = loss_fn(w_lst[i], tmp_X_train, y_train)</span><br><span class="line">        tmp_rmse_test = loss_fn(w_lst[i], tmp_X_test, y_test)</span><br><span class="line">        rmse_train.append(tmp_rmse_train)</span><br><span class="line">        rmse_test.append(tmp_rmse_test)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(degrees)):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;polynomials of degrees=&#123;&#125;, root mean squared error of the training data is: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(degrees[i], rmse_train[i]))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(degrees)):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;polynomials of degrees=&#123;&#125;, root mean squared error of the test data is: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(degrees[i], rmse_test[i]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(w_lst)):</span><br><span class="line">        x = np.linspace(X_train[:,<span class="number">0</span>].<span class="built_in">min</span>(), X_train[:,<span class="number">0</span>].<span class="built_in">max</span>(), <span class="number">100</span>)</span><br><span class="line">        plt.figure()</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(X_train.shape[<span class="number">0</span>]):</span><br><span class="line">            plt.scatter(X_train[j,<span class="number">0</span>], y_train[j, <span class="number">0</span>], marker = <span class="string">&quot;o&quot;</span>, color = <span class="string">&quot;black&quot;</span>)</span><br><span class="line">        x = add_degree(x, degrees[i])</span><br><span class="line">        plt.plot(x[<span class="number">1</span>].flatten(), (w_lst[i].T @ x).flatten(), color = <span class="string">&quot;blue&quot;</span>)</span><br><span class="line">        plt.title(<span class="string">&quot;polynomial_features with degree = &#123;&#125;&quot;</span>.<span class="built_in">format</span>(degrees[i]))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main_b</span>():</span></span><br><span class="line">    polynomial_features(X_train, X_test, y_train, y_test)</span><br></pre></td></tr></table></figure> (c): <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bayesian_linear_regression</span>(<span class="params">X_train, y_train, X_test, y_test, mu = <span class="number">0</span>, sigma = <span class="number">0.1</span>, _lambda = <span class="number">0.01</span>, std = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span>):</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_dim_bias</span>(<span class="params">X</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        [x1, x2, x3, ... , xn] -&gt; [[x1,1], [x2,1], ... ,[xn,1]]</span></span><br><span class="line"><span class="string">        param:</span></span><br><span class="line"><span class="string">            X(ndarray): train_data (N_train, 1)</span></span><br><span class="line"><span class="string">        return:</span></span><br><span class="line"><span class="string">            new_X(ndarray): train_data (2, N_train)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        N_x = X.shape[<span class="number">0</span>]</span><br><span class="line">        bias_dim = np.ones((N_x, <span class="number">1</span>))</span><br><span class="line">        new_X = np.c_[X, bias_dim].T</span><br><span class="line">        <span class="keyword">return</span> new_X</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">prediction_mu_sigma</span>(<span class="params">X</span>):</span></span><br><span class="line">        <span class="comment">#X_bias = add_dim_bias(X)#(2,50)</span></span><br><span class="line">        mu_or_pred = X.T @ w<span class="comment">#(50,1)</span></span><br><span class="line">        sigma_2 = <span class="number">1</span> / beta + X.T @ np.linalg.inv(alpha * np.eye(X.shape[<span class="number">0</span>]) + beta *</span><br><span class="line">                                                          X @ X.T) @ X</span><br><span class="line">        <span class="comment"># Here take attention</span></span><br><span class="line">        sigma_2 += sigma * np.eye(sigma_2.shape[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">return</span> mu_or_pred, np.sqrt(sigma_2.diagonal())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss_fn</span>(<span class="params">X, y</span>):</span></span><br><span class="line">        N = <span class="built_in">len</span>(y)</span><br><span class="line">        pred, _ = prediction_mu_sigma(X)</span><br><span class="line">        rmse = np.sqrt(np.<span class="built_in">sum</span>((pred - y) ** <span class="number">2</span>) / N)</span><br><span class="line">        <span class="keyword">return</span> rmse</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">log_likelihood_fn</span>(<span class="params">X, y</span>):</span></span><br><span class="line">        n = <span class="built_in">len</span>(y)</span><br><span class="line">        log_likelihood = n / <span class="number">2</span> * (np.log(beta) - np.log(<span class="number">2</span> * np.pi)) - beta / <span class="number">2</span> * (np.linalg.norm(y - w.T @ X) ** <span class="number">2</span>)</span><br><span class="line">        average_ll = log_likelihood / n</span><br><span class="line">        <span class="keyword">return</span> average_ll</span><br><span class="line">        </span><br><span class="line">    </span><br><span class="line">    beta = <span class="number">1</span> / (sigma ** <span class="number">2</span>)</span><br><span class="line">    alpha = _<span class="keyword">lambda</span> * beta</span><br><span class="line">    X_train = add_dim_bias(X_train)<span class="comment">#(2,50)   y_train (50,1)</span></span><br><span class="line">    X_test = add_dim_bias(X_test)<span class="comment">#(2,100)   y_test  (100,1)</span></span><br><span class="line"></span><br><span class="line">    w = np.linalg.inv(X_train @ X_train.T + _<span class="keyword">lambda</span> * np.eye(X_train.shape[<span class="number">0</span>])) @ X_train @ y_train <span class="comment"># w(2, 1)</span></span><br><span class="line"></span><br><span class="line">    rmse_train = loss_fn(X_train, y_train)</span><br><span class="line">    rmse_test = loss_fn(X_test, y_test)</span><br><span class="line"></span><br><span class="line">    log_ll_train = log_likelihood_fn(X_train, y_train)</span><br><span class="line">    log_ll_test = log_likelihood_fn(X_test, y_test)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;the RMSE of the train data is: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(rmse_train))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;the RMSE of the test data is: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(rmse_test))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;the average log-likelihood of the train data is: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(log_ll_train))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;the average log-likelihood of the test data is: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(log_ll_test))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    pred, x_std = prediction_mu_sigma(X_train)</span><br><span class="line">    </span><br><span class="line">    plt.figure()</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(X_train.shape[<span class="number">1</span>]):</span><br><span class="line">        plt.scatter(X_train[<span class="number">0</span>,j], y_train[j, <span class="number">0</span>], marker = <span class="string">&quot;o&quot;</span>, color = <span class="string">&quot;black&quot;</span>)</span><br><span class="line">    plt.plot(X_train[<span class="number">0</span>], pred.flatten(), color = <span class="string">&quot;blue&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(std)):</span><br><span class="line">        plt.fill_between(X_train[<span class="number">0</span>], pred.flatten()+std[i]*x_std, pred.flatten()-std[i]*x_std,</span><br><span class="line">                          color = <span class="string">&quot;blue&quot;</span>, alpha = <span class="number">0.2</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;bayesian_linear_regression&quot;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main_c</span>():</span></span><br><span class="line">    bayesian_linear_regression(X_train, y_train, X_test, y_test)</span><br></pre></td></tr></table></figure> (d): <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">squared_exponential_features</span>(<span class="params">X_train, y_train, X_test, y_test, _lambda = <span class="number">0.01</span>, k = <span class="number">20</span>, sigma = <span class="number">0.1</span>, beta = <span class="number">10</span>, std = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">new_data</span>(<span class="params">X</span>):</span></span><br><span class="line">        n = <span class="built_in">len</span>(X)</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        new_x = np.zeros((n, k))</span></span><br><span class="line"><span class="string">        for i in range(n):</span></span><br><span class="line"><span class="string">            for j in range(k):</span></span><br><span class="line"><span class="string">                alpha_j = j * 0.1 - 1</span></span><br><span class="line"><span class="string">                tmp = np.exp(-0.5 * beta * (X[i] - alpha_j) ** 2)</span></span><br><span class="line"><span class="string">                new_x[i][j] = tmp</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        X_poly = np.ones(X.shape)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, k+<span class="number">1</span>):</span><br><span class="line">                X_poly = np.hstack((X_poly, np.exp(-beta/<span class="number">2</span>*np.power(X-(j*<span class="number">0.1</span> -<span class="number">1</span>), <span class="number">2</span>))))</span><br><span class="line">        <span class="keyword">return</span> X_poly.T</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">prediction_mu_sigma</span>(<span class="params">X</span>):</span></span><br><span class="line">        mu_or_pred = X.T @ w</span><br><span class="line">        sigma_2 = <span class="number">1</span> / beta + X.T @ np.linalg.inv(alpha * np.eye(X.shape[<span class="number">0</span>]) + beta *</span><br><span class="line">                                                          X @ X.T) @ X</span><br><span class="line">        sigma_2 += sigma * np.eye(sigma_2.shape[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">return</span> mu_or_pred, np.sqrt(sigma_2.diagonal())</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss_fn</span>(<span class="params">X, y</span>):</span></span><br><span class="line">        N = <span class="built_in">len</span>(y_test)</span><br><span class="line">        pred, _ = prediction_mu_sigma(X)</span><br><span class="line">        rmse = np.sqrt(np.<span class="built_in">sum</span>((pred - y) ** <span class="number">2</span>) / N)</span><br><span class="line">        <span class="keyword">return</span> rmse</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">log_likelihood_fn</span>(<span class="params">X, y</span>):</span></span><br><span class="line">        _beta = <span class="number">1</span> / (sigma ** <span class="number">2</span>)</span><br><span class="line">        n = <span class="built_in">len</span>(y)</span><br><span class="line">        log_likelihood = n / <span class="number">2</span> * (np.log(_beta) - np.log(<span class="number">2</span> * np.pi)) - _beta / <span class="number">2</span> * (np.linalg.norm(y - w.T @ X) ** <span class="number">2</span>)</span><br><span class="line">        average_ll = log_likelihood / n</span><br><span class="line">        <span class="keyword">return</span> average_ll    </span><br><span class="line"></span><br><span class="line">    orignal_data = X_train.copy()</span><br><span class="line">    X_train = new_data(X_train)</span><br><span class="line">    X_test = new_data(X_test)</span><br><span class="line">    alpha = _<span class="keyword">lambda</span> * beta</span><br><span class="line">    w = np.linalg.pinv(X_train @ X_train.T + _<span class="keyword">lambda</span> * np.eye(X_train.shape[<span class="number">0</span>])) @ X_train @ y_train</span><br><span class="line">    rmse_train = loss_fn(X_train, y_train)</span><br><span class="line">    rmse_test = loss_fn(X_test, y_test)</span><br><span class="line"></span><br><span class="line">    log_ll_train = log_likelihood_fn(X_train, y_train)</span><br><span class="line">    log_ll_test = log_likelihood_fn(X_test, y_test)</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;the RMSE of the train data is: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(rmse_train))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;the RMSE of the test data is: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(rmse_test))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;the average log-likelihood of the train data is: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(log_ll_train))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;the average log-likelihood of the test data is: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(log_ll_test))</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    pred, x_std = prediction_mu_sigma(X_train)</span><br><span class="line">    sorted_idx = np.argsort(X_train[<span class="number">0</span>])</span><br><span class="line">    orignal_data = orignal_data[sorted_idx]</span><br><span class="line">    pred = pred.flatten()[sorted_idx]</span><br><span class="line">    y_train = y_train[sorted_idx]</span><br><span class="line">    plt.figure()</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(orignal_data.shape[<span class="number">0</span>]):</span><br><span class="line">        plt.scatter(orignal_data[j, <span class="number">0</span>], y_train[j, <span class="number">0</span>], marker = <span class="string">&quot;o&quot;</span>, color = <span class="string">&quot;black&quot;</span>)</span><br><span class="line">    plt.plot(orignal_data[:,<span class="number">0</span>], pred.flatten(), color = <span class="string">&quot;blue&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(std)):</span><br><span class="line">        plt.fill_between(orignal_data[<span class="number">0</span>], pred.flatten()+std[i]*x_std, pred.flatten()-std[i]*x_std,</span><br><span class="line">                          color = <span class="string">&quot;blue&quot;</span>, alpha = <span class="number">0.2</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;bayesian_linear_regression&quot;</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></p>
<p>效果图如下： <img src="https://img-blog.csdnimg.cn/20210716232731384.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> <img src="https://img-blog.csdnimg.cn/20210716232742944.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> <img src="https://img-blog.csdnimg.cn/20210716232759263.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /></p>
]]></content>
      <categories>
        <category>学习总结</category>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>概率密度估计（Probability Density Estimation）--Part 3：混合模型</title>
    <url>/posts/8.html</url>
    <content><![CDATA[<h1 id="引入">引入</h1>
<p>在结束了有参估计，无参估计后，现在记录混合模型（Mixture models）。这里附一张有参和无参的对比图（本来应该附在Part 2的，不想回去改了。。）： <span id="more"></span> <img src="https://img-blog.csdnimg.cn/20210703061913869.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /></p>
<blockquote>
<p>字面意思，混合模型就是有参模型和无参模型的混合。</p>
</blockquote>
<p>举个例子，高斯模型的混合（Mixture of Gaussians，MoG）。<br />
现有三个高斯模型如下： <img src="https://img-blog.csdnimg.cn/20210703062219101.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 我们可以将其视为： <img src="https://img-blog.csdnimg.cn/20210703062258416.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 其概率密度可以近似表示为：<span class="math display">\[p(x)=\sum^M_{j=1}p(x|j)p(j)\]</span>各变量有如下关系： <img src="https://img-blog.csdnimg.cn/20210703062522599.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 大概解释一下：<span class="math inline">\(p(x|j)\)</span>表示第<span class="math inline">\(j\)</span>个高斯模型的概率密度，<span class="math inline">\(p(j)=\pi_j\)</span>表示第<span class="math inline">\(j\)</span>个高斯模型出现的概率，是一种先验概率。<br />
这里记住两点： <img src="https://img-blog.csdnimg.cn/20210703062812442.png#pic_center" alt="在这里插入图片描述" /> 即是说，混合模型概率密度积分为1，然后对于混合模型的每个子模型，我们都要求出对应的三个值（均值，方差和模型先验），这是对于MoG而言的。</p>
<h1 id="求解方法">求解方法</h1>
<h2 id="mle法">MLE法</h2>
<p>对于简单的（单个）高斯模型，我们可以用MLE近似求解，但在高斯混合中无法使用该方法，因为我们在求参数时并不知道这个参数属于高斯混合的哪个子分布。如下： <img src="https://img-blog.csdnimg.cn/20210707183134950.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 这个<span class="math inline">\(p(j|x)\)</span>我们是无法观测到的，毕竟如果这个都知道，那高斯混合就只是多个高斯模型的简单叠加而已。<br />
也就是说单个高斯模型的参数会取决于所有高斯模型的参数。所以MLE不可用。</p>
<h2 id="clustering">Clustering</h2>
<p>分为两种：</p>
<blockquote>
<ol type="1">
<li>Clustering with soft assignments<br />
</li>
<li>Clustering with hard assignments</li>
</ol>
</blockquote>
<p>简单说的话，hard assignments是根据Label划分的，也就是说每个数据点只属于一个label，比如说K-mean；soft assignment是根据概率区分的，每个数据点可能属于多个label，但概率不同。如下图： <img src="https://img-blog.csdnimg.cn/20210707184402519.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> <img src="https://img-blog.csdnimg.cn/2021070718442371.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="所有点都可能是label1或2" /> 这里只介绍soft assignment，毕竟两个几乎是一样的。这就要用到这章的重点了，<span class="math inline">\(EM\)</span>算法。</p>
<h3 id="em算法"><span class="math inline">\(EM\)</span>算法</h3>
<h4 id="大概的说明">大概的说明</h4>
<p><span class="math inline">\(EM\)</span>算法是一种迭代算法，本质上它也是一种最大似然估计的方法，其特点是，当我们的数据不完整时，比如说只有观测数据，缺乏隐含数据时，可以用EM算法进行迭代推导。<br />
该算法包括<span class="math inline">\(E\)</span>步骤和<span class="math inline">\(M\)</span>步骤，这里不具体介绍一般性的<span class="math inline">\(EM\)</span>算法，主要说明其在高斯混合中如何运用。其步骤大体可以概括为：</p>
<blockquote>
<ol type="1">
<li>随机初始化各子分布的期望<span class="math inline">\(\mu_1,\mu_2...\mu_m\)</span>。</li>
<li><strong>E-step</strong>：计算每个子分布的后验<span class="math display">\[p(j|x_n)\]</span></li>
<li><strong>M-step</strong>计算所有数据点的加权平均值 <img src="https://img-blog.csdnimg.cn/202107090127095.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 其效果如下图所示： <img src="https://img-blog.csdnimg.cn/20210709012920879.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /></li>
</ol>
</blockquote>
<h4 id="较为详细的说明">较为详细的说明</h4>
<p>现在说一下更加详细的步骤。<br />
假设现在有两组数据，即是观测数据和隐藏数据：</p>
<blockquote>
<ol type="1">
<li>Incomplete (observed) data: <span class="math inline">\(X={(X_1, X_2, ... ,X_n)}\)</span></li>
<li>Hidden (unobserved) data: <span class="math inline">\(Y=(Y_1, Y_2,...,Y_n)\)</span></li>
</ol>
</blockquote>
<p>组合后形参完整数据：</p>
<blockquote>
<ol type="1">
<li>Complete data: <span class="math inline">\(Z=(X,Y)\)</span></li>
</ol>
</blockquote>
<p>联合密度为<span class="math display">\[ p(Z)=p(X,Y)=p(Y|X)p(X)\]</span>即是<span class="math display">\[p(Z|\theta)=p(X,Y|\theta)=p(Y|X,\theta)p(X|\theta)\]</span>在高斯混合中：<br />
<span class="math inline">\(p(X|\theta)\)</span>是混合模型的似然<br />
<span class="math inline">\(p(Y|X,\theta)\)</span>是混合模型中子分布的估计</p>
<p>对于不完整的数据（观测数据），其似然为：<span class="math display">\[L(\theta|X)=p(X|\theta)=\prod_{n=1}^{N}p(X_n|\theta)\]</span>对于完整数据（Z），其似然为： <img src="https://img-blog.csdnimg.cn/20210712040548473.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 在这里我们虽然不知道<span class="math inline">\(Y\)</span>，但如果我们知道当前的参数猜测<span class="math inline">\(\theta^{i-1}\)</span>，我们就能用它来预测<span class="math inline">\(Y\)</span>。<br />
在这里我们计算完整数据的对数似然的期望，如下： <img src="https://img-blog.csdnimg.cn/20210712042100870.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 其中<span class="math inline">\(X\)</span>和<span class="math inline">\(\theta^{i-1}\)</span>是已知的。更进一步展开如下： <img src="https://img-blog.csdnimg.cn/20210712051332414.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 这个等式是根据均值和积分的关系写出来的。即是如下关系：<br />
<span class="math display">\[E[x]=\int_Xxf(x)dx\]</span>其中<span class="math inline">\(f(x)\)</span>是概率密度（<em>这部分不太确定，有错的话麻烦大家指出</em>）。<br />
我们需要最大化这个<span class="math inline">\(Q\)</span>函数。<br />
接下来是<span class="math inline">\(EM\)</span>算法：<br />
<strong>E-step(expectation)</strong>: 计算<span class="math inline">\(p(y|X,\theta^{i-1})\)</span>以便计算<span class="math inline">\(Q(\theta,\theta^{i-1})\)</span>;<br />
<strong>M-step(maximization)</strong>: 最大化<span class="math inline">\(Q\)</span>函数求出<span class="math inline">\(\theta\)</span> <span class="math display">\[\hat{\theta}=arg max_\theta Q(\theta,\theta^{i-1})\]</span> 这是一种迭代运算，我们要确保每次迭代中，第<span class="math inline">\(i\)</span>次的结果至少和第<span class="math inline">\(i-1\)</span>的一样好，即是：<span class="math display">\[Q(\theta^i,\theta^{i-1})\geq Q(\theta^{i-1},\theta^{i-1})\]</span> 若该期望值对于<span class="math inline">\(\theta\)</span>来说是最大的，则可以认为(==这部分未理解==)：<span class="math display">\[L(\theta^i|X)\geq L(\theta^{i-1}|X)\]</span> 也就是说，在每次迭代中观测数据的对数似然都会不断增大（或者至少保持不变），最终达到局部最大值。<br />
所以，在实际运用中，初始化对于<span class="math inline">\(EM\)</span>算法很重要，一个不好的初始化可能会使结果停在一个不好的局部最优值中。</p>
<h4 id="高斯混合中的em算法em-for-gaussian-mixtures">高斯混合中的<span class="math inline">\(EM\)</span>算法（EM for Gaussian Mixtures）</h4>
<p>步骤：</p>
<ol type="1">
<li><p>初始化参数<span class="math inline">\(\mu_1,\sigma_1, \pi_1...\)</span><br />
</p></li>
<li><p>循环，直到满足终止条件：</p>
<ol type="1">
<li>E-step: 计算每个数据点对于每个子分布的后验分布: <img src="https://img-blog.csdnimg.cn/20210712054310641.png#pic_center" alt="在这里插入图片描述" /> 这里的<span class="math inline">\(\alpha\)</span>可以理解成每个数据点属于各个子分布的权重或者说概率。<br />
</li>
<li>M-step: 使用E步骤的权重进行更新数据： <img src="https://img-blog.csdnimg.cn/2021071205454123.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" />至此，高斯混合的<span class="math inline">\(EM\)</span>算法就结束了，然后还有最后一个问题，这部分不太理解，但把结论贴上来吧，以后再探究：</li>
</ol></li>
</ol>
<figure>
<img src="https://img-blog.csdnimg.cn/2021071205595953.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="" /><figcaption>在这里插入图片描述</figcaption>
</figure>
<h1 id="附作业相关代码">（附）作业相关代码</h1>
<p><img src="https://img-blog.csdnimg.cn/2021071502051214.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 也就是给出数据点，然后用EM算法进行高斯拟合： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> multivariate_normal</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span>(<span class="params">path</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.loadtxt(path)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_params</span>(<span class="params">k, d = <span class="number">2</span></span>):</span></span><br><span class="line">    pi = np.ones(k) * <span class="number">1</span>/k</span><br><span class="line">    mu = [np.random.rand(<span class="number">2</span>, <span class="number">1</span>) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k)]</span><br><span class="line">    cov = [np.eye(<span class="number">2</span>) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> pi, mu, cov</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">EM</span>(<span class="params">iter_times, data, k</span>):</span></span><br><span class="line">    pi, mu, cov = init_params(k)</span><br><span class="line">    N = data.shape[<span class="number">0</span>]</span><br><span class="line">    alpha = np.zeros((N, k))</span><br><span class="line">    likelihood_list = []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_alpha</span>(<span class="params">alpha, N, k</span>):</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">                p_ij = pi[j] * multivariate_normal.pdf(data[i], mu[j].flatten(), cov[j])</span><br><span class="line">                alpha[i][j] = p_ij</span><br><span class="line">                likelihood[i] += p_ij</span><br><span class="line">            alpha[i] = alpha[i] / np.<span class="built_in">sum</span>(alpha[i])</span><br><span class="line">        <span class="keyword">return</span> alpha</span><br><span class="line"></span><br><span class="line">    sigma = np.empty((N, <span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">        d = data[i, :].reshape(<span class="number">2</span>, -<span class="number">1</span>)</span><br><span class="line">        sigma[i] = np.dot(d, d.T)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iter_times):</span><br><span class="line">        likelihood = np.zeros(N)</span><br><span class="line">        <span class="comment"># E-step: update alpha</span></span><br><span class="line">        alpha = update_alpha(alpha, N, k)</span><br><span class="line">        likelihood_list.append(np.<span class="built_in">sum</span>(np.log(likelihood)))</span><br><span class="line">        <span class="comment"># M-step: update mu, cov, pi</span></span><br><span class="line">        N_j = np.<span class="built_in">sum</span>(alpha, axis = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">            <span class="comment"># update mu</span></span><br><span class="line">            alpha_x = <span class="number">0</span></span><br><span class="line">            alpha_x_mu = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">                alpha_x += (alpha[n][j] * data[n])</span><br><span class="line">            alpha_x = alpha_x.reshape(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">            mu[j] = alpha_x / N_j[j]</span><br><span class="line">            <span class="comment"># update pi</span></span><br><span class="line">            pi[j] = N_j[j] / N</span><br><span class="line">            <span class="comment"># update cov</span></span><br><span class="line">            <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">                alpha_x_mu += alpha[n][j]*(data[n] - mu[j].T)*(data[n] - mu[j].T).T</span><br><span class="line">            cov[j] = alpha_x_mu / N_j[j]</span><br><span class="line"></span><br><span class="line">    plot_contour(iter_times, data, mu, cov, k)</span><br><span class="line">    <span class="keyword">if</span> iter_times == <span class="number">30</span>:</span><br><span class="line">        plt.figure()</span><br><span class="line">        plt.plot(np.arange(<span class="number">1</span>, <span class="number">31</span>), np.array(likelihood_list))</span><br><span class="line">        plt.title(<span class="string">&#x27;log-likelihood for every iteration&#x27;</span>)</span><br><span class="line">        plt.xlabel(<span class="string">&#x27;iteration&#x27;</span>)</span><br><span class="line">        plt.ylabel(<span class="string">&#x27;log-likelihood&#x27;</span>)</span><br><span class="line">        plt.grid()</span><br><span class="line">        plt.show()</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_contour</span>(<span class="params">iter_num, data, mu, cov, k</span>):</span></span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.title(<span class="string">&quot;iter_num = %d&quot;</span> % iter_num)</span><br><span class="line">    plt.scatter(data[:,<span class="number">0</span>], data[:,<span class="number">1</span>])</span><br><span class="line">    x_min, x_max = plt.gca().get_xlim()</span><br><span class="line">    y_min, y_max = plt.gca().get_ylim()</span><br><span class="line"></span><br><span class="line">    num = <span class="number">50</span></span><br><span class="line">    x = np.linspace(x_min, x_max, num)</span><br><span class="line">    y = np.linspace(y_min, y_max, num)</span><br><span class="line">    X, Y = np.meshgrid(x, y)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> sub_k <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">        Z = np.zeros_like(X)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x)):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(y)):</span><br><span class="line">                z = multivariate_normal.pdf(np.array([[x[i]], [y[j]]]).flatten(), mu[sub_k].flatten(), cov[sub_k])</span><br><span class="line">                Z[j, i] = z</span><br><span class="line">        plt.contour(X, Y, Z)</span><br><span class="line">        plt.show()</span><br><span class="line"></span><br><span class="line">path = <span class="string">&quot;./dataSets/gmm.txt&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    k = <span class="number">4</span></span><br><span class="line">    iter_lst = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">30</span>]</span><br><span class="line">    pi, mu, cov = init_params(k)</span><br><span class="line">    data = load_data(path)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> iter_lst:</span><br><span class="line">        EM(i, data, k)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure></p>
<p>效果如下图所示： 迭代1次： <img src="https://img-blog.csdnimg.cn/20210715020905924.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 迭代3次： <img src="https://img-blog.csdnimg.cn/20210715020944160.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 迭代5次： <img src="https://img-blog.csdnimg.cn/20210715021008112.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 迭代10次： <img src="https://img-blog.csdnimg.cn/20210715021034201.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 迭代30次： <img src="https://img-blog.csdnimg.cn/20210715021055371.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 每次迭代的对数似然： <img src="https://img-blog.csdnimg.cn/20210715021143437.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /></p>
]]></content>
      <categories>
        <category>学习总结</category>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>概率密度估计（Probability Density Estimation）--Part 2：无参估计</title>
    <url>/posts/7.html</url>
    <content><![CDATA[<h1 id="引入">引入</h1>
<p>接上一篇的【有参估计】，这篇介绍无参估计，也就是说在这里我们事先不知道数据的模型，而要求数据进行划分，这也是实际中比较常见的情况。 <span id="more"></span></p>
<p>这主要介绍三种无参估计方法，分别是：</p>
<pre><code>1. 直方图（Histograms）
2. 核密度估计（Kernel Density Estimation，KDE）
3. K最邻近法（K-nearest Neighbors，KNN）</code></pre>
<h1 id="直方图histograms">直方图（Histograms）</h1>
<p>直方图是最简单的一种方法，通过对数据进行统计分类，画出直方图即可，这里需要注意的是直方图中<span class="math inline">\(bin\)</span>值的选取。<br />
对于<span class="math inline">\(bin\)</span>值的不同选取可能会有下面三种情况： <img src="https://img-blog.csdnimg.cn/20210627054925776.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 直方图的特点是：</p>
<blockquote>
<ol type="1">
<li>使用非常普遍，没有数据的限制，任何概率密度都可以被任意地近似；<br />
</li>
<li>这个方法太粗暴。。。</li>
</ol>
</blockquote>
<p>直方图的缺陷有：</p>
<blockquote>
<ol type="1">
<li>高维空间中，<span class="math inline">\(bin\)</span>的数目以指数级增长；<br />
</li>
<li><span class="math inline">\(bin\)</span>值得选取不容易。</li>
</ol>
</blockquote>
<p>直方图的相关计算： （也不局限于直方图，这些计算方法是很通用的，后面也会用到。）<br />
我们现在有数据点<span class="math inline">\(x\)</span>，其是从概率密度<span class="math inline">\(p(x)\)</span>中取样的。<br />
<span class="math inline">\(x\)</span>落在区域<span class="math inline">\(R\)</span>的概率是：<span class="math display">\[P(x\in R)=\int_Rp(x)dx\]</span>如果<span class="math inline">\(R\)</span>非常小，体积为<span class="math inline">\(V\)</span>，则<span class="math inline">\(p(x)\)</span>几乎是常数：<span class="math display">\[P(x\in R)=\int_Rp(x)dx\approx p(x)V\]</span>如果<span class="math inline">\(R\)</span>很大，则：<span class="math display">\[P(x\in R) = \frac{K}{N}\rightarrow p(x)\approx \frac{K}{NV}\]</span>其中<span class="math inline">\(N\)</span>是数据总数，<span class="math inline">\(K\)</span>是落在区域<span class="math inline">\(R\)</span>内的数据数。</p>
<p>对于Kernel density estimation (KDE)，是固定<span class="math inline">\(V\)</span>并确定<span class="math inline">\(K\)</span>，比如说：确定固定在超立方体中的数据点<span class="math inline">\(K\)</span>的数量，如下图示： <img src="https://img-blog.csdnimg.cn/20210627061013461.png#pic_center" alt="在这里插入图片描述" /> 对于K-nearest neighbors (kNN)，是固定<span class="math inline">\(K\)</span>并确定<span class="math inline">\(V\)</span>，比如说：增加球的尺寸直到<span class="math inline">\(K\)</span>个点都落在球内，如下图： <img src="https://img-blog.csdnimg.cn/20210627061147105.png#pic_center" alt="在这里插入图片描述" /></p>
<h1 id="kde">KDE</h1>
<h2 id="parzen-window">Parzen Window</h2>
<p>对于一个<span class="math inline">\(d\)</span>维，边长为<span class="math inline">\(h\)</span>的超立方体，我们有如下关系： <img src="https://img-blog.csdnimg.cn/20210628225842881.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /></p>
<h2 id="gaussian-kernel">Gaussian Kernel</h2>
<p>对于高斯核，我们则可以写成： <img src="https://img-blog.csdnimg.cn/2021062822595979.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /></p>
<h2 id="general-formulation-arbitrary-kernel">General Formulation – Arbitrary Kernel</h2>
<p>对于更一般的形式，则有： <img src="https://img-blog.csdnimg.cn/20210628230351141.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /></p>
<h2 id="各种内核的总结">各种内核的总结</h2>
<h3 id="高斯核gaussian-kernel">高斯核（Gaussian Kernel）</h3>
<p><span class="math display">\[k(u)=\frac{1}{\sqrt{2\pi}}exp\left\{-\frac{1}{2}u^2\right\}\]</span> 缺点是：</p>
<pre><code>1. kernel has infinite support
2. Requires a lot of computation</code></pre>
<h3 id="parzen-window-1">Parzen window</h3>
<p><img src="https://img-blog.csdnimg.cn/20210701060654119.png#pic_center" alt="在这里插入图片描述" /> 缺点是：</p>
<pre><code>1. Not very smooth results</code></pre>
<h3 id="epanechnikov-kernel">Epanechnikov kernel</h3>
<p><img src="https://img-blog.csdnimg.cn/20210701060839776.png#pic_center" alt="在这里插入图片描述" /> 缺点是：</p>
<pre><code>1. Smoother, but finite support</code></pre>
<h3 id="总结">总结</h3>
<p>核方法的缺点是：我们必须合适地选择核的带宽<span class="math inline">\(h\)</span>。<br />
如果<span class="math inline">\(h\)</span>太大则会过于平滑，太小则不够平滑。<br />
一个高斯核的例子： <img src="https://img-blog.csdnimg.cn/20210701061312403.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /></p>
<h1 id="knn">KNN</h1>
<p>KNN分类大体意思是：假设我们的数据集有<span class="math inline">\(N\)</span>个点，有<span class="math inline">\(N_j\)</span>个点属于类<span class="math inline">\(C_j\)</span>，并且有<span class="math inline">\(\sum_jN_j=N\)</span>。现在要对点<span class="math inline">\(x\)</span>进行分类，我们先画一个球，中心在<span class="math inline">\(x\)</span>处，并且包含了（任意类的）<span class="math inline">\(K\)</span>个点。假设球体积为<span class="math inline">\(V\)</span>，且包含有<span class="math inline">\(K_j\)</span>个点是属于<span class="math inline">\(C_j\)</span>的，则我们有如下关系： <img src="https://img-blog.csdnimg.cn/20210701062118992.png#pic_center" alt="在这里插入图片描述" /></p>
<blockquote>
<p>注意：与灰度图的<span class="math inline">\(bins\)</span>和KDE的<span class="math inline">\(h\)</span>类似，<span class="math inline">\(K\)</span>值太大则会过于平滑，太小则不够平滑。</p>
</blockquote>
<p>一个例子： <img src="https://img-blog.csdnimg.cn/20210701062621685.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /></p>
<h1 id="附作业相关代码">（附）作业相关代码</h1>
<p><img src="https://img-blog.csdnimg.cn/20210701062801504.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 简单解释下作业：<br />
有两个数据集，分别为训练集和测试集，第1题要求画出不同<span class="math inline">\(bins\)</span>下的直方图；第2题则是用高斯核，用不同的<span class="math inline">\(\sigma\)</span>计算训练集的对数似然，比较差异性；第3题要求用不同的<span class="math inline">\(K\)</span>值写<span class="math inline">\(KNN\)</span>代码。<br />
代码如下： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.special <span class="keyword">import</span> logsumexp</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span>(<span class="params">path</span>):</span></span><br><span class="line">    data = np.loadtxt(path)</span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_histogram</span>(<span class="params">bins, arr</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(bins) % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">        fir = <span class="built_in">len</span>(bins) / <span class="number">2</span> * <span class="number">100</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        fir = (<span class="built_in">len</span>(bins) // <span class="number">2</span> + <span class="number">1</span>) * <span class="number">100</span></span><br><span class="line">    </span><br><span class="line">    pos = fir + <span class="number">20</span> + <span class="number">1</span></span><br><span class="line">    interval = np.<span class="built_in">max</span>(arr) - np.<span class="built_in">min</span>(arr)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(bins)):</span><br><span class="line">        sub_bin = <span class="built_in">int</span>(interval // bins[i]) + <span class="number">1</span></span><br><span class="line">        plt.subplot(pos+i)</span><br><span class="line">        plt.hist(arr, bins = sub_bin, facecolor=<span class="string">&quot;blue&quot;</span>, edgecolor=<span class="string">&quot;black&quot;</span>, alpha=<span class="number">0.7</span>)</span><br><span class="line">        plt.title(<span class="string">&quot;Bin = %.3f&quot;</span> % bins[i])</span><br><span class="line">        plt.xlabel(<span class="string">&#x27;data&#x27;</span>)</span><br><span class="line">        plt.ylabel(<span class="string">&#x27;times&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kernel_density_estimate</span>(<span class="params">x_arr, x_lst, sigma</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_H_Gauss</span>(<span class="params">h, u, d</span>):</span></span><br><span class="line">        H_u = np.exp(-(np.linalg.norm(u) ** <span class="number">2</span>) / (<span class="number">2</span> * h * h)) \</span><br><span class="line">              / (np.sqrt(<span class="number">2</span> * np.pi * h * h) ** d)</span><br><span class="line">        <span class="keyword">return</span> H_u</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_K</span>(<span class="params">N, x, x_arr, h, d</span>):</span></span><br><span class="line">        k = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">            u = x - x_arr[i]</span><br><span class="line">            k += _H_Gauss(h, u, d)</span><br><span class="line">        <span class="keyword">return</span> k</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">p</span>(<span class="params">N, x, x_arr, h, d, V</span>):</span></span><br><span class="line">        p_x = _K(N, x, x_arr, h, d) / (N * V)</span><br><span class="line">        <span class="keyword">return</span> p_x</span><br><span class="line">    </span><br><span class="line">    V = <span class="number">1</span></span><br><span class="line">    N = <span class="built_in">len</span>(x_arr.flatten())</span><br><span class="line">    p_x_lst = []</span><br><span class="line">    d = <span class="built_in">len</span>(x_arr.flatten())/(x_arr.shape[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> x_lst:</span><br><span class="line">        temp_p = p(N, x, x_arr, sigma, d, V)</span><br><span class="line">        p_x_lst.append(temp_p)</span><br><span class="line">    <span class="keyword">return</span> p_x_lst</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_logsumexp</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.log(np.<span class="built_in">sum</span>(np.exp(x)))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ked_log_likelihood</span>(<span class="params">x_arr, x_lst, sigma</span>):</span></span><br><span class="line">    N = x_arr.shape[<span class="number">0</span>]</span><br><span class="line">    ll_lst = []</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> x_lst:</span><br><span class="line">        single_val = _logsumexp(-np.linalg.norm(x_arr.reshape(-<span class="number">1</span>,<span class="number">1</span>) - x, axis=<span class="number">1</span>) ** <span class="number">2</span> / (<span class="number">2</span> * sigma * sigma) \</span><br><span class="line">                               - np.log(np.sqrt(<span class="number">2</span> * np.pi * sigma * sigma) * N))</span><br><span class="line">        ll_lst.append(single_val)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> np.<span class="built_in">sum</span>(ll_lst)</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kNN</span>(<span class="params">data, x_lst, k</span>):</span></span><br><span class="line">    knn_lst = []</span><br><span class="line">    N = <span class="built_in">len</span>(data.flatten())</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> x_lst:</span><br><span class="line">        u = np.linalg.norm(x - data.reshape(-<span class="number">1</span>,<span class="number">1</span>), axis = <span class="number">1</span>)</span><br><span class="line">        r = np.sort(u)[k-<span class="number">1</span>]</span><br><span class="line">        v = <span class="number">2</span> * r</span><br><span class="line">        temp_p = k / (N * v)</span><br><span class="line">        knn_lst.append(temp_p)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> knn_lst, np.<span class="built_in">sum</span>(np.log(knn_lst))</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Comparison_NPM</span>(<span class="params">data, test_data, k_lst, sigma_lst</span>):</span></span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> sigma_lst:</span><br><span class="line">        temp_kde_test = ked_log_likelihood(data, test_data, s)</span><br><span class="line">        temp_kde_train = ked_log_likelihood(data, data, s)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;train_data: log_likelihood of KDE with sigma = %.2f: %.12f&#x27;</span> % (s, temp_kde_train) )</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;test_data : log_likelihood of KDE with sigma = %.2f: %.12f&#x27;</span> % (s, temp_kde_test) )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> k_lst:</span><br><span class="line">        _, temp_knn_test = kNN(data, test_data, k)</span><br><span class="line">        _, temp_knn_train = kNN(data, data, k)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;train_data: log_likelihood of KDE with k = %d: %.12f&#x27;</span> % (k, temp_knn_train) )</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;test_data : log_likelihood of KDE with k = %d: %.12f&#x27;</span> % (k, temp_knn_test) )        </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    path = <span class="string">&quot;./dataSets/nonParamTrain.txt&quot;</span></span><br><span class="line">    path_test = <span class="string">&#x27;./dataSets/nonParamTest.txt&#x27;</span></span><br><span class="line">    data = load_data(path)</span><br><span class="line">    data_test = load_data(path_test)</span><br><span class="line">    bins_w = [<span class="number">0.02</span>, <span class="number">0.5</span>, <span class="number">2.0</span>]</span><br><span class="line">    plot_histogram(bins_w, data)</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    num = <span class="built_in">len</span>(data.flatten())</span><br><span class="line"></span><br><span class="line">    p_x = []</span><br><span class="line">    p_test = []</span><br><span class="line">    sigma = [<span class="number">0.03</span>, <span class="number">0.2</span>, <span class="number">0.8</span>]</span><br><span class="line">    x_lst = np.linspace(-<span class="number">4</span>, <span class="number">8</span>, data.shape[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> h <span class="keyword">in</span> sigma:</span><br><span class="line">        temp_p = kernel_density_estimate(data, x_lst, h)</span><br><span class="line">        p_x.append(temp_p)</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> p_x:</span><br><span class="line">        plt.plot(x_lst, x)</span><br><span class="line">    plt.xlabel(<span class="string">&quot;X&quot;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&quot;kde&quot;</span>)</span><br><span class="line">    plt.legend([<span class="string">&#x27;sigma=0.03&#x27;</span>,<span class="string">&#x27;sigma=0.2&#x27;</span>,<span class="string">&#x27;sigma=0.8&#x27;</span>])</span><br><span class="line">    plt.title(<span class="string">&#x27;KDE with different sigma&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    k = [<span class="number">2</span>, <span class="number">8</span>, <span class="number">35</span>]</span><br><span class="line">    knn_lst = []</span><br><span class="line">    <span class="keyword">for</span> sub_k <span class="keyword">in</span> k:</span><br><span class="line">        temp_knn, _ = kNN(data, x_lst, sub_k)</span><br><span class="line">        knn_lst.append(temp_knn)</span><br><span class="line">    <span class="keyword">for</span> knn <span class="keyword">in</span> knn_lst:</span><br><span class="line">        plt.plot(x_lst, knn)</span><br><span class="line">    plt.xlabel(<span class="string">&quot;X&quot;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&quot;knn&quot;</span>)</span><br><span class="line">    plt.axis([-<span class="number">4</span>, <span class="number">8</span>, <span class="number">0</span>, <span class="number">1.5</span>])</span><br><span class="line">    plt.legend([<span class="string">&#x27;k=2&#x27;</span>,<span class="string">&#x27;k=8&#x27;</span>,<span class="string">&#x27;k=35&#x27;</span>])</span><br><span class="line">    plt.title(<span class="string">&#x27;KNN with different k&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    Comparison_NPM(data, data_test, k, sigma)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p>结果显示如下： <img src="https://img-blog.csdnimg.cn/20210701063451438.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> <img src="https://img-blog.csdnimg.cn/2021070106345256.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> <img src="https://img-blog.csdnimg.cn/20210701063452883.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /></p>
]]></content>
      <categories>
        <category>学习总结</category>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>概率密度估计（Probability Density Estimation）--Part 1：参数估计</title>
    <url>/posts/6.html</url>
    <content><![CDATA[<h1 id="概率密度的引入">概率密度的引入</h1>
<span id="more"></span>
<p>当我们有如下的点分布 <img src="https://img-blog.csdnimg.cn/20210623072951840.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_2,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 为了能区分它们，我们需要知道这些点的概率分布。常见的有贝叶斯最优分类(Bayes optimal classification)，这是基于如下的概率分布：<span class="math display">\[
p(x|C_k)p(C_k)\]</span> 其中的先验<span class="math inline">\(p(C_k)\)</span>很容易算，就是统计，或者说数数，现在的问题就成了计算如下的条件概率密度：<span class="math inline">\(p(x|C_k)\)</span>。这主要有三种情况：</p>
<pre><code>1. Parametric model
2. Non-parametric model
3. Mixture models</code></pre>
<p>（简单解释一下，有参估计是指我们知道样本数据符合某种概率密度模型，通过给出的数据求出所需要的参数，比如高斯分布的均值和方差，有参估计的Robust比较好；无参估计是指我们不知道这些数据点符合哪些模型，所以无法求出其参数，这种情况在现实生活中更加常见）</p>
<h1 id="参数模型parametric-model">参数模型（Parametric model）</h1>
<p>概率密度模型的符号表示为：<span class="math display">\[x \sim p(x|\theta)\]</span> 以高斯模型为例：<span class="math display">\[p(x|\mu,\sigma)=\frac{1}{\sqrt{2\pi \sigma^2}}\exp \left\{ -\frac{(x-\mu)^2}{2\sigma^2}\right\}\]</span> 我们只要知道它的均值和方差就能完整地描述这个模型，即是：<span class="math display">\[\theta = (\mu, \sigma)\]</span><span class="math display">\[x\sim p(x|\mu,\sigma)\]</span>现在我们需要根据模型和数据集求出<span class="math inline">\(\theta\)</span>。<br />
其似然函数表示为：(<span class="math inline">\(X\)</span>为数据集数据) <span class="math display">\[L(\theta)=p(X|\theta)\]</span>对于有参模型，这里介绍最大似然方法（Maximum Likelihood Method）</p>
<h2 id="最大似然方法maximum-likelihood-method">最大似然方法（Maximum Likelihood Method）</h2>
<p>现假设我们有数据集<span class="math inline">\(X=\left\{x_1,x_2,...,x_N\right\}\)</span><br />
对于这些数据集我们假设数据是i.i.d(independent and identically distributed)的，即是：</p>
<blockquote>
<ol type="1">
<li><span class="math inline">\(P(x_1\le\alpha, x_2\le\beta)=P(x_1\le\alpha)P(x_2\le\beta)\)</span><br />
</li>
<li><span class="math inline">\(P(x_1\le\alpha)=P(x_2\le\alpha)\)</span> 通过该假设，似然计算可写为： <img src="https://img-blog.csdnimg.cn/20210624014545305.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 两边取对数为： <img src="https://img-blog.csdnimg.cn/20210624014705555.png#pic_center" alt="在这里插入图片描述" /> 然后就是求导之类的事了。<br />
现在有个问题，如果<span class="math inline">\(N=1\)</span>，即是说<span class="math inline">\(X=\left\{x_1\right\}\)</span>，这时候产生的高斯模型会如下图所示： <img src="https://img-blog.csdnimg.cn/2021062402093825.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 看上去就像个<span class="math inline">\(\delta\)</span>函数。<br />
这时候我们可以给平均数加上先验 <img src="https://img-blog.csdnimg.cn/20210624021134780.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /></li>
</ol>
</blockquote>
<h2 id="贝叶斯估计bayesian-estimation">贝叶斯估计（Bayesian Estimation）</h2>
<p>与极大似然估计不同，贝叶斯估计假设待估计参数不是固定而是随机的。<br />
现在我们要根据给出的数据集<span class="math inline">\(X\)</span>，得到<span class="math inline">\(x\)</span>的密度函数<span class="math inline">\(p(x)\)</span>。将其写成条件概率形式：<span class="math display">\[p(x|X)\]</span>用边缘概率和贝叶斯公式得出下图关系： <img src="https://img-blog.csdnimg.cn/20210625183231837.png#pic_center" alt="在这里插入图片描述" /> 由于<span class="math inline">\(p(x)\)</span>能根据<span class="math inline">\(\theta\)</span>完全确定，即是说<span class="math inline">\(\theta\)</span>是sufficient statistic，所以<span class="math display">\[p(x|\theta, X) = p(x|\theta)\]</span>进而上面两个式子可写成： <img src="https://img-blog.csdnimg.cn/20210625184001131.png#pic_center" alt="在这里插入图片描述" /> （简单解释一下这个式子，<span class="math inline">\(p(\theta |X)\)</span>表示估计参数对数据集<span class="math inline">\(X\)</span>的依赖程度，也就是说<span class="math inline">\(\theta\)</span>在一定程度上可以代指<span class="math inline">\(X\)</span>，比如当<span class="math inline">\(p(\theta|X)\)</span>在大多数地方都很小，但唯独在<span class="math inline">\(\hat{\theta}\)</span>处很大，我们就能近似表示：<span class="math inline">\(p(x|X) \approx p(x|\hat{\theta})\)</span>，该点也称为Bayes point）<br />
继续计算如下： <img src="https://img-blog.csdnimg.cn/20210625184553971.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 这里稍微对图里的参数进行解释：<br />
<span class="math inline">\(p(\theta)\)</span>是估计参数的先验分布，它与给的数据集无关，而是我们的一种经验性判断。对于<span class="math inline">\(p(\theta|X)\)</span>，是一个后验分布，所以一般取期望值，而且 <img src="https://img-blog.csdnimg.cn/20210625191701870.png#pic_center" alt="在这里插入图片描述" /> 举个例子：<br />
我们现在有高斯分布的数据集，其方差已知，我们要估计其均值，有如下关系： <img src="https://img-blog.csdnimg.cn/20210625191859887.png#pic_center" alt="在这里插入图片描述" /> 对于先验我们的判断是： <img src="https://img-blog.csdnimg.cn/20210625191940406.png#pic_center" alt="在这里插入图片描述" /> 根据采样均值和贝叶斯估计就能对均值进行估计： <img src="https://img-blog.csdnimg.cn/20210625192158823.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 最后这里补充个概念：</p>
<blockquote>
<p>在贝叶斯统计中，如果后验分布与先验分布属于同类，则先验分布与后验分布被称为共轭分布，而先验分布被称为似然函数的共轭先验（Conjugate prior）。比如，高斯分布家族在高斯似然函数下与其自身共轭 (自共轭)。(来自 wiki百科)</p>
</blockquote>
<p>比如上面那个均值的高斯先验，就是高斯模型的共轭，这里使用是因为：</p>
<blockquote>
<ol type="1">
<li>The product of two Gaussians is a Gaussian.<br />
</li>
<li>The marginal of a Gaussian is a Gaussian.</li>
</ol>
</blockquote>
<h2 id="附参数估计的代码作业">（附）参数估计的代码作业</h2>
<p><img src="https://img-blog.csdnimg.cn/20210625194745608.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 简单来说就是给两组二维数据，模型是高斯分布，进行参数估计拟合，第一题是纯粹的统计，跳过，第二题的话。。。还是直接附代码了吧（代码正确性未知，因为作业还没有公布结果） <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span>(<span class="params">path</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.loadtxt(path)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prior_probability</span>(<span class="params">arr_n, arr_sum</span>):</span></span><br><span class="line">    h_n, w_n = arr_n.shape</span><br><span class="line">    h_sum, w_sum = arr_sum.shape</span><br><span class="line">    p_n = (h_n * w_n) / (h_sum * w_sum)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> p_n</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">MLE_mean</span>(<span class="params">arr</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.mean(arr, axis = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_unbias_var</span>(<span class="params">arr</span>):</span></span><br><span class="line">    num = arr.shape[<span class="number">0</span>]</span><br><span class="line">    arr_mean = MLE_mean(arr)</span><br><span class="line">    dist = arr - arr_mean</span><br><span class="line">    sum_vor_cov = np.zeros((<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num):</span><br><span class="line">        ls = []</span><br><span class="line">        ls.append(dist[i])</span><br><span class="line">        dist_arr = np.array(ls)</span><br><span class="line">        sum_vor_cov += np.dot(dist_arr.T, dist_arr)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> sum_vor_cov / num, sum_vor_cov / (num - <span class="number">1</span>)</span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">two_dim_Gauss</span>(<span class="params">x, mu, cov</span>):</span></span><br><span class="line">    <span class="comment">#x = x.reshape(2, -1)</span></span><br><span class="line">    p_x = np.exp((-<span class="number">0.5</span>*(x - mu) @ np.linalg.inv(cov) @ (x - mu).T)) \</span><br><span class="line">          / np.sqrt((<span class="number">2</span> * np.pi) ** <span class="number">2</span> * np.linalg.det(cov))</span><br><span class="line">    <span class="comment">#print(p_x.shape)</span></span><br><span class="line">    <span class="keyword">return</span> p_x</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">posterior</span>(<span class="params">mu1, mu2, cov1, cov2, p_c1, p_c2</span>):</span></span><br><span class="line">    num = <span class="number">300</span></span><br><span class="line">    x = np.linspace(-<span class="number">10</span>, <span class="number">10</span>, num)</span><br><span class="line">    y = np.linspace(-<span class="number">10</span>, <span class="number">10</span>, num)</span><br><span class="line"></span><br><span class="line">    X,Y = np.meshgrid(x, y)</span><br><span class="line">    C1 = []</span><br><span class="line">    C2 = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x)):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(y)):</span><br><span class="line">            z1 = two_dim_Gauss(np.array([[x[i], y[j]]]), mu1, cov1)</span><br><span class="line">            z2 = two_dim_Gauss(np.array([[x[i], y[j]]]), mu2, cov2)</span><br><span class="line">            temp1 = z1 * p_c1</span><br><span class="line">            temp2 = z2 * p_c2</span><br><span class="line">            <span class="keyword">if</span> temp1 &gt; temp2:</span><br><span class="line">                C1.append(np.array([[x[i], y[j]]]))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                C2.append(np.array([[x[i], y[j]]]))</span><br><span class="line">    C1 = np.array(C1).reshape(-<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">    C2 = np.array(C2).reshape(-<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    plt.scatter(C1[:,<span class="number">0</span>], C1[:,<span class="number">1</span>])</span><br><span class="line">    plt.scatter(C2[:,<span class="number">0</span>], C2[:,<span class="number">1</span>])</span><br><span class="line">    plt.xlabel(<span class="string">&quot;X&quot;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&quot;Y&quot;</span>)</span><br><span class="line">    plt.legend([<span class="string">&#x27;Class1&#x27;</span>, <span class="string">&#x27;Class2&#x27;</span>])</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_Gauss_point</span>(<span class="params">arr, mu, cov</span>):</span></span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.scatter(arr[:,<span class="number">0</span>], arr[:,<span class="number">1</span>])</span><br><span class="line">    x_min, x_max = plt.gca().get_xlim()</span><br><span class="line">    y_min, y_max = plt.gca().get_ylim()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    num = <span class="number">50</span></span><br><span class="line">    x = np.linspace(x_min, x_max, num)</span><br><span class="line">    y = np.linspace(y_min, y_max, num)</span><br><span class="line">    X, Y = np.meshgrid(x, y)</span><br><span class="line">    </span><br><span class="line">    Z = np.zeros_like(X)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x)):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(y)):</span><br><span class="line">            z = two_dim_Gauss(np.array([[x[i], y[j]]]), mu, cov)</span><br><span class="line">            Z[j][i] = z</span><br><span class="line">    plt.contour(X,Y,Z)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;X&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;Y&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">path1 = <span class="string">&quot;./dataSets/densEst1.txt&quot;</span></span><br><span class="line">path2 = <span class="string">&quot;./dataSets/densEst2.txt&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    arr1 = load_data(path1)</span><br><span class="line">    arr2 = load_data(path2)</span><br><span class="line"></span><br><span class="line">    arr_sum = np.vstack((arr1, arr2))</span><br><span class="line"></span><br><span class="line">    p_C1 = prior_probability(arr1, arr_sum)</span><br><span class="line">    p_C2 = prior_probability(arr2, arr_sum)</span><br><span class="line"></span><br><span class="line">    mu2 = MLE_mean(arr2)</span><br><span class="line">    bias_s2, sigma2 = bias_unbias_var(arr2)</span><br><span class="line"></span><br><span class="line">    mu1 = MLE_mean(arr1)</span><br><span class="line">    bias_s1, sigma1 = bias_unbias_var(arr1)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;The mean for C1 is: &quot;</span>, mu1)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;The mean for C2 is: &quot;</span>, mu2)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;The sigma with bias for C1 is&quot;</span>, bias_s1)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;The sigma without bias for C1 is&quot;</span>, sigma1)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;The sigma with bias for C2 is&quot;</span>, bias_s2)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;The sigma without bias for C2 is&quot;</span>, sigma2)</span><br><span class="line">    </span><br><span class="line">    plot_Gauss_point(arr1, mu1, sigma1)</span><br><span class="line">    plt.title(<span class="string">&quot;densEst1&quot;</span>)</span><br><span class="line">    plot_Gauss_point(arr2, mu2, sigma2)</span><br><span class="line">    plt.title(<span class="string">&quot;densEst2&quot;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    posterior(mu1, mu2, sigma1, sigma2, p_C1, p_C2)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure> 结果如下： <img src="https://img-blog.csdnimg.cn/20210625195929319.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> <img src="https://img-blog.csdnimg.cn/20210625195925925.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> <img src="https://img-blog.csdnimg.cn/20210625195929478.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /></p>
]]></content>
      <categories>
        <category>学习总结</category>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>李代数和扰动模型</title>
    <url>/posts/5.html</url>
    <content><![CDATA[<p>前一篇大概介绍了【李群李代数】的相关性质，这里主要介绍李代数在SLAM中的作用。 <span id="more"></span></p>
<h2 id="bch公式及其近似形式">BCH公式及其近似形式</h2>
<p>李代数在SLAM中的作用主要是进行优化，优化过程就不可避免地需要涉及到求导的过程。<br />
与标量不同，在矩阵中指数的运算法则并不适用，即是说在矩阵中：<span class="math display">\[ln(exp(A)exp(B))\ne A+B\]</span> 取而代之的时BCH公式：<span class="math display">\[ln(exp(A)exp(B))=A+B+\frac{1}{2}[A,B]+\frac{1}{12}[A,[A,B]]-\frac{1}{12}[B,[A,B]]+...\]</span> 其中的"[,]"为李括号，也就是说，两个矩阵指数之积会由这两个矩阵的和与一系列由李括号组成的余项组成。对于SO(3)，当<span class="math inline">\(\phi_{1}\)</span>或<span class="math inline">\(\phi_{2}\)</span>为小量时，小量二次以上的项都可以被忽略，即是有以下关系： <img src="https://img-blog.csdnimg.cn/2021050722481379.png#pic_center" alt="在这里插入图片描述" /> 以第一个近似为例，当对一个旋转向量<span class="math inline">\(R_{2}\)</span>（对应的李代数为<span class="math inline">\(\phi_{2}\)</span>）左乘一个微小旋转矩阵<span class="math inline">\(R_{1}\)</span>，可以近似看作在原有李代数<span class="math inline">\(\phi_{2}\)</span>的基础上加上了<span class="math inline">\(J_{l}(\phi_{2})^{-1}\phi_{1}\)</span>（第二个近似同理，但是是右乘）。<br />
李代数在BCH近似下分为左乘和右乘两种，注意区分。<br />
这里的<span class="math inline">\(J\)</span>就是在上一篇关于李群李代数里提到的式子： <img src="https://img-blog.csdnimg.cn/20210507230638737.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 右乘的话只需要改变自变量的符号即可：<span class="math display">\[J_{r}(\phi)=J_{l}(-\phi)\]</span> 现在对于一个旋转<span class="math inline">\(R\)</span>(对应李代数为<span class="math inline">\(\phi\)</span>)，给之左乘一个微小旋转<span class="math inline">\(\triangle R\)</span>(对应于<span class="math inline">\(\triangle \phi\)</span>)，我们就有了如下关系： <img src="https://img-blog.csdnimg.cn/20210507231646273.png#pic_center" alt="在这里插入图片描述" /> 反之，当我们在李代数上做加法运算时则有： <img src="https://img-blog.csdnimg.cn/20210507231729943.png#pic_center" alt="在这里插入图片描述" /> 对于SE(3)，也有类似的关系： <img src="https://img-blog.csdnimg.cn/20210507231854489.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 这里的<span class="math inline">\(\jmath\)</span>是一个<span class="math inline">\(6\times6\)</span>的矩阵。</p>
<h2 id="so3上的李代数的求导">SO(3)上的李代数的求导</h2>
<p>在SLAM中要估计一个相机的位置和位姿，我们是通过SO(3)上的旋转矩阵或者SE(3)上的变换矩阵描述的。举个例子，在某时刻相机位姿为<span class="math inline">\(T\)</span>，其观察到世界坐标位于<span class="math inline">\(p\)</span>的一点，产生一个观测数据<span class="math inline">\(z\)</span>，那么我们就有如下关系：<span class="math display">\[z=Tp+\omega\]</span>其中的<span class="math inline">\(\omega\)</span>为随机噪音。<br />
由于噪音的存在我们无法获得精准的<span class="math inline">\(z-p\)</span>对应关系，所以现在我们就要对这个噪音产生的误差进行计算，即：<span class="math display">\[e=z-Tp\]</span> 假设一共有<span class="math inline">\(N\)</span>个这样的路标点和观察，即是有<span class="math inline">\(N\)</span>个上式，我们现在要预计该相机的位姿，就是要求出一组<span class="math inline">\(T\)</span>，使整体误差最小化：<span class="math display">\[\min_T J(T)=\sum_{i=1}^{N}||z_{i}-Tp_{i}||_{2}^{2}\]</span> 要求解这个问题，就要求<span class="math inline">\(J\)</span>对<span class="math inline">\(T\)</span>的导数，然而因为群SO(3)和SE(3)没有加法，所以我们一般转而处理它们的李代数（因为李代数是向量的集合，对加法封闭），因此思路一般分为两种：</p>
<blockquote>
<ol type="1">
<li>用李代数表示位姿，然后对李代数进行求导；<br />
</li>
<li>对李群左乘或者右乘微小扰动，然后对这个扰动进行求导，称为左扰动和右扰动模型</li>
</ol>
</blockquote>
<p>第一种对应李代数求导模型，第二种对应扰动模型。<br />
以下分别进行说明。</p>
<h3 id="李代数求导">李代数求导</h3>
<p><em>（其实这部分可不看，因为最后会发现扰动模型更适合）</em><br />
现在处理SO(3)的情况。假设对空间一点<span class="math inline">\(p\)</span>进行旋转，得到<span class="math inline">\(Rp\)</span>，我们要计算旋转后的点坐标相对于旋转的角度，即是：<span class="math display">\[\frac{\partial(Rp)}{\partial R}\]</span> 由于SO(3)没有加法，所以我们转而求其对应的李代数，即是：<span class="math display">\[\frac{\partial(exp(\phi^{\land})p)}{\partial \phi}\]</span> 根据导数的定义计算： <img src="https://img-blog.csdnimg.cn/20210508033658749.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 第二行的近似为BCH近似，第三行为在0附近的泰勒展开舍去高阶项后的近似，第四到第五行将反对称符号看作叉积，交换后变号。于是我们可以得出以下式子： <img src="https://img-blog.csdnimg.cn/20210508033958370.png#pic_center" alt="在这里插入图片描述" /> 这里简单解释下反对称矩阵和叉积的关系，也就是上面第四到第五行的变换： <span class="math display">\[a=[a_{1},a_{2},a_{3}]\]</span> <span class="math display">\[b=[b_{1},b_{2},b_{3}]\]</span> <span class="math display">\[a\times b=\begin{bmatrix} i&amp; j&amp; k\\ a_{1}&amp;a_{2}&amp; a_{3}\\ b_{1}&amp; b_{2}&amp; b_{3}\end{bmatrix} \\= \begin{bmatrix} a_{2}b_{3}-a_{3}b_{2} \\ -(a_{1}b_{3}-a_{3}b_{1}) \\ a_{1}b_{2}-a_{2}b_{1}\end{bmatrix} \\ =\begin{bmatrix} 0 &amp; -a_{3} &amp; a_{2} \\ a_{3} &amp; 0 &amp; -a_{1} \\ -a_{2} &amp; a_{1} &amp; 0 \end{bmatrix} \begin{bmatrix} b_{1} \\ b_{2} \\ b_{3} \end{bmatrix}\]</span> 所以，<span class="math inline">\(a\)</span> 与<span class="math inline">\(b\)</span>的叉乘即是等于<span class="math inline">\(a\)</span>的反对称矩阵乘以<span class="math inline">\(b\)</span>，同时叉乘满足如下反交换律：<span class="math display">\[a\times b=-b\times a\]</span> 回到原题，在上图中，由于结果式子含有形式比较复杂的<span class="math inline">\(J_{l}\)</span>，不容易计算，所以我们一般不采用上面的方法，而用下面的扰动模型的方法。</p>
<h3 id="扰动模型左乘">扰动模型（左乘）</h3>
<p>扰动模型的求导方法是对<span class="math inline">\(R\)</span>进行一次扰动<span class="math inline">\(\triangle R\)</span>，这个微小扰动可以左乘，也可以右乘，最后结果会有一点微小差异，这里以左乘为例。设左扰动<span class="math inline">\(\triangle R\)</span>对应的李代数为<span class="math inline">\(\varphi\)</span>，然后对<span class="math inline">\(\varphi\)</span>进行求导： <img src="https://img-blog.csdnimg.cn/20210508041300661.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 相比直接对李代数求导，这里省去了一个雅可比<span class="math inline">\(J_{l}\)</span>的计算，更为简便。</p>
<h2 id="se3上的李代数求导">SE(3)上的李代数求导</h2>
<p>假设空间一点<span class="math inline">\(p\)</span>经过一次变换<span class="math inline">\(T\)</span>(对应的李代数为<span class="math inline">\(\xi\)</span>)，得到<span class="math inline">\(Tp\)</span>(这里的<span class="math inline">\(p\)</span>注意用齐次坐标)。现给<span class="math inline">\(T\)</span>左乘一个扰动<span class="math inline">\(\triangle T=exp(\delta \xi^{\land})\)</span>，设扰动项的李代数为<span class="math inline">\(\delta \xi=[\delta \rho,\delta \phi]^{T}\)</span>，可得： <img src="https://img-blog.csdnimg.cn/20210508042117565.png#pic_center" alt="在这里插入图片描述" /> <img src="https://img-blog.csdnimg.cn/20210508042125983.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 把最后的结果定义成一个算符<span class="math inline">\(\odot\)</span>，它把一个齐次空间坐标变换成一个<span class="math inline">\(4\times 6\)</span>的矩阵。<br />
最后这里简单解释下矩阵求导的顺序，设<span class="math inline">\(a,b,x,y\)</span>都是列向量，其求导顺序为： <img src="https://img-blog.csdnimg.cn/2021050804251624.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> <strong><em>注：本文主要参考自《视觉SLAM十四讲》</em></strong></p>
]]></content>
      <categories>
        <category>视觉SLAM相关</category>
      </categories>
  </entry>
  <entry>
    <title>李群和李代数</title>
    <url>/posts/4.html</url>
    <content><![CDATA[<h2 id="群的定义">1. 群的定义</h2>
<p>群(Group)是<strong>一种集合</strong>加上<strong>一种运算</strong>的代数结构，以A表示集合，“·”表示运算，则群一般写作 G(A, ·)。群要求满足以下四个条件： <span id="more"></span> <img src="https://img-blog.csdnimg.cn/20210429051226112.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 举个例子，旋转矩阵和乘法可以构成旋转矩阵群，因为其满足：</p>
<pre><code>1). 旋转矩阵相乘后仍是旋转矩阵；
2). 矩阵乘法满足结合律；
3). 幺元为单位矩阵，也属旋转矩阵；
4). 旋转矩阵的逆也为旋转矩阵，且相乘为单位矩阵。</code></pre>
<p>常见的群有： <img src="https://img-blog.csdnimg.cn/20210501063426658.png#pic_center" alt="在这里插入图片描述" /></p>
<h2 id="李群">2. 李群</h2>
<p>李群是指拥有(连续)光滑性质的群，比如上面提到的旋转矩阵群，因其能在空间中连续旋转，变换矩阵群也是李群（就是在旋转过程中加上平移）。</p>
<h2 id="李代数的引入">3. 李代数的引入</h2>
<h3 id="直观的导出">直观的导出</h3>
<p>在视觉SLAM中，我们需要做的就是不断计算相机的位姿和构建地图（其中的相机位姿表现在其变换矩阵T），但由于干扰的存在，我们无法准确获得所需要的信息，所以我们转而求其最小误差。<br />
现假设我们有N个三维点p和对应的观测值z，那么我们的目标就是寻找一个最佳的位姿T，使得整体误差最小化，也就是求： <img src="https://img-blog.csdnimg.cn/20210501044418122.png#pic_center" alt="在这里插入图片描述" /> 要求解上面的方程，即是要求目标函数J对T的导数，但由于T所在的变换矩阵群（下面用SO(3)空间表示）对加法不封闭，无法直接求取，所以我们需要引入一个新的量，通过对该量的计算间接获得对变换矩阵T的求导，这个引入的量就是李代数。</p>
<h3 id="数学层面的导出">数学层面的导出</h3>
<p><strong>（之后的部分《视觉SLAM 十四讲》里写得已经很精炼很清楚了，这里大概重述一遍以加深印象，也方便以后的查询）</strong></p>
<p>先给一个结论：<strong>李代数对应李群的正切空间，描述了李群的局部导数。</strong></p>
<p>这里用旋转矩阵群SO(3)为例：<br />
我们知道旋转矩阵满足以下关系：<span class="math display">\[ RR^{T}=I\]</span> 然后引入时间变量<span class="math inline">\(t\)</span>变为时间的函数<span class="math inline">\(R(t)\)</span>，即是有：<span class="math display">\[ R(t)R(t)^{T}=I\]</span> 对两边求导得到：<span class="math display">\[\dot{R}(t)R(t)^{T}+\dot{R}(t)^{T}R(t)=0\]</span> 整理得：<span class="math display">\[\dot{R}(t)R(t)^{T}=-(\dot{R}(t)R(t)^{T})^{T}\]</span> 可见，<span class="math inline">\(\dot{R}(t)R(t)^{T}\)</span>是一个反对称矩阵。用符号<span class="math inline">\(^{\land}\)</span>表示反对称矩阵，设：<span class="math display">\[ a=\begin{bmatrix} a_{1} \\ a_{2} \\ a_{3} \end{bmatrix}\]</span> 则<span class="math inline">\(a^{\land}\)</span>为：<span class="math display">\[ a^{\land}=A=\begin{bmatrix} 0 &amp; -a_{3} &amp; a_{2} \\ a_{3} &amp; 0 &amp; -a_{1} \\ -a_{2} &amp; a_{1} &amp; 0 \end{bmatrix}\]</span> 同理：<span class="math display">\[A^{\vee}=a\]</span> 我们设一个三维向量<span class="math inline">\(\phi(t)\in\Re^{3}\)</span>与反对称矩阵<span class="math inline">\(\dot{R}(t)R(t)^{T}\)</span>对应，即：<span class="math display">\[\dot{R}(t)R(t)^{T}=\phi(t)^{\land}\]</span> 等式两边右乘<span class="math inline">\(R(t)\)</span>，由于<span class="math inline">\(R\)</span>为正交阵，得：<span class="math display">\[\dot{R}(t)=\phi(t)^{\land}R(t)=\begin{bmatrix}0 &amp; -\phi_{3} &amp; \phi_{2}\\\phi_{3}&amp;0&amp;-\phi_{1}\\-\phi_{2}&amp;\phi_{1}&amp;0\end{bmatrix}R(t)\]</span> 由此可见，每对<span class="math inline">\(R\)</span>求一次导数，只需左乘一个<span class="math inline">\(\phi^{\land}(t)\)</span>即可。<br />
当<span class="math inline">\(t=0\)</span>时，设此时<span class="math inline">\(R(0)=I\)</span>，把<span class="math inline">\(R(t)\)</span>在<span class="math inline">\(t=0\)</span>附近进行一阶泰勒展开得：<span class="math display">\[R(t)\approx R(t_{0})+\dot{R}(t_{0})(t-t_{0})=I+\phi(t_{0})^{\land}(t)\]</span> 可见，<span class="math inline">\(\phi\)</span>反映了<span class="math inline">\(R\)</span>的导数性质，所以称它为SO(3)原点附近的正切空间。同时在<span class="math inline">\(t_{0}\)</span>附近设<span class="math inline">\(\phi\)</span>保持为常数<span class="math inline">\(\phi(t_{0})=\phi_{0}\)</span>，故有：<span class="math display">\[\dot{R}(t)=\phi(t_{0})^{\land}R(t)=\phi_{0}^{\land}R(t)\]</span> 这是一个关于<span class="math inline">\(R\)</span>的微分方程，且初始值<span class="math inline">\(R(0)=I\)</span>，解得：<span class="math display">\[R(t)=exp(\phi_{0}^{\land}t)\]</span> 表明，旋转矩阵可由<span class="math inline">\(exp(\phi_{0}^{\land}t)\)</span>计算出来，给定某时刻的<span class="math inline">\(R\)</span>我们就能求得一个<span class="math inline">\(\phi\)</span>，它描述<span class="math inline">\(R\)</span>在局部的导数关系，<strong>这个<span class="math inline">\(\phi\)</span>正是对应到SO(3)上的李代数so(3)</strong>。</p>
<h2 id="李代数的定义">4. 李代数的定义</h2>
<p>每个李群都有其对应的李代数，一般李代数的定义如下： <img src="https://img-blog.csdnimg.cn/20210501060351439.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 二元运算[,]称为李括号，表达两元素的差异。举个例子，三维向量<span class="math inline">\(\Re^{3}\)</span>的叉积就是一种李括号，因此<span class="math inline">\(g=(\Re^{3},\Re,\times)\)</span>构成了一个李代数。</p>
<h2 id="李代数so3">5. 李代数so(3)</h2>
<p>由上面的结果可得，三维向量<span class="math inline">\(\phi\)</span>为李群SO(3)所对应的，定义在<span class="math inline">\(\Re^{3}\)</span>上的李代数，每个<span class="math inline">\(\phi\)</span>对应一个反对称矩阵：<span class="math display">\[\Phi=\phi(t)^{\land}R(t)=\begin{bmatrix}0 &amp; -\phi_{3} &amp; \phi_{2}\\\phi_{3}&amp;0&amp;-\phi_{1}\\-\phi_{2}&amp;\phi_{1}&amp;0\end{bmatrix}\in \Re^{3\times3}\]</span> 两个向量<span class="math inline">\(\phi_{1},\phi_{2}\)</span>的李括号为：<span class="math display">\[[\phi_{1},\phi_{2}]=(\Phi_{1}\Phi_{2}-\Phi_{2}\Phi_{1})^{\vee}\]</span> 它与SO(3)的关系由指数映射指定：<span class="math display">\[R=exp(\phi^{\land})\]</span></p>
<h2 id="李代数se3">6. 李代数se(3)</h2>
<p>SE(3)对应的李代数se(3)与so(3)类似，但其定义在<span class="math inline">\(\Re^{6}\)</span>空间中，如下： <img src="https://img-blog.csdnimg.cn/20210501232727577.png#pic_center" alt="在这里插入图片描述" /> 我们把每个se(3)的元素记作<span class="math inline">\(\xi\)</span>，表示一个六维向量，前三维表平移，但不同于变换矩阵的平移，记作<span class="math inline">\(\rho\)</span>，后三维表旋转，记作<span class="math inline">\(\phi\)</span>，其实就是上面提到的so(3)元素。se(3)表示如下（<strong>注：这里的<span class="math inline">\(\land\)</span>不再表示反对称矩阵，而是表示六维向量向四维矩阵的转换</strong>）： <span class="math display">\[\xi^{\land}=\begin{bmatrix}\phi^{\land}&amp;\rho\\0^{T}&amp;0\end{bmatrix}\]</span> (注：<span class="math inline">\(\land\)</span>和<span class="math inline">\(\vee\)</span>这里表示“从向量到矩阵”和“从矩阵到向量”，se(3)可理解成：由平移加上一个so(3)元素构成的向量)<br />
其李括号为：<span class="math display">\[[\xi_{1},\xi_{2}]=(\xi^{\land}_{1}\xi^{\land}_{2}-\xi^{\land}_{2}\xi^{\land}_{1})^{\vee}\]</span></p>
<h2 id="指数与对数映射">7. 指数与对数映射</h2>
<p>矩阵的指数如<span class="math inline">\(exp(\phi^{\land})\)</span>在李群李代数中称为指数映射。</p>
<h3 id="关于so3的映射">关于so(3)的映射</h3>
<p>那么如何计算<span class="math inline">\(exp(\phi^{\land})\)</span>呢？首先，任何矩阵<span class="math inline">\(A\)</span>的指数都可以表示成一个泰勒展开，但只有收敛的情况下才会有结果，结果如下：<span class="math display">\[exp(A)=\sum_{n=0}^{\infty}\frac{1}{n!}A^{n}\]</span> 即是对so(3)的元素有：<span class="math display">\[exp(\phi^{\land})=\sum_{n=0}^{\infty}\frac{1}{n!}(\phi^{\land})^{n}\]</span> 但要计算这个太复杂，因为有无穷次幂。所以一般情况下我们用下面的方法：<span class="math inline">\(\phi\)</span>是一个三维向量，我们定义其模长和方向，分别记为<span class="math inline">\(\theta\)</span>和<span class="math inline">\(a\)</span>，写作<span class="math inline">\(\phi=\theta a\)</span>，这里<span class="math inline">\(a\)</span>的模长为1.即: <span class="math inline">\(||a||=1\)</span>: <span class="math display">\[a = [a_{1}, a_{2}, a_{3}]^{T}\]</span> <span class="math display">\[a^{2}_{1}+a^{2}_{2}+a^{2}_{3}=1\]</span> 对于<span class="math inline">\(a^{\land}\)</span>有以下性质： <img src="https://img-blog.csdnimg.cn/20210502002859946.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 图里的两个式子提供了处理高阶<span class="math inline">\(a^{\land}\)</span>的方法，所以其指数映射可写成一个神奇的形式：（<em>最好自己写一遍</em>） <img src="https://img-blog.csdnimg.cn/20210503231438538.png#pic_center" alt="在这里插入图片描述" /> <img src="https://img-blog.csdnimg.cn/20210503231603885.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 这个式子与罗德里格斯公式如出一辙。</p>
<p>这里简单回顾下罗德里格斯公式（旋转向量到旋转矩阵的转化）： <span class="math display">\[R=cos\theta I+(1-cos\theta)nn^{T}+sin\theta n^{\land}\]</span> 两边取迹（即是求矩阵对角线元素之和）：<span class="math display">\[tr(R)=cos\theta tr(I)+(1-cos\theta)tr(nn^{T})+sin\theta tr(n^{\land})\\=3cos\theta+(1-cos\theta)=1+2cos\theta\]</span> 所以角度为： <span class="math display">\[\theta=arc cos\frac{tr(R)-1}{2} \]</span> 这表明，so(3)实际就是由旋转向量组成的空间，而指数映射即是罗德里格斯公式。<br />
反之，通过对数映射也可以将SO(3)的元素对应到so(3)中： <img src="https://img-blog.csdnimg.cn/202105032347198.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 对这个式子的求解也没必要用泰勒展开，而是可以通过迹的性质分别求解转角和转轴（上面回顾罗德里格斯公式的时候已经一起列出来了）。<br />
最后需要提一下：每个SO(3)中的元素都可以找到一个so(3)元素与之对应，但一个so(3)元素却可能有多个SO(3)中的元素，直观一点理解的话：比如旋转360度会对应一个周期。</p>
<h3 id="关于se3的映射待补充">关于se(3)的映射（待补充。。。）</h3>
<p>与so(3)的过程类似，所以这里主要给结论（<strong>这个过程没具体算，之后补上。</strong>）。<br />
se(3)的指数映射形式如下： <img src="https://img-blog.csdnimg.cn/20210504000629746.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 同样令<span class="math inline">\(\phi=\theta a\)</span>，<span class="math inline">\(a\)</span>为单位向量，则： <img src="https://img-blog.csdnimg.cn/20210504001031241.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 从结果来看，<span class="math inline">\(\xi\)</span>的指数映射左上角的<span class="math inline">\(R\)</span>就是上面的SO(3)，与se(3)中的旋转部分<span class="math inline">\(\phi\)</span>对应，而右上角的<span class="math inline">\(J\)</span>由上面图里的推导给出，即： <img src="https://img-blog.csdnimg.cn/20210504001730154.png#pic_center" alt="在这里插入图片描述" /></p>
<h3 id="映射表">映射表*</h3>
<p>这里给出SO(3), SE(3), so(3), se(3)的对应映射表 <img src="https://img-blog.csdnimg.cn/20210507171027871.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /></p>
<p><strong><em>本文主要参考《视觉SLAM十四讲》</em></strong></p>
]]></content>
      <categories>
        <category>视觉SLAM相关</category>
      </categories>
  </entry>
  <entry>
    <title>学习记录_Computer Vision1_作业1_(2) Projective Transformation</title>
    <url>/posts/1.html</url>
    <content><![CDATA[<h1 id="projective-transformation">（2）Projective Transformation</h1>
<span id="more"></span>
<p>作业要求： <img src="https://img-blog.csdnimg.cn/2020121818034961.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 简单总结一下，就是说：给２Ｄ点核其他一些信息重构回３Ｄ图。然后再回构２Ｄ图。</p>
<h2 id="步骤1">步骤1：</h2>
<p>笛卡尔坐标与转齐次坐标的相互转换，代码如下： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def cart2hom(points):</span><br><span class="line">  &quot;&quot;&quot; Transforms from cartesian to homogeneous coordinates.</span><br><span class="line"></span><br><span class="line">  Args:</span><br><span class="line">    points: a np array of points in cartesian coordinates</span><br><span class="line"></span><br><span class="line">  Returns:</span><br><span class="line">    points_hom: a np array of points in homogeneous coordinates</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">  one = np.ones((1,points.shape[1]))</span><br><span class="line">  points_hom = np.r_[points, one]       #Add 1 to the last line</span><br><span class="line"></span><br><span class="line">  return points_hom</span><br></pre></td></tr></table></figure> <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def hom2cart(points):</span><br><span class="line">  &quot;&quot;&quot; Transforms from homogeneous to cartesian coordinates.</span><br><span class="line"></span><br><span class="line">  Args:</span><br><span class="line">    points: a np array of points in homogenous coordinates</span><br><span class="line"></span><br><span class="line">  Returns:</span><br><span class="line">    points_hom: a np array of points in cartesian coordinates</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">  points_cart = np.delete(points/points[-1], points.shape[0]-1, axis=0)    #Delete the last line</span><br><span class="line">  return points_cart</span><br></pre></td></tr></table></figure> 这里补充一点关于齐次坐标的东西...<br />
（本来想自己总结的，但偶然看到一篇写得实在好，就在这里给个<a href="https://blog.csdn.net/zhuiqiuzhuoyue583/article/details/95228010">链接</a>了）<br />
（这里简单总结总结下，当作唤醒记忆版：常见的笛卡尔坐标没办法表示无穷远，所以在n维笛卡尔坐标中添加一维变成n+1维，以二维为例，原来的(X, Y)变成(x,y,w)，其中 X=x/w，Y=y/w，当w=0时即可表示笛卡尔中得无穷远处。引进齐次坐标可把坐标的旋转平移放缩等操作用一个矩阵表示。）</p>
<h2 id="步骤2">步骤2：</h2>
<p>3D坐标的平移变换。齐次坐标的平移可如下表示： <img src="https://img-blog.csdnimg.cn/20201219195822542.png#pic_center" alt="在这里插入图片描述" /> 左边矩阵为平移后的齐次坐标，vx, vy, vz为平移分量。<br />
代码如下： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def gettranslation(v):</span><br><span class="line">  &quot;&quot;&quot; Returns translation matrix T in homogeneous coordinates for translation by v.</span><br><span class="line"></span><br><span class="line">  Args:</span><br><span class="line">    v: 3d translation vector</span><br><span class="line"></span><br><span class="line">  Returns:</span><br><span class="line">    T: translation matrix in homogeneous coordinates</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  # [x,y,z] --&gt; [x,y,z,1]</span><br><span class="line">  T = np.array([[1,0,0,v[0]],[0,1,0,v[1]],[0,0,1,v[2]],[0,0,0,1]])</span><br><span class="line"></span><br><span class="line">  return T</span><br></pre></td></tr></table></figure></p>
<h2 id="步骤3">步骤3：</h2>
<p>获得x, y, z 的旋转矩阵，三个轴的旋转矩阵都在代码的注释里给出了，代码如下： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def getxrotation(d):</span><br><span class="line">  &quot;&quot;&quot; Returns rotation matrix Rx in homogeneous coordinates for a rotation of d degrees around the x axis.</span><br><span class="line"></span><br><span class="line">  Args:</span><br><span class="line">    d: degrees of the rotation</span><br><span class="line"></span><br><span class="line">  Returns:</span><br><span class="line">    Rx: rotation matrix</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  # R_x: [1      0      0   ]</span><br><span class="line">  #      [0   cos(d)  sin(d)]</span><br><span class="line">  #      [0  -sin(d)  cos(d)]</span><br><span class="line">  n_x = np.array([[0,0,0],[0,0,-1],[0,1,0]])</span><br><span class="line">  R_x = np.eye(3) + np.sin(d*np.pi/180)*n_x + (1 - np.cos(d*np.pi/180))*n_x.dot(n_x)</span><br><span class="line">  Rx = np.c_[R_x, [0,0,0]]</span><br><span class="line">  # matrix in homogeneous coordinates</span><br><span class="line">  Rx = np.r_[Rx, [[0,0,0,1]]]</span><br><span class="line">  return Rx</span><br></pre></td></tr></table></figure> <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def getyrotation(d):</span><br><span class="line">  &quot;&quot;&quot; Returns rotation matrix Ry in homogeneous coordinates for a rotation of d degrees around the y axis.</span><br><span class="line"></span><br><span class="line">  Args:</span><br><span class="line">    d: degrees of the rotation</span><br><span class="line"></span><br><span class="line">  Returns:</span><br><span class="line">    Ry: rotation matrix</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  # R_y: [ cos(d)  0  sin(d)]</span><br><span class="line">  #      [   0     1     0  ]</span><br><span class="line">  #      [-sin(d)  0  cos(d)]</span><br><span class="line">  n_y = np.array([[0,0,1],[0,0,0],[-1,0,0]])</span><br><span class="line">  R_y = np.eye(3) + np.sin(d*np.pi/180)*n_y + (1 - np.cos(d*np.pi/180))*n_y.dot(n_y)</span><br><span class="line">  Ry = np.c_[R_y, [0,0,0]]</span><br><span class="line">  # matrix in homogeneous coordinates</span><br><span class="line">  Ry = np.r_[Ry, [[0,0,0,1]]]</span><br><span class="line">  return Ry</span><br></pre></td></tr></table></figure> <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def getzrotation(d):</span><br><span class="line">  &quot;&quot;&quot; Returns rotation matrix Rz in homogeneous coordinates for a rotation of d degrees around the z axis.</span><br><span class="line"></span><br><span class="line">  Args:</span><br><span class="line">    d: degrees of the rotation</span><br><span class="line"></span><br><span class="line">  Returns:</span><br><span class="line">    Rz: rotation matrix</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  # R_z: [cos(d)  -sin(d)  0]</span><br><span class="line">  #      [sin(d)   cos(d)  0]</span><br><span class="line">  #      [  0       0      1]</span><br><span class="line">  n_z = np.array([[0,-1,0],[1,0,0],[0,0,0]])</span><br><span class="line">  R_z = np.eye(3) + np.sin(d*np.pi/180)*n_z + (1 - np.cos(d*np.pi/180))*n_z.dot(n_z)</span><br><span class="line">  Rz = np.c_[R_z, [0,0,0]]</span><br><span class="line">  # matrix in homogeneous coordinates</span><br><span class="line">  Rz = np.r_[Rz, [[0,0,0,1]]]</span><br><span class="line">  return Rz</span><br></pre></td></tr></table></figure> 这里再简单说一下绕其它轴旋转的情况：<br />
绕其它轴旋转的话可以用轴角公式（Rodrigues' rotation），通过给定旋转轴和角度可计算出旋转后的向量。这里依旧给个<a href="https://www.bilibili.com/video/BV1h7411c7zK?p=1">链接</a>，B站的一个视频。</p>
<h2 id="步骤4">步骤4：</h2>
<p>然后中心投影到ｘｙ平面上，主点为[8，10]，焦距为8，像素为正方形。 我们最终获得了图像平面中的２D点。<br />
代码如下： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def getcentralprojection(principal, focal):</span><br><span class="line">  &quot;&quot;&quot; Returns the (3 x 4) matrix L that projects homogeneous camera coordinates on homogeneous</span><br><span class="line">  image coordinates depending on the principal point and focal length.</span><br><span class="line">  </span><br><span class="line">  Args:</span><br><span class="line">    principal: the principal point, 2d vector</span><br><span class="line">    focal: focal length</span><br><span class="line"></span><br><span class="line">  Returns:</span><br><span class="line">    L: central projection matrix</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  # central: [f_x  0  u_0  0]</span><br><span class="line">  #          [ 0  f_y v_0  0]</span><br><span class="line">  #          [ 0   0   1   0]</span><br><span class="line">  square_L = np.array([[focal,0,principal[0]],[0,focal,principal[1]],[0,0,1]])</span><br><span class="line">  I = np.eye(3)</span><br><span class="line">  I_0 = np.c_[I, [0,0,0]]   #[I|0]</span><br><span class="line">  L = square_L.dot(I_0)</span><br><span class="line">  return L</span><br></pre></td></tr></table></figure> 简单说明下：<br />
f_x和f_y是焦距，在这里都是8，u_0和v_0是中心点坐标，中心点坐标一般是取图像中点，比如，如果图像宽和高分别是W和H的话，中心点为（W/2，H/2）.</p>
<h2 id="步骤5">步骤5：</h2>
<p>将上面的平移，旋转，中心投影矩阵写在一起，如下： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def getfullprojection(T, Rx, Ry, Rz, L):</span><br><span class="line">  &quot;&quot;&quot; Returns full projection matrix P and full extrinsic transformation matrix M.</span><br><span class="line"></span><br><span class="line">  Args:</span><br><span class="line">    T: translation matrix</span><br><span class="line">    Rx: rotation matrix for rotation around the x-axis</span><br><span class="line">    Ry: rotation matrix for rotation around the y-axis</span><br><span class="line">    Rz: rotation matrix for rotation around the z-axis</span><br><span class="line">    L: central projection matrix</span><br><span class="line"></span><br><span class="line">  Returns:</span><br><span class="line">    P: projection matrix</span><br><span class="line">    M: matrix that summarizes extrinsic transformations</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  # The order is: y-axis first, then x-axis, and finally z-axis</span><br><span class="line">  # R = R_xR_yR_z</span><br><span class="line">  # extrinsic transformations:</span><br><span class="line">  # M = [R T]</span><br><span class="line">  #     [0 1]</span><br><span class="line">  # matrix in homogeneous coordinates:</span><br><span class="line">  M = Rz.dot(Rx.dot(Ry.dot(T)))			  # [4,4]</span><br><span class="line">  # P = L.M</span><br><span class="line">  P = L.dot(M)            # [3,4]</span><br><span class="line">  return P, M</span><br></pre></td></tr></table></figure> <strong>这里要注意计算矩阵M的顺序！！！！！！！</strong> 这里贴一张图方便理解： <img src="https://img-blog.csdnimg.cn/20201223221703759.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> x<sup>I</sup> 是图像坐标，x<sup>C</sup> 是摄像机坐标，x<sup>W</sup>是世界3D坐标，K·[I | 0]是代码里的投影矩阵L。</p>
<h2 id="步骤6">步骤6：</h2>
<p>借助上面得出的转换矩阵将世界３D坐标转化为图像２D坐标，代码如下： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def projectpoints(P, X):</span><br><span class="line">  &quot;&quot;&quot; Apply full projection matrix P to 3D points X in cartesian coordinates.</span><br><span class="line"></span><br><span class="line">  Args:</span><br><span class="line">    P: projection matrix</span><br><span class="line">    X: 3d points in cartesian coordinates</span><br><span class="line"></span><br><span class="line">  Returns:</span><br><span class="line">    x: 2d points in cartesian coordinates</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  X = cart2hom(X)</span><br><span class="line">  x = P.dot(X)        #[3,2904] &amp; [x,y,1]</span><br><span class="line">  x = hom2cart(x)     #[2,2904] &amp; [x,y]</span><br><span class="line">  return x</span><br></pre></td></tr></table></figure></p>
<h2 id="步骤7">步骤7：</h2>
<p>一些常规的加载操作。<br />
加载2D点： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">def loadpoints():</span><br><span class="line">  &quot;&quot;&quot; Load 2D points from obj2d.npy.</span><br><span class="line"></span><br><span class="line">  Returns:</span><br><span class="line">    x: np array of points loaded from obj2d.npy</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  x = np.load(&#x27;data/obj2d.npy&#x27;)       #[2,2904]&amp;[x,y]</span><br><span class="line">  return x</span><br></pre></td></tr></table></figure> 加载z坐标点： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def loadz():</span><br><span class="line">  &quot;&quot;&quot; Load z-coordinates from zs.npy.</span><br><span class="line"></span><br><span class="line">  Returns:</span><br><span class="line">    z: np array containing the z-coordinates</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">  z = np.load(&#x27;data/zs.npy&#x27;)         #[1,2904]&amp;[z]</span><br><span class="line">  return z</span><br></pre></td></tr></table></figure></p>
<h2 id="步骤8">步骤8：</h2>
<p>通过投影矩阵以及z方向的坐标，将上面的2D点显示为3D点，代码如下： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def invertprojection(L, P2d, z):</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  Invert just the projection L of cartesian image coordinates P2d with z-coordinates z.</span><br><span class="line"></span><br><span class="line">  Args:</span><br><span class="line">    L: central projection matrix</span><br><span class="line">    P2d: 2d image coordinates of the projected points</span><br><span class="line">    z: z-components of the homogeneous image coordinates</span><br><span class="line"></span><br><span class="line">  Returns:</span><br><span class="line">    P3d: 3d cartesian camera coordinates of the points</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  # square_L = np.delete(L, 3, axis=1)   #(3,3)</span><br><span class="line">  # L^-1</span><br><span class="line">  pinv_L = np.linalg.pinv(L)</span><br><span class="line">  # [x,y](2,2904) --&gt; [x,y,1](3,2904)</span><br><span class="line">  hom_point_xy = cart2hom(P2d)  </span><br><span class="line">  for i in range(P2d.shape[1]):</span><br><span class="line">    hom_point_xy[:,i] = hom_point_xy[:,i]*z[:,i]</span><br><span class="line">  hom_point_xy = pinv_L.dot(hom_point_xy) </span><br><span class="line"></span><br><span class="line">  # [x,y,z]</span><br><span class="line">  P3d = np.delete(hom_point_xy, 3, axis = 0)        </span><br><span class="line">  return P3d</span><br></pre></td></tr></table></figure> 效果如下： <img src="https://img-blog.csdnimg.cn/20201224033516517.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 而原来2D点的图则如下： <img src="https://img-blog.csdnimg.cn/2020122403354922.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /></p>
<h2 id="步骤9">步骤9：</h2>
<p>获得外部转化后的3D点的笛卡尔坐标，以便用步骤6的代码获得3D点对应的２D点（步骤写得有点乱，不过不改了。。。。），代码如下： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def inverttransformation(M, P3d):</span><br><span class="line">  &quot;&quot;&quot; Invert just the model transformation in homogeneous coordinates</span><br><span class="line">  for the 3D points P3d in cartesian coordinates.</span><br><span class="line"></span><br><span class="line">  Args:</span><br><span class="line">    M: matrix summarizing the extrinsic transformations</span><br><span class="line">    P3d: 3d points in cartesian coordinates</span><br><span class="line"></span><br><span class="line">  Returns:</span><br><span class="line">    X: 3d points after the extrinsic transformations have been reverted</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  hom_P3d = cart2hom(P3d)      #(4,2904)(x,y,z,1)</span><br><span class="line">  inv_M = np.linalg.inv(M)    #Inverse matrix of M</span><br><span class="line">  P3d = inv_M.dot(hom_P3d)                   </span><br><span class="line">  </span><br><span class="line">  return P3d       #(4,2904)</span><br></pre></td></tr></table></figure></p>
<h2 id="步骤10">步骤10：</h2>
<p>对以上函数的引用： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def problem3():</span><br><span class="line">    &quot;&quot;&quot;Example code implementing the steps in Problem 3&quot;&quot;&quot;</span><br><span class="line">    t = np.array([-27.1, -2.9, -3.2])</span><br><span class="line">    principal_point = np.array([8, -10])</span><br><span class="line">    focal_length = 8</span><br><span class="line"></span><br><span class="line">    # model transformations</span><br><span class="line">    T = gettranslation(t)</span><br><span class="line">    Ry = getyrotation(135)</span><br><span class="line">    Rx = getxrotation(-30)</span><br><span class="line">    Rz = getzrotation(90)</span><br><span class="line">    print(T)</span><br><span class="line">    print(Ry)</span><br><span class="line">    print(Rx)</span><br><span class="line">    print(Rz)</span><br><span class="line"></span><br><span class="line">    K = getcentralprojection(principal_point, focal_length)</span><br><span class="line"></span><br><span class="line">    P,M = getfullprojection(T, Rx, Ry, Rz, K)</span><br><span class="line">    print(P)</span><br><span class="line">    print(M)</span><br><span class="line"></span><br><span class="line">    points = loadpoints()</span><br><span class="line">    #displaypoints2d(points)</span><br><span class="line"></span><br><span class="line">    z = loadz()</span><br><span class="line">    Xt = invertprojection(K, points, z)</span><br><span class="line"></span><br><span class="line">    Xh = inverttransformation(M, Xt)</span><br><span class="line"></span><br><span class="line">    worldpoints = hom2cart(Xh)</span><br><span class="line">    displaypoints3d(worldpoints)</span><br><span class="line"></span><br><span class="line">    points2 = projectpoints(P, worldpoints)</span><br><span class="line">    displaypoints2d(points2)</span><br><span class="line"></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></p>
<h1 id="总结">总结：</h1>
<p>最大收获是弄懂了齐次坐标，然后就是一些杂七杂八的变换矩阵</p>
<h1 id="剩下待完善的">剩下待完善的：</h1>
<ol type="1">
<li>关于摄像机矩阵，包括镜头畸变，本科毕设的时候有涉及到，但现在忘得差不多了，纠结下有没有必要单独总结一篇。。。</li>
</ol>
]]></content>
      <categories>
        <category>学习总结</category>
      </categories>
  </entry>
  <entry>
    <title>学习记录_Computer Vision1_作业1_(1) Bayer Interpolation</title>
    <url>/posts/2.html</url>
    <content><![CDATA[<p>这次作业需要记录的有三道题，包括：</p>
<blockquote>
<ol type="1">
<li>Bayer Interpolation<br />
</li>
<li>Projective Transformation<br />
</li>
<li>Image Filtering and Edge Detection</li>
</ol>
</blockquote>
<h1 id="bayer-interpolation">（1）Bayer Interpolation</h1>
<span id="more"></span>
<p>作业要求： <img src="https://img-blog.csdnimg.cn/20201214062515791.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 简单来说，即是给出一张Bayer图片，将其转化为RGB图。<br />
作业给出的图片如下：<img src="https://img-blog.csdnimg.cn/20201214185058357.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /></p>
<p><strong>这里先对Bayer图进行简要说明。</strong><br />
与一般RGB图不同，Bayer图的像素分布如下图所示： <img src="https://img-blog.csdnimg.cn/20201214063319235.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /></p>
<p>在Bayer图中，每个像素点都是由单一颜色所控制，而且绿色所占的比例是其他两种颜色的两倍，所以整张图片看起来会偏绿，我们所要做的就是通过插值将每个像素点的其他两种颜色算出来。</p>
<h2 id="步骤1">步骤1：</h2>
<p>将Bayer数据分成RGB通道，以便每个颜色通道仅保留Bayer模式给定的相应值，而缺失值则用零填充。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">separatechannels</span>(<span class="params">bayerdata</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Separate bayer data into RGB channels so that</span></span><br><span class="line"><span class="string">    each color channel retains only the respective</span></span><br><span class="line"><span class="string">    values given by the bayer pattern and missing values</span></span><br><span class="line"><span class="string">    are filled with zero</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        Numpy array containing bayer data (H,W)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        red, green, and blue channel as numpy array (H,W)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    H, W = bayerdata.shape</span><br><span class="line">    r = np.zeros((H, W))</span><br><span class="line">    g = np.zeros((H, W))</span><br><span class="line">    b = np.zeros((H, W))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(H):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(W):</span><br><span class="line">            <span class="keyword">if</span>((i + j) % <span class="number">2</span> == <span class="number">0</span>):</span><br><span class="line">                g[i][j] = bayerdata[i][j]</span><br><span class="line">            <span class="keyword">elif</span>(i % <span class="number">2</span> == <span class="number">0</span> <span class="keyword">and</span> (i + j) % <span class="number">2</span> != <span class="number">0</span>):</span><br><span class="line">                r[i][j] = bayerdata[i][j]</span><br><span class="line">            <span class="keyword">elif</span>(i % <span class="number">2</span> != <span class="number">0</span> <span class="keyword">and</span> (i + j) % <span class="number">2</span> != <span class="number">0</span>):</span><br><span class="line">                b[i][j] = bayerdata[i][j]</span><br><span class="line">    <span class="keyword">return</span> r, g, b</span><br></pre></td></tr></table></figure>
<h2 id="步骤2">步骤2：</h2>
<p>将单独的通道组合成图像。 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">assembleimage</span>(<span class="params">r, g, b</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Assemble separate channels into image</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        red, green, blue color channels as numpy array (H,W)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Image as numpy array (H,W,3)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    H, W = r.shape</span><br><span class="line">    imgData = np.zeros((H, W, <span class="number">3</span>))</span><br><span class="line">    imgData[:,:,<span class="number">0</span>] = r[:,:]</span><br><span class="line">    imgData[:,:,<span class="number">1</span>] = g[:,:]</span><br><span class="line">    imgData[:,:,<span class="number">2</span>] = b[:,:]</span><br><span class="line">    <span class="keyword">return</span> imgData</span><br></pre></td></tr></table></figure> 结果如下图： <img src="https://img-blog.csdnimg.cn/20201218023313111.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /></p>
<h2 id="步骤3">步骤3：</h2>
<p>通过使用双线性插值对Bayer模式中的缺失值进行插值。<br />
双线性插值，简单来说就是通过邻近4个点平均出中间点的数值。<br />
其实主要就是计算R,G,B三通道的卷积核。说之前先解释一个函数：scipy.ndimage.filters.convolve，（python的各种库里有非常多用于卷积的函数，改天做个对比总结）。</p>
<p><strong>scipy.ndimage.filters.convolve：</strong><br />
(这里给个 <a href="https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.filters.convolve.html/">链接</a>，说得很详细了已经)<br />
<em>scipy.ndimage.filters.convolve(input, weights, output=None, mode='reflect', cval=0.0, origin=0)</em></p>
<pre><code>    input: 待卷积的图片     
    weight: 卷积核       
    output:（这个未知，好像一直没用过这个，（待定））   
    mode: 边缘处理，有5种（关于各个边缘模式的选用现在还未理解）

    ‘reflect’ (d c b a | a b c d | d c b a)
    ‘constant’ (k k k k | a b c d | k k k k)
    ‘nearest’ (a a a a | a b c d | d d d d)
    ‘mirror’ (d c b | a b c d | c b a)
    ‘wrap’ (a b c d | a b c d | a b c d)</code></pre>
<blockquote>
<p>cval：默认是0，当mode取constant时有效，这个的取值既是边缘外补什么值<br />
origin：默认0，应该是控制weight在input上的位置的，目前也还没用过</p>
</blockquote>
<p>这里有一个要特别注意的是，<strong>卷积核不是直接对应相乘然后相加，而是要先翻转</strong>，举个例子，比如一个卷积核设为： <img src="https://img-blog.csdnimg.cn/20201218094517319.png#pic_center" alt="在这里插入图片描述" /> 但实际上并不是上面这个与图片卷积，而是下面这个卷积核： <img src="https://img-blog.csdnimg.cn/20201218094610325.png#pic_center" alt="在这里插入图片描述" /> 这点在初始化卷积核的时候要注意（不过目前遇到的好像都是中心对称的）</p>
<p>回到题里，G通道对应的矩阵如下图： <img src="https://img-blog.csdnimg.cn/20201218104911416.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /> 按照双线性插值，很容易能得到卷积核四个角的值，又由于应该把原本的像素值保留下来，再根据边界插值条件，就可以确定出通道G的卷积核，其他两个通道同理可各自获得卷积核值。<br />
代码如下： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">interpolate</span>(<span class="params">r, g, b</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Interpolate missing values in the bayer pattern</span></span><br><span class="line"><span class="string">    by using bilinear interpolation</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        red, green, blue color channels as numpy array (H,W)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Interpolated image as numpy array (H,W,3)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    r_weight = np.array([[<span class="number">0.25</span>,<span class="number">0.5</span>,<span class="number">0.25</span>],[<span class="number">0.5</span>,<span class="number">1</span>,<span class="number">0.5</span>],[<span class="number">0.25</span>,<span class="number">0.5</span>,<span class="number">0.25</span>]])</span><br><span class="line">    g_weight = np.array([[<span class="number">0</span>,<span class="number">0.25</span>,<span class="number">0</span>],[<span class="number">0.25</span>,<span class="number">1</span>,<span class="number">0.25</span>],[<span class="number">0</span>,<span class="number">0.25</span>,<span class="number">0</span>]])</span><br><span class="line">    b_weight = np.array([[<span class="number">0.25</span>,<span class="number">0.5</span>,<span class="number">0.25</span>],[<span class="number">0.5</span>,<span class="number">1</span>,<span class="number">0.5</span>],[<span class="number">0.25</span>,<span class="number">0.5</span>,<span class="number">0.25</span>]])</span><br><span class="line"></span><br><span class="line">    new_r = convolve(r, r_weight, mode=<span class="string">&#x27;mirror&#x27;</span>)</span><br><span class="line">    new_g = convolve(g, g_weight, mode=<span class="string">&#x27;mirror&#x27;</span>)</span><br><span class="line">    new_b = convolve(b, b_weight, mode=<span class="string">&#x27;mirror&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#print(new_g)</span></span><br><span class="line">    <span class="comment">#print(new_r)</span></span><br><span class="line">    <span class="comment">#print(new_b)</span></span><br><span class="line"></span><br><span class="line">    H, W = new_r.shape</span><br><span class="line">    new_img = np.zeros((H, W, <span class="number">3</span>))</span><br><span class="line">    new_img[:,:,<span class="number">0</span>] = new_r[:,:]</span><br><span class="line">    new_img[:,:,<span class="number">1</span>] = new_g[:,:]</span><br><span class="line">    new_img[:,:,<span class="number">2</span>] = new_b[:,:]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> new_img</span><br></pre></td></tr></table></figure> 最后就是对以上函数的调用，如下： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">problem2</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Example code implementing the steps in Problem 2</span></span><br><span class="line"><span class="string">    Note: uses display_image() from Problem 1&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    data = loaddata(<span class="string">&quot;data/bayerdata.npy&quot;</span>)</span><br><span class="line">    r, g, b = separatechannels(data)</span><br><span class="line"></span><br><span class="line">    img = assembleimage(r, g, b)</span><br><span class="line">    display_image(img)</span><br><span class="line"></span><br><span class="line">    img_interpolated = interpolate(r, g, b)</span><br><span class="line">    display_image(img_interpolated)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment">#problem1()</span></span><br><span class="line">    problem2()</span><br><span class="line">    <span class="comment">#problem3()</span></span><br><span class="line">    <span class="comment">#problem4()   </span></span><br></pre></td></tr></table></figure> 最后得到的图片如下： <img src="https://img-blog.csdnimg.cn/20201218115816873.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzUwMTg4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /></p>
<h1 id="总结">总结：</h1>
<p>本次作业及该篇学习记录遗留下的问题：</p>
<blockquote>
<ol type="1">
<li>卷积的mode是“mirror”，原因未明；<br />
</li>
<li>要对python里各个库的卷积函数进行总结；<br />
</li>
<li>这次作业用到的卷积函数的output以及origin参数</li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>学习总结</category>
      </categories>
  </entry>
  <entry>
    <title>四元数（三维旋转）</title>
    <url>/posts/3.html</url>
    <content><![CDATA[<h1 id="四元数的基本知识">四元数的基本知识：</h1>
<p>与复数类似，四元数也由实部和虚部组成，但四元数有三个虚部，通常表示为：<span class="math display">\[q=q_{0} + q_{1}i + q_{2}j + q_{3}k\]</span> 或: <span class="math display">\[ q=[s, v]^{T}, 其中: s=q_{0} \in R, v=[q_{1},q_{2},q_{3}]^{T} \in R^{3}\]</span> <span id="more"></span></p>
<p>这三个虚部满足下面关系： <img src="https://img-blog.csdnimg.cn/20210402045509774.png#pic_center" /> 在复数中，乘以一个 <span class="math inline">\(i\)</span> 相当于旋转90度，但四元数中不同，乘以<span class="math inline">\(i\)</span> 表示对应旋转180度，这样才能保证 <span class="math inline">\(ij = k\)</span>，而<span class="math inline">\(i^2 = -1\)</span>表明，旋转360度后会得到一个相反的东西，<strong>要旋转两周才能回到原来的样子</strong>（看着挺反常识的。。。。。。）</p>
<h1 id="四元数的运算">四元数的运算</h1>
<p>(这里介绍加减，乘，模，共轭，逆，数乘)<br />
现定义两个四元数<span class="math display">\[q_{a}= [s_{a},v_{a}]^{T}=s_{a}+x_{a}i+y_{a}j+z_{a}k\]</span> <span class="math display">\[q_{b}= [s_{b},v_{b}]^{T}=s_{b}+x_{b}i+y_{b}j+z_{b}k  \]</span></p>
<h2 id="加减">1).加减</h2>
<p><span class="math display">\[q_{a} \pm q_{b} = [s_{a} \pm s_{b}, v_{a} \pm v_{b}]^{T}\]</span></p>
<h2 id="乘法">2). 乘法</h2>
<p>乘法是把<span class="math inline">\(q_{a}\)</span>每一项都与<span class="math inline">\(q_{b}\)</span>相乘，虚部部分的运算要按照上面图1的规则计算： <span class="math display">\[q_{a}q_{b}=s_{a}s_{b}-x_{a}x_{b}-y_{a}y_{b}-z_{a}z_{b}\\+(s_{a}x_{b}+x_{a}s_{b}+y_{a}z_{b}-z_{a}y_{b})i\\ 
+(s_{a}y_{b}-x_{a}z_{b}+y_{a}s_{b}+z_{a}x_{b})j\\ 
+(s_{a}z_{b}+x_{a}y_{b}-y_{a}x_{b}+z_{a}s_{b})k\]</span> (看着挺复杂，但写成向量内外积的形式就会简洁很多: ) <span class="math display">\[q_{a}q_{b}=[s_{a}s_{b}-v_{a}^{T}v_{b},s_{a}v_{b}+s_{b}v_{a}+v_{a} \times v_{b}]^{T}\]</span> (由于最后一项外积的存在，这个乘法通常是不可交换的，除非<span class="math inline">\(v_{a},v_{b}\)</span>在<span class="math inline">\(R^{3}\)</span>中共线，此时外积项为0.)</p>
<h2 id="模长">3). 模长</h2>
<p>四元数的模长定义为： <span class="math display">\[||q_{a}|| = \sqrt{s^{2}_{a}+x^{2}_{a}+y^{2}_{a}+z_{a}^{2}}\]</span> 两个四元数乘积的模等于模的乘积： <span class="math display">\[||q_{a}q_{b}|| =\parallel q_{a}\parallel \parallel q_{b}\parallel\]</span></p>
<h2 id="共轭">4). 共轭</h2>
<p>即是把虚部取相反数： <span class="math display">\[q_{a}^{*}= [s_{a},-v_{a}]^{T}=s_{a}-x_{a}i-y_{a}j-z_{a}k\]</span> 四元数共轭与其本身相乘会得到一个实四元数，其实部等于模长的平方： <span class="math display">\[q_{a}q^{*}_{a}=q_{a}^{*}q_{a}=[s_{a}^{2}+v^{T}v,0]^{T}\]</span></p>
<h2 id="逆">5). 逆</h2>
<p>四元数的逆为： <span class="math display">\[q^{-1}=q^{*}/ \parallel q \parallel ^{2}\]</span> 若<span class="math inline">\(q\)</span>为单位四元数，则其逆和共轭就是同一个量。另外，乘积的逆具有和矩阵相似的性质： <span class="math display">\[ (q_{a}q_{b})^{-1}=q^{-1}_{b}q^{-1}_{a}\]</span></p>
<h2 id="数乘">6). 数乘</h2>
<p><span class="math display">\[kq_{a} = [ks_{a},kv_{a}]^{T}\]</span></p>
<h1 id="旋转表示">旋转表示</h1>
<p>首先有一个待旋转点<span class="math inline">\(p\)</span>，表示成一个纯四元数为：<span class="math display">\[ p=[0,x,y,z]^{T}=[0,v]^{T}\]</span>其三个虚部分别对应<span class="math inline">\(x,y,z\)</span>三个坐标轴，旋转四元数表示为<span class="math inline">\(q\)</span>，是一个单位四元数。对于旋转后的点<span class="math inline">\(p&#39;\)</span>表示如下：<span class="math display">\[ p&#39;=qp\]</span> 对于旋转有如下要求：</p>
<blockquote>
<ol type="1">
<li>旋转前后的模应相等；<br />
</li>
<li>旋转后的点<span class="math inline">\(p&#39;\)</span>应该也为纯四元数。</li>
</ol>
</blockquote>
<p>由于<span class="math inline">\(q\)</span>为单位四元数，模为<span class="math inline">\(1\)</span>，对应上面四元数模的性质可知<span class="math inline">\(p&#39;\)</span>满足条件1，但对于条件2则只有当<span class="math inline">\(p\)</span>和<span class="math inline">\(q\)</span>虚部正交时才能满足（原因的话网上有很多篇文章都有对其展开计算，这里就懒得写了......）。<br />
一般情况下会把旋转表示成下面这种形似：<span class="math display">\[p&#39;=qpq^{-1}\]</span> （有些地方这个逆也会用共轭代替，这无所谓，因为<span class="math inline">\(q\)</span>是单位四元数）<br />
至于为什么是这种形式，因为Hamilton发现，如果右乘一个<span class="math inline">\(p\)</span>的逆，则可以同时满足上面两个条件（可具体展开乘一下或者直接用特殊值法）。</p>
<h1 id="四元数与其它旋转方式之间的转换">四元数与其它旋转方式之间的转换</h1>
<p>任意一个单位四元数可以表示一个旋转，这个旋转也可以用旋转矩阵或者旋转向量来表示，这里主要讨论这三者的关系。  <br />
（现在把四元数乘法表示为矩阵乘法）<br />
设<span class="math inline">\(q=[s,v]^{T}\)</span>，首先定义两种新的符号<span class="math inline">\(^{+}\)</span>和<span class="math inline">\(^{\oplus}\)</span>。<span class="math display">\[ q^{+}=\begin{bmatrix} s &amp; -v^{T} \\ v &amp; sI+v^{\land} \end{bmatrix}\]</span> <span class="math display">\[ q^{\oplus}=\begin{bmatrix} s &amp; -v^{T} \\ v &amp; sI-v^{\land} \end{bmatrix}\]</span> 其中的符号<span class="math inline">\(^{\land}\)</span>表示反对称矩阵，即：<span class="math inline">\(A&#39;=-A\)</span>，举个例子，设：<span class="math display">\[ a=\begin{bmatrix} a_{1} \\ a_{2} \\ a_{3} \end{bmatrix}\]</span> 则<span class="math inline">\(a^{\land}\)</span>为：<span class="math display">\[ a^{\land}=\begin{bmatrix} 0 &amp; -a_{3} &amp; a_{2} \\ a_{3} &amp; 0 &amp; -a_{1} \\ -a_{2} &amp; a_{1} &amp; 0 \end{bmatrix}\]</span> 这两个符号将四元数映射成一个<span class="math inline">\(4\times4\)</span>矩阵。<br />
所以，四元数乘法写成矩阵形式如下：<span class="math display">\[ q_{1}^{+}q_{2}=\begin{bmatrix} s_{1} &amp; -v_{1}^{T} \\ v_{1} &amp; s_{1}I+v_{1}^{\land} \end{bmatrix} \begin{bmatrix} s_{2} \\ v_{2} \end{bmatrix}=\begin{bmatrix} -v_{1}^{T}v_{2}+s_{1}s_{2}  \\ s_{1}v_{2}+s_{2}v_{1}+v_{1}^{\land}v_{2} \end{bmatrix}=q_{1}q_{2}\]</span> 同理：<span class="math display">\[ q_{1}q_{2}=q_{1}^{+}q_{2}=q_{2}^{\oplus}q_{1}\]</span> 进一步：<span class="math display">\[ p&#39;=qpq^{-1}=q^{+}p^{+}q^{-1}=q^{+}q^{-1 ^{\oplus}}p\]</span> 代入对应矩阵：<span class="math display">\[ q^{+}(q^{-1})^{\oplus}=\begin{bmatrix} s &amp; -v^{T} \\ v &amp; sI+v^{\land} \end{bmatrix} \begin{bmatrix} s &amp; v^{T} \\ -v &amp; sI+v^{\land} \end{bmatrix}=\begin{bmatrix} 1 &amp; 0 \\ 0^{T} &amp; vv^{T}+s^{2}I+2sv^{\land}+(v^{\land})^{2} \end{bmatrix}\]</span> 由于<span class="math inline">\(p\)</span>和<span class="math inline">\(p&#39;\)</span>都是纯四元数，所以上面矩阵的右下角即是从四元数到旋转矩阵的变换关系：<span class="math display">\[ R=vv^{T}+s^{2}I+2sv^{\land}+(v^{\land})^{2}\]</span> 然后现在是四元数到旋转向量的变换，对上式两侧求迹（对角线的总和）：<span class="math display">\[tr(R)=tr(vv^{T}+s^{2}I+2sv^{\land}+tr((v^{\land})^{2}))\\ =v_{1}^{2}+v_{2}^{2}+v_{3}^{2}+3s^{2}-2(v_{1}^{2}+v_{2}^{2}+v_{3}^{2}) \\ =(1-s^{2})+3s^{2}-2(1-s^{2}) \\ =4s^{2}-1 \]</span> 转角<span class="math inline">\(\theta\)</span>为：<span class="math display">\[ \theta=arccos\frac{tr(R-1)}{2}=arccos(2s^{2}-1)\]</span> 即：<span class="math display">\[ cos\theta =2s^{2}-1=2cos^{2}\frac{\theta}{2}-1\]</span> 所以：<span class="math display">\[\theta = 2arccos s\]</span> 然后是旋转轴：<br />
上面最后一个矩阵那，如果用<span class="math inline">\(q\)</span> 的虚部代替<span class="math inline">\(p\)</span>，则表示<span class="math inline">\(q\)</span>的虚部组成的向量在旋转时是不变的，即为旋转轴。此时只要把它除以它的模长即可。<br />
最后，四元数到旋转向量的转换如下：<span class="math display">\[\begin{cases} \theta = 2arccos q_{0} \\ [n_{x},n_{y},n_{z}]^{T}=[q_{1},q_{2},q_{3}]^{T} / sin\frac{\theta}{2}\end{cases}\]</span> <strong><em>注：本文主要参考《视觉SLAM十四讲》</em></strong></p>
]]></content>
      <categories>
        <category>视觉SLAM相关</category>
      </categories>
  </entry>
</search>
